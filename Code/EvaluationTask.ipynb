{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EvaluationTask_Alpha.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kG0rnbnraa4j"
      },
      "source": [
        "#Libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import preprocessing\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import random_split\n",
        "import torch.nn as nn\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "bHg0X7j4r82T",
        "outputId": "fbd6d682-4c10-46b2-abd7-6a751d5969af"
      },
      "source": [
        "dataset = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/GSoC2021/Data/Dataset.csv\")\n",
        "dataset"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Experiment</th>\n",
              "      <th>Particle</th>\n",
              "      <th>E1</th>\n",
              "      <th>pt1</th>\n",
              "      <th>eta1</th>\n",
              "      <th>phi1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>j</td>\n",
              "      <td>1069460.0</td>\n",
              "      <td>751597.0</td>\n",
              "      <td>0.858186</td>\n",
              "      <td>-1.842170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>j</td>\n",
              "      <td>676000.0</td>\n",
              "      <td>640429.0</td>\n",
              "      <td>0.330450</td>\n",
              "      <td>0.704554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>j</td>\n",
              "      <td>936707.0</td>\n",
              "      <td>616229.0</td>\n",
              "      <td>0.973383</td>\n",
              "      <td>-1.565920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>j</td>\n",
              "      <td>640313.0</td>\n",
              "      <td>589524.0</td>\n",
              "      <td>0.390749</td>\n",
              "      <td>1.237340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>j</td>\n",
              "      <td>583373.0</td>\n",
              "      <td>545730.0</td>\n",
              "      <td>0.364057</td>\n",
              "      <td>-1.607320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22656</th>\n",
              "      <td>4003</td>\n",
              "      <td>j</td>\n",
              "      <td>58571.5</td>\n",
              "      <td>31726.6</td>\n",
              "      <td>1.209330</td>\n",
              "      <td>-1.832740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22657</th>\n",
              "      <td>4484</td>\n",
              "      <td>j</td>\n",
              "      <td>124037.0</td>\n",
              "      <td>36075.6</td>\n",
              "      <td>-1.901150</td>\n",
              "      <td>1.240720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22658</th>\n",
              "      <td>4998</td>\n",
              "      <td>j</td>\n",
              "      <td>271819.0</td>\n",
              "      <td>24791.0</td>\n",
              "      <td>3.085510</td>\n",
              "      <td>0.893034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22659</th>\n",
              "      <td>4484</td>\n",
              "      <td>j</td>\n",
              "      <td>187700.0</td>\n",
              "      <td>29610.3</td>\n",
              "      <td>2.532910</td>\n",
              "      <td>-2.436230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22660</th>\n",
              "      <td>4484</td>\n",
              "      <td>j</td>\n",
              "      <td>214097.0</td>\n",
              "      <td>25754.3</td>\n",
              "      <td>-2.806860</td>\n",
              "      <td>-2.143100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>22661 rows Ã— 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Experiment Particle         E1       pt1      eta1      phi1\n",
              "0               2        j  1069460.0  751597.0  0.858186 -1.842170\n",
              "1               3        j   676000.0  640429.0  0.330450  0.704554\n",
              "2               4        j   936707.0  616229.0  0.973383 -1.565920\n",
              "3               5        j   640313.0  589524.0  0.390749  1.237340\n",
              "4               6        j   583373.0  545730.0  0.364057 -1.607320\n",
              "...           ...      ...        ...       ...       ...       ...\n",
              "22656        4003        j    58571.5   31726.6  1.209330 -1.832740\n",
              "22657        4484        j   124037.0   36075.6 -1.901150  1.240720\n",
              "22658        4998        j   271819.0   24791.0  3.085510  0.893034\n",
              "22659        4484        j   187700.0   29610.3  2.532910 -2.436230\n",
              "22660        4484        j   214097.0   25754.3 -2.806860 -2.143100\n",
              "\n",
              "[22661 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWfvy16eklmi"
      },
      "source": [
        "normalized_data = preprocessing.normalize([dataset.E1, dataset.pt1, dataset.eta1, dataset.phi1])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBPCa03Xk2Nz"
      },
      "source": [
        "dataset[\"E1_Norm\"] = normalized_data[0]\n",
        "dataset[\"pt1_Norm\"] = normalized_data[1]\n",
        "dataset[\"eta1_Norm\"] = normalized_data[2]\n",
        "dataset[\"phi1_Norm\"] = normalized_data[3]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "gddw5JSNlkxc",
        "outputId": "abc2046d-03c9-46a8-ed7b-794b95a9ffce"
      },
      "source": [
        "dataset"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Experiment</th>\n",
              "      <th>Particle</th>\n",
              "      <th>E1</th>\n",
              "      <th>pt1</th>\n",
              "      <th>eta1</th>\n",
              "      <th>phi1</th>\n",
              "      <th>E1_Norm</th>\n",
              "      <th>pt1_Norm</th>\n",
              "      <th>eta1_Norm</th>\n",
              "      <th>phi1_Norm</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>j</td>\n",
              "      <td>1069460.0</td>\n",
              "      <td>751597.0</td>\n",
              "      <td>0.858186</td>\n",
              "      <td>-1.842170</td>\n",
              "      <td>0.011660</td>\n",
              "      <td>0.013896</td>\n",
              "      <td>0.003785</td>\n",
              "      <td>-0.006760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>j</td>\n",
              "      <td>676000.0</td>\n",
              "      <td>640429.0</td>\n",
              "      <td>0.330450</td>\n",
              "      <td>0.704554</td>\n",
              "      <td>0.007370</td>\n",
              "      <td>0.011840</td>\n",
              "      <td>0.001457</td>\n",
              "      <td>0.002585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>j</td>\n",
              "      <td>936707.0</td>\n",
              "      <td>616229.0</td>\n",
              "      <td>0.973383</td>\n",
              "      <td>-1.565920</td>\n",
              "      <td>0.010213</td>\n",
              "      <td>0.011393</td>\n",
              "      <td>0.004293</td>\n",
              "      <td>-0.005746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>j</td>\n",
              "      <td>640313.0</td>\n",
              "      <td>589524.0</td>\n",
              "      <td>0.390749</td>\n",
              "      <td>1.237340</td>\n",
              "      <td>0.006981</td>\n",
              "      <td>0.010899</td>\n",
              "      <td>0.001723</td>\n",
              "      <td>0.004540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>j</td>\n",
              "      <td>583373.0</td>\n",
              "      <td>545730.0</td>\n",
              "      <td>0.364057</td>\n",
              "      <td>-1.607320</td>\n",
              "      <td>0.006360</td>\n",
              "      <td>0.010089</td>\n",
              "      <td>0.001606</td>\n",
              "      <td>-0.005898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22656</th>\n",
              "      <td>4003</td>\n",
              "      <td>j</td>\n",
              "      <td>58571.5</td>\n",
              "      <td>31726.6</td>\n",
              "      <td>1.209330</td>\n",
              "      <td>-1.832740</td>\n",
              "      <td>0.000639</td>\n",
              "      <td>0.000587</td>\n",
              "      <td>0.005334</td>\n",
              "      <td>-0.006725</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22657</th>\n",
              "      <td>4484</td>\n",
              "      <td>j</td>\n",
              "      <td>124037.0</td>\n",
              "      <td>36075.6</td>\n",
              "      <td>-1.901150</td>\n",
              "      <td>1.240720</td>\n",
              "      <td>0.001352</td>\n",
              "      <td>0.000667</td>\n",
              "      <td>-0.008385</td>\n",
              "      <td>0.004553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22658</th>\n",
              "      <td>4998</td>\n",
              "      <td>j</td>\n",
              "      <td>271819.0</td>\n",
              "      <td>24791.0</td>\n",
              "      <td>3.085510</td>\n",
              "      <td>0.893034</td>\n",
              "      <td>0.002964</td>\n",
              "      <td>0.000458</td>\n",
              "      <td>0.013609</td>\n",
              "      <td>0.003277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22659</th>\n",
              "      <td>4484</td>\n",
              "      <td>j</td>\n",
              "      <td>187700.0</td>\n",
              "      <td>29610.3</td>\n",
              "      <td>2.532910</td>\n",
              "      <td>-2.436230</td>\n",
              "      <td>0.002046</td>\n",
              "      <td>0.000547</td>\n",
              "      <td>0.011172</td>\n",
              "      <td>-0.008940</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22660</th>\n",
              "      <td>4484</td>\n",
              "      <td>j</td>\n",
              "      <td>214097.0</td>\n",
              "      <td>25754.3</td>\n",
              "      <td>-2.806860</td>\n",
              "      <td>-2.143100</td>\n",
              "      <td>0.002334</td>\n",
              "      <td>0.000476</td>\n",
              "      <td>-0.012380</td>\n",
              "      <td>-0.007864</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>22661 rows Ã— 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Experiment Particle         E1  ...  pt1_Norm  eta1_Norm  phi1_Norm\n",
              "0               2        j  1069460.0  ...  0.013896   0.003785  -0.006760\n",
              "1               3        j   676000.0  ...  0.011840   0.001457   0.002585\n",
              "2               4        j   936707.0  ...  0.011393   0.004293  -0.005746\n",
              "3               5        j   640313.0  ...  0.010899   0.001723   0.004540\n",
              "4               6        j   583373.0  ...  0.010089   0.001606  -0.005898\n",
              "...           ...      ...        ...  ...       ...        ...        ...\n",
              "22656        4003        j    58571.5  ...  0.000587   0.005334  -0.006725\n",
              "22657        4484        j   124037.0  ...  0.000667  -0.008385   0.004553\n",
              "22658        4998        j   271819.0  ...  0.000458   0.013609   0.003277\n",
              "22659        4484        j   187700.0  ...  0.000547   0.011172  -0.008940\n",
              "22660        4484        j   214097.0  ...  0.000476  -0.012380  -0.007864\n",
              "\n",
              "[22661 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPxdKNB6sibw"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O47a6AI7Xg-m"
      },
      "source": [
        "# Dataset\n",
        "dataset_comp = [dataset.E1_Norm, dataset.pt1_Norm, dataset.eta1_Norm, dataset.phi1_Norm]\n",
        "dataset_comp = np.array(dataset_comp)\n",
        "dataset_comp = dataset_comp.transpose()\n",
        "\n",
        "\n",
        "#\n",
        "\n",
        "test_abs = int(len(dataset_comp) * 0.8)\n",
        "data_train, data_test = random_split(dataset_comp, [test_abs, len(dataset_comp) - test_abs])\n",
        "\n",
        "bs = 256\n",
        "\n",
        "train_ds = TensorDataset(torch.tensor(data_train, dtype=torch.float, device=\"cuda\"), torch.tensor(data_train, dtype=torch.float, device=\"cuda\"))\n",
        "valid_ds = TensorDataset(torch.tensor(data_test, dtype=torch.float, device=\"cuda\"), torch.tensor(data_test, dtype=torch.float, device=\"cuda\"))\n",
        "\n",
        "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
        "valid_dl = DataLoader(valid_ds, batch_size=bs * 2)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNwm3a9Lnh0z",
        "outputId": "42ee5b5c-e4eb-4eed-fedc-d97553aefb97"
      },
      "source": [
        "class AE_3D_200_LeakyReLU(torch.nn.Module):\n",
        "    def __init__(self, n_features=4):\n",
        "        super(AE_3D_200_LeakyReLU, self).__init__()\n",
        "        self.en1 = nn.Linear(n_features, 200)\n",
        "        self.en2 = nn.Linear(200, 200)\n",
        "        self.en3 = nn.Linear(200, 20)\n",
        "        self.en4 = nn.Linear(20, 3)\n",
        "        self.de1 = nn.Linear(3, 20)\n",
        "        self.de2 = nn.Linear(20, 200)\n",
        "        self.de3 = nn.Linear(200, 200)\n",
        "        self.de4 = nn.Linear(200, n_features)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def encode(self, x):\n",
        "        return self.en4(self.tanh(self.en3(self.tanh(self.en2(self.tanh(self.en1(x)))))))\n",
        "\n",
        "    def decode(self, x):\n",
        "        return self.de4(self.tanh(self.de3(self.tanh(self.de2(self.tanh(self.de1(self.tanh(x))))))))\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encode(x)\n",
        "        return self.decode(z)\n",
        "\n",
        "    def describe(self):\n",
        "        return 'in-200-200-20-3-20-200-200-out'\n",
        "\n",
        "model = AE_3D_200_LeakyReLU()\n",
        "model.to('cuda')\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AE_3D_200_LeakyReLU(\n",
              "  (en1): Linear(in_features=4, out_features=200, bias=True)\n",
              "  (en2): Linear(in_features=200, out_features=200, bias=True)\n",
              "  (en3): Linear(in_features=200, out_features=20, bias=True)\n",
              "  (en4): Linear(in_features=20, out_features=3, bias=True)\n",
              "  (de1): Linear(in_features=3, out_features=20, bias=True)\n",
              "  (de2): Linear(in_features=20, out_features=200, bias=True)\n",
              "  (de3): Linear(in_features=200, out_features=200, bias=True)\n",
              "  (de4): Linear(in_features=200, out_features=4, bias=True)\n",
              "  (tanh): Tanh()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wbDmtulgDl4"
      },
      "source": [
        "training_loss = []\n",
        "validation_loss = []"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WJf8fgtnkbx",
        "outputId": "d677d005-9c78-43e9-e012-0a616d8fee90"
      },
      "source": [
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-7)\n",
        "\n",
        "\n",
        "for e in range(100):\n",
        "    \n",
        "    for t in range(100):\n",
        "        # Forward pass: Compute predicted y by passing x to the model\n",
        "        train_features, train_labels = next(iter(train_dl))\n",
        "        valid_features, valid_labels = next(iter(valid_dl))\n",
        "\n",
        "        y_pred = model(train_features)\n",
        "        valid_pred = model(valid_features)\n",
        "        # Compute and print loss\n",
        "        loss = criterion(y_pred, train_labels)\n",
        "        valid_loss = criterion(valid_pred, valid_labels)\n",
        "\n",
        "        training_loss.append(loss.item())\n",
        "        validation_loss.append(valid_loss.item())\n",
        "\n",
        "        if t % 4 == 0:\n",
        "            print(f\"Epoch {e}: {t} -> Training Loss {loss.item()} Validation Loss {valid_loss.item()}\")\n",
        "\n",
        "        # Zero gradients, perform a backward pass, and update the weights.\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0: 0 -> Training Loss 0.008396790362894535 Validation Loss 0.008486373350024223\n",
            "Epoch 0: 4 -> Training Loss 0.00843090284615755 Validation Loss 0.008474167436361313\n",
            "Epoch 0: 8 -> Training Loss 0.008425788022577763 Validation Loss 0.008461973629891872\n",
            "Epoch 0: 12 -> Training Loss 0.008435152471065521 Validation Loss 0.008449788205325603\n",
            "Epoch 0: 16 -> Training Loss 0.008356526494026184 Validation Loss 0.008437613025307655\n",
            "Epoch 0: 20 -> Training Loss 0.008361268788576126 Validation Loss 0.008425445295870304\n",
            "Epoch 0: 24 -> Training Loss 0.008413458243012428 Validation Loss 0.008413288742303848\n",
            "Epoch 0: 28 -> Training Loss 0.008322849869728088 Validation Loss 0.008401142433285713\n",
            "Epoch 0: 32 -> Training Loss 0.008336488157510757 Validation Loss 0.008389008231461048\n",
            "Epoch 0: 36 -> Training Loss 0.008363083004951477 Validation Loss 0.00837688334286213\n",
            "Epoch 0: 40 -> Training Loss 0.008327275514602661 Validation Loss 0.00836477056145668\n",
            "Epoch 0: 44 -> Training Loss 0.008300447836518288 Validation Loss 0.008352668955922127\n",
            "Epoch 0: 48 -> Training Loss 0.008286576718091965 Validation Loss 0.00834058877080679\n",
            "Epoch 0: 52 -> Training Loss 0.008296370506286621 Validation Loss 0.008328516967594624\n",
            "Epoch 0: 56 -> Training Loss 0.008264711126685143 Validation Loss 0.008316458202898502\n",
            "Epoch 0: 60 -> Training Loss 0.008271105587482452 Validation Loss 0.008304418995976448\n",
            "Epoch 0: 64 -> Training Loss 0.008243386633694172 Validation Loss 0.008292392827570438\n",
            "Epoch 0: 68 -> Training Loss 0.008298113010823727 Validation Loss 0.008280377835035324\n",
            "Epoch 0: 72 -> Training Loss 0.008230537176132202 Validation Loss 0.00826837308704853\n",
            "Epoch 0: 76 -> Training Loss 0.008200470358133316 Validation Loss 0.008256386034190655\n",
            "Epoch 0: 80 -> Training Loss 0.008149638772010803 Validation Loss 0.008244413882493973\n",
            "Epoch 0: 84 -> Training Loss 0.008183840662240982 Validation Loss 0.008232451975345612\n",
            "Epoch 0: 88 -> Training Loss 0.008161811158061028 Validation Loss 0.008220501244068146\n",
            "Epoch 0: 92 -> Training Loss 0.008131994865834713 Validation Loss 0.008208559826016426\n",
            "Epoch 0: 96 -> Training Loss 0.008213149383664131 Validation Loss 0.008196631446480751\n",
            "Epoch 1: 0 -> Training Loss 0.008137769997119904 Validation Loss 0.008184713311493397\n",
            "Epoch 1: 4 -> Training Loss 0.008116215467453003 Validation Loss 0.008172807283699512\n",
            "Epoch 1: 8 -> Training Loss 0.008127978071570396 Validation Loss 0.008160914294421673\n",
            "Epoch 1: 12 -> Training Loss 0.008092950098216534 Validation Loss 0.008149033412337303\n",
            "Epoch 1: 16 -> Training Loss 0.008109403774142265 Validation Loss 0.008137165568768978\n",
            "Epoch 1: 20 -> Training Loss 0.00806792639195919 Validation Loss 0.008125308901071548\n",
            "Epoch 1: 24 -> Training Loss 0.008043663576245308 Validation Loss 0.008113465271890163\n",
            "Epoch 1: 28 -> Training Loss 0.00801156647503376 Validation Loss 0.008101632818579674\n",
            "Epoch 1: 32 -> Training Loss 0.008043976500630379 Validation Loss 0.008089814335107803\n",
            "Epoch 1: 36 -> Training Loss 0.008082162588834763 Validation Loss 0.00807801540941\n",
            "Epoch 1: 40 -> Training Loss 0.008020968176424503 Validation Loss 0.008066224865615368\n",
            "Epoch 1: 44 -> Training Loss 0.008015412837266922 Validation Loss 0.008054453879594803\n",
            "Epoch 1: 48 -> Training Loss 0.008039933629333973 Validation Loss 0.008042700588703156\n",
            "Epoch 1: 52 -> Training Loss 0.008013471029698849 Validation Loss 0.008030962198972702\n",
            "Epoch 1: 56 -> Training Loss 0.007960457354784012 Validation Loss 0.008019237779080868\n",
            "Epoch 1: 60 -> Training Loss 0.007968258112668991 Validation Loss 0.00800752080976963\n",
            "Epoch 1: 64 -> Training Loss 0.007954652421176434 Validation Loss 0.007995827123522758\n",
            "Epoch 1: 68 -> Training Loss 0.007774241268634796 Validation Loss 0.007984178140759468\n",
            "Epoch 1: 72 -> Training Loss 0.0079283956438303 Validation Loss 0.007972588762640953\n",
            "Epoch 1: 76 -> Training Loss 0.007874315604567528 Validation Loss 0.007961015217006207\n",
            "Epoch 1: 80 -> Training Loss 0.007922285236418247 Validation Loss 0.007949456572532654\n",
            "Epoch 1: 84 -> Training Loss 0.007849525660276413 Validation Loss 0.007937894202768803\n",
            "Epoch 1: 88 -> Training Loss 0.00789257325232029 Validation Loss 0.007926346734166145\n",
            "Epoch 1: 92 -> Training Loss 0.007891812361776829 Validation Loss 0.007914816960692406\n",
            "Epoch 1: 96 -> Training Loss 0.007880967110395432 Validation Loss 0.007903298363089561\n",
            "Epoch 2: 0 -> Training Loss 0.007829196751117706 Validation Loss 0.007891787216067314\n",
            "Epoch 2: 4 -> Training Loss 0.007810501381754875 Validation Loss 0.00788029283285141\n",
            "Epoch 2: 8 -> Training Loss 0.007830285467207432 Validation Loss 0.007868818007409573\n",
            "Epoch 2: 12 -> Training Loss 0.007759641390293837 Validation Loss 0.00785735622048378\n",
            "Epoch 2: 16 -> Training Loss 0.0077679343521595 Validation Loss 0.007845909334719181\n",
            "Epoch 2: 20 -> Training Loss 0.007821452803909779 Validation Loss 0.007834473624825478\n",
            "Epoch 2: 24 -> Training Loss 0.007774721831083298 Validation Loss 0.007823048159480095\n",
            "Epoch 2: 28 -> Training Loss 0.007769884541630745 Validation Loss 0.007811632938683033\n",
            "Epoch 2: 32 -> Training Loss 0.007766482420265675 Validation Loss 0.0078002274967730045\n",
            "Epoch 2: 36 -> Training Loss 0.0076982369646430016 Validation Loss 0.0077888318337500095\n",
            "Epoch 2: 40 -> Training Loss 0.007725914008915424 Validation Loss 0.0077774496749043465\n",
            "Epoch 2: 44 -> Training Loss 0.007754005026072264 Validation Loss 0.007766074500977993\n",
            "Epoch 2: 48 -> Training Loss 0.007740821689367294 Validation Loss 0.007754707243293524\n",
            "Epoch 2: 52 -> Training Loss 0.007744898088276386 Validation Loss 0.007743350695818663\n",
            "Epoch 2: 56 -> Training Loss 0.007632909342646599 Validation Loss 0.00773200485855341\n",
            "Epoch 2: 60 -> Training Loss 0.007665087003260851 Validation Loss 0.007720674388110638\n",
            "Epoch 2: 64 -> Training Loss 0.007642767392098904 Validation Loss 0.007709362078458071\n",
            "Epoch 2: 68 -> Training Loss 0.007657645735889673 Validation Loss 0.007698066532611847\n",
            "Epoch 2: 72 -> Training Loss 0.00768459215760231 Validation Loss 0.00768678355962038\n",
            "Epoch 2: 76 -> Training Loss 0.007670511491596699 Validation Loss 0.007675510365515947\n",
            "Epoch 2: 80 -> Training Loss 0.0075805094093084335 Validation Loss 0.007664247415959835\n",
            "Epoch 2: 84 -> Training Loss 0.0075555588118731976 Validation Loss 0.007652998901903629\n",
            "Epoch 2: 88 -> Training Loss 0.007636004127562046 Validation Loss 0.007641764357686043\n",
            "Epoch 2: 92 -> Training Loss 0.007632874883711338 Validation Loss 0.007630537264049053\n",
            "Epoch 2: 96 -> Training Loss 0.007560832425951958 Validation Loss 0.007619316689670086\n",
            "Epoch 3: 0 -> Training Loss 0.00755919236689806 Validation Loss 0.0076081110164523125\n",
            "Epoch 3: 4 -> Training Loss 0.007510174997150898 Validation Loss 0.007596923038363457\n",
            "Epoch 3: 8 -> Training Loss 0.007593938149511814 Validation Loss 0.007585747167468071\n",
            "Epoch 3: 12 -> Training Loss 0.007518993224948645 Validation Loss 0.007574580609798431\n",
            "Epoch 3: 16 -> Training Loss 0.007588201202452183 Validation Loss 0.007563427090644836\n",
            "Epoch 3: 20 -> Training Loss 0.007536632940173149 Validation Loss 0.007552281953394413\n",
            "Epoch 3: 24 -> Training Loss 0.0075495424680411816 Validation Loss 0.007541142404079437\n",
            "Epoch 3: 28 -> Training Loss 0.007493542041629553 Validation Loss 0.007530015893280506\n",
            "Epoch 3: 32 -> Training Loss 0.0074806371703743935 Validation Loss 0.007518901489675045\n",
            "Epoch 3: 36 -> Training Loss 0.007421046495437622 Validation Loss 0.007507799193263054\n",
            "Epoch 3: 40 -> Training Loss 0.007475989870727062 Validation Loss 0.007496709935367107\n",
            "Epoch 3: 44 -> Training Loss 0.0074121891520917416 Validation Loss 0.007485635578632355\n",
            "Epoch 3: 48 -> Training Loss 0.007425762712955475 Validation Loss 0.0074745784513652325\n",
            "Epoch 3: 52 -> Training Loss 0.007457137573510408 Validation Loss 0.007463532499969006\n",
            "Epoch 3: 56 -> Training Loss 0.007409552112221718 Validation Loss 0.0074524953961372375\n",
            "Epoch 3: 60 -> Training Loss 0.007372194901108742 Validation Loss 0.00744146853685379\n",
            "Epoch 3: 64 -> Training Loss 0.00738365575671196 Validation Loss 0.007430451922118664\n",
            "Epoch 3: 68 -> Training Loss 0.007343584671616554 Validation Loss 0.007419446017593145\n",
            "Epoch 3: 72 -> Training Loss 0.007351309526711702 Validation Loss 0.007408451288938522\n",
            "Epoch 3: 76 -> Training Loss 0.0073547461070120335 Validation Loss 0.007397462613880634\n",
            "Epoch 3: 80 -> Training Loss 0.0074106063693761826 Validation Loss 0.007386484649032354\n",
            "Epoch 3: 84 -> Training Loss 0.007321237586438656 Validation Loss 0.007375514134764671\n",
            "Epoch 3: 88 -> Training Loss 0.007284543476998806 Validation Loss 0.007364555727690458\n",
            "Epoch 3: 92 -> Training Loss 0.0073420582339167595 Validation Loss 0.007353612221777439\n",
            "Epoch 3: 96 -> Training Loss 0.007322113960981369 Validation Loss 0.007342677563428879\n",
            "Epoch 4: 0 -> Training Loss 0.007246006280183792 Validation Loss 0.007331752218306065\n",
            "Epoch 4: 4 -> Training Loss 0.007231732830405235 Validation Loss 0.007320842705667019\n",
            "Epoch 4: 8 -> Training Loss 0.007255650125443935 Validation Loss 0.007309949025511742\n",
            "Epoch 4: 12 -> Training Loss 0.007307804189622402 Validation Loss 0.007299061864614487\n",
            "Epoch 4: 16 -> Training Loss 0.007250582799315453 Validation Loss 0.007288185879588127\n",
            "Epoch 4: 20 -> Training Loss 0.007284331135451794 Validation Loss 0.007277325727045536\n",
            "Epoch 4: 24 -> Training Loss 0.0072202663868665695 Validation Loss 0.007266471162438393\n",
            "Epoch 4: 28 -> Training Loss 0.007185261696577072 Validation Loss 0.007255627773702145\n",
            "Epoch 4: 32 -> Training Loss 0.007160038687288761 Validation Loss 0.007244803011417389\n",
            "Epoch 4: 36 -> Training Loss 0.007191971410065889 Validation Loss 0.007233995012938976\n",
            "Epoch 4: 40 -> Training Loss 0.007173966150730848 Validation Loss 0.007223193068057299\n",
            "Epoch 4: 44 -> Training Loss 0.007149519864469767 Validation Loss 0.007212396711111069\n",
            "Epoch 4: 48 -> Training Loss 0.007170620374381542 Validation Loss 0.007201609201729298\n",
            "Epoch 4: 52 -> Training Loss 0.007173811085522175 Validation Loss 0.007190834730863571\n",
            "Epoch 4: 56 -> Training Loss 0.0071920473128557205 Validation Loss 0.0071800705045461655\n",
            "Epoch 4: 60 -> Training Loss 0.007190207950770855 Validation Loss 0.007169312797486782\n",
            "Epoch 4: 64 -> Training Loss 0.007160445675253868 Validation Loss 0.007158562541007996\n",
            "Epoch 4: 68 -> Training Loss 0.0071098096668720245 Validation Loss 0.007147822994738817\n",
            "Epoch 4: 72 -> Training Loss 0.007082059048116207 Validation Loss 0.007137098349630833\n",
            "Epoch 4: 76 -> Training Loss 0.007042710669338703 Validation Loss 0.0071263848803937435\n",
            "Epoch 4: 80 -> Training Loss 0.007095129694789648 Validation Loss 0.00711568770930171\n",
            "Epoch 4: 84 -> Training Loss 0.007077790796756744 Validation Loss 0.007105002645403147\n",
            "Epoch 4: 88 -> Training Loss 0.007075101137161255 Validation Loss 0.007094331085681915\n",
            "Epoch 4: 92 -> Training Loss 0.00705315638333559 Validation Loss 0.007083668373525143\n",
            "Epoch 4: 96 -> Training Loss 0.006962442770600319 Validation Loss 0.007073018699884415\n",
            "Epoch 5: 0 -> Training Loss 0.007021716330200434 Validation Loss 0.007062386721372604\n",
            "Epoch 5: 4 -> Training Loss 0.007071474101394415 Validation Loss 0.007051763124763966\n",
            "Epoch 5: 8 -> Training Loss 0.0069681196473538876 Validation Loss 0.007041148375719786\n",
            "Epoch 5: 12 -> Training Loss 0.006972882896661758 Validation Loss 0.0070305485278368\n",
            "Epoch 5: 16 -> Training Loss 0.006960101891309023 Validation Loss 0.00701996311545372\n",
            "Epoch 5: 20 -> Training Loss 0.0069329263642430305 Validation Loss 0.007009394001215696\n",
            "Epoch 5: 24 -> Training Loss 0.006918260827660561 Validation Loss 0.006998843513429165\n",
            "Epoch 5: 28 -> Training Loss 0.006980425212532282 Validation Loss 0.006988306995481253\n",
            "Epoch 5: 32 -> Training Loss 0.0069486116990447044 Validation Loss 0.006977779325097799\n",
            "Epoch 5: 36 -> Training Loss 0.0069208815693855286 Validation Loss 0.006967264227569103\n",
            "Epoch 5: 40 -> Training Loss 0.006882176734507084 Validation Loss 0.006956763565540314\n",
            "Epoch 5: 44 -> Training Loss 0.006892289500683546 Validation Loss 0.006946278735995293\n",
            "Epoch 5: 48 -> Training Loss 0.00689314678311348 Validation Loss 0.006935812532901764\n",
            "Epoch 5: 52 -> Training Loss 0.0068546137772500515 Validation Loss 0.006925356574356556\n",
            "Epoch 5: 56 -> Training Loss 0.006909267045557499 Validation Loss 0.006914910860359669\n",
            "Epoch 5: 60 -> Training Loss 0.006857350934296846 Validation Loss 0.006904464680701494\n",
            "Epoch 5: 64 -> Training Loss 0.0068260543048381805 Validation Loss 0.0068940334022045135\n",
            "Epoch 5: 68 -> Training Loss 0.006821769289672375 Validation Loss 0.006883619818836451\n",
            "Epoch 5: 72 -> Training Loss 0.006844877265393734 Validation Loss 0.006873228587210178\n",
            "Epoch 5: 76 -> Training Loss 0.006843778304755688 Validation Loss 0.006862848997116089\n",
            "Epoch 5: 80 -> Training Loss 0.00682944618165493 Validation Loss 0.006852474994957447\n",
            "Epoch 5: 84 -> Training Loss 0.006841341033577919 Validation Loss 0.006842115893959999\n",
            "Epoch 5: 88 -> Training Loss 0.006754780188202858 Validation Loss 0.006831765174865723\n",
            "Epoch 5: 92 -> Training Loss 0.006813820451498032 Validation Loss 0.006821430753916502\n",
            "Epoch 5: 96 -> Training Loss 0.006760647986084223 Validation Loss 0.006811106111854315\n",
            "Epoch 6: 0 -> Training Loss 0.006683357059955597 Validation Loss 0.006800796836614609\n",
            "Epoch 6: 4 -> Training Loss 0.006744114216417074 Validation Loss 0.006790506653487682\n",
            "Epoch 6: 8 -> Training Loss 0.006756148301064968 Validation Loss 0.006780228577554226\n",
            "Epoch 6: 12 -> Training Loss 0.006797341629862785 Validation Loss 0.006769968196749687\n",
            "Epoch 6: 16 -> Training Loss 0.006695312447845936 Validation Loss 0.006759715266525745\n",
            "Epoch 6: 20 -> Training Loss 0.006732098292559385 Validation Loss 0.00674947677180171\n",
            "Epoch 6: 24 -> Training Loss 0.006614629179239273 Validation Loss 0.006739246193319559\n",
            "Epoch 6: 28 -> Training Loss 0.006727107334882021 Validation Loss 0.006729035172611475\n",
            "Epoch 6: 32 -> Training Loss 0.00667162099853158 Validation Loss 0.006718828342854977\n",
            "Epoch 6: 36 -> Training Loss 0.006640136241912842 Validation Loss 0.006708634085953236\n",
            "Epoch 6: 40 -> Training Loss 0.00670714071020484 Validation Loss 0.006698456592857838\n",
            "Epoch 6: 44 -> Training Loss 0.006646747700870037 Validation Loss 0.006688287016004324\n",
            "Epoch 6: 48 -> Training Loss 0.006597187370061874 Validation Loss 0.00667812954634428\n",
            "Epoch 6: 52 -> Training Loss 0.006565225310623646 Validation Loss 0.006667987443506718\n",
            "Epoch 6: 56 -> Training Loss 0.006566913798451424 Validation Loss 0.006657867226749659\n",
            "Epoch 6: 60 -> Training Loss 0.006531726568937302 Validation Loss 0.006647756323218346\n",
            "Epoch 6: 64 -> Training Loss 0.00650367233902216 Validation Loss 0.006637663580477238\n",
            "Epoch 6: 68 -> Training Loss 0.0065542529337108135 Validation Loss 0.006627585738897324\n",
            "Epoch 6: 72 -> Training Loss 0.0066123357973992825 Validation Loss 0.006617519538849592\n",
            "Epoch 6: 76 -> Training Loss 0.006541208364069462 Validation Loss 0.006607457064092159\n",
            "Epoch 6: 80 -> Training Loss 0.006545315962284803 Validation Loss 0.006597401108592749\n",
            "Epoch 6: 84 -> Training Loss 0.006579659413546324 Validation Loss 0.006587355397641659\n",
            "Epoch 6: 88 -> Training Loss 0.006544926669448614 Validation Loss 0.0065773180685937405\n",
            "Epoch 6: 92 -> Training Loss 0.006500901188701391 Validation Loss 0.006567292381078005\n",
            "Epoch 6: 96 -> Training Loss 0.006475144997239113 Validation Loss 0.006557275075465441\n",
            "Epoch 7: 0 -> Training Loss 0.0064628468826413155 Validation Loss 0.006547266151756048\n",
            "Epoch 7: 4 -> Training Loss 0.006487696897238493 Validation Loss 0.006537268869578838\n",
            "Epoch 7: 8 -> Training Loss 0.006524489261209965 Validation Loss 0.00652728695422411\n",
            "Epoch 7: 12 -> Training Loss 0.006436397321522236 Validation Loss 0.006517314352095127\n",
            "Epoch 7: 16 -> Training Loss 0.006431855261325836 Validation Loss 0.006507354322820902\n",
            "Epoch 7: 20 -> Training Loss 0.006514015607535839 Validation Loss 0.006497410126030445\n",
            "Epoch 7: 24 -> Training Loss 0.006430759094655514 Validation Loss 0.006487475708127022\n",
            "Epoch 7: 28 -> Training Loss 0.006477729417383671 Validation Loss 0.006477554328739643\n",
            "Epoch 7: 32 -> Training Loss 0.006459643132984638 Validation Loss 0.006467642728239298\n",
            "Epoch 7: 36 -> Training Loss 0.006356613244861364 Validation Loss 0.0064577367156744\n",
            "Epoch 7: 40 -> Training Loss 0.006355426274240017 Validation Loss 0.006447848863899708\n",
            "Epoch 7: 44 -> Training Loss 0.006387494970113039 Validation Loss 0.006437971256673336\n",
            "Epoch 7: 48 -> Training Loss 0.006343759596347809 Validation Loss 0.006428100168704987\n",
            "Epoch 7: 52 -> Training Loss 0.006395326461642981 Validation Loss 0.006418242584913969\n",
            "Epoch 7: 56 -> Training Loss 0.006321782246232033 Validation Loss 0.006408394314348698\n",
            "Epoch 7: 60 -> Training Loss 0.006348918657749891 Validation Loss 0.006398558616638184\n",
            "Epoch 7: 64 -> Training Loss 0.006375142373144627 Validation Loss 0.006388734094798565\n",
            "Epoch 7: 68 -> Training Loss 0.006283610127866268 Validation Loss 0.006378924008458853\n",
            "Epoch 7: 72 -> Training Loss 0.006351902149617672 Validation Loss 0.00636912789195776\n",
            "Epoch 7: 76 -> Training Loss 0.006330091506242752 Validation Loss 0.006359333638101816\n",
            "Epoch 7: 80 -> Training Loss 0.006284553557634354 Validation Loss 0.006349546834826469\n",
            "Epoch 7: 84 -> Training Loss 0.0062191067263484 Validation Loss 0.0063397688791155815\n",
            "Epoch 7: 88 -> Training Loss 0.006279514171183109 Validation Loss 0.0063300104811787605\n",
            "Epoch 7: 92 -> Training Loss 0.006260876543819904 Validation Loss 0.006320261396467686\n",
            "Epoch 7: 96 -> Training Loss 0.006297323852777481 Validation Loss 0.006310530938208103\n",
            "Epoch 8: 0 -> Training Loss 0.006288101896643639 Validation Loss 0.006300807930529118\n",
            "Epoch 8: 4 -> Training Loss 0.006217331625521183 Validation Loss 0.006291091907769442\n",
            "Epoch 8: 8 -> Training Loss 0.006233419757336378 Validation Loss 0.00628138892352581\n",
            "Epoch 8: 12 -> Training Loss 0.006257282570004463 Validation Loss 0.006271693855524063\n",
            "Epoch 8: 16 -> Training Loss 0.006208210717886686 Validation Loss 0.006262004841119051\n",
            "Epoch 8: 20 -> Training Loss 0.006273398641496897 Validation Loss 0.006252330727875233\n",
            "Epoch 8: 24 -> Training Loss 0.006158006377518177 Validation Loss 0.006242665462195873\n",
            "Epoch 8: 28 -> Training Loss 0.006171465385705233 Validation Loss 0.006233015563338995\n",
            "Epoch 8: 32 -> Training Loss 0.006238778121769428 Validation Loss 0.006223378702998161\n",
            "Epoch 8: 36 -> Training Loss 0.006175851449370384 Validation Loss 0.006213742308318615\n",
            "Epoch 8: 40 -> Training Loss 0.006169987842440605 Validation Loss 0.006204115692526102\n",
            "Epoch 8: 44 -> Training Loss 0.006126173306256533 Validation Loss 0.006194496992975473\n",
            "Epoch 8: 48 -> Training Loss 0.00618066405877471 Validation Loss 0.006184895057231188\n",
            "Epoch 8: 52 -> Training Loss 0.0060915653593838215 Validation Loss 0.006175307556986809\n",
            "Epoch 8: 56 -> Training Loss 0.0061791702173650265 Validation Loss 0.006165738217532635\n",
            "Epoch 8: 60 -> Training Loss 0.0061104437336325645 Validation Loss 0.006156178191304207\n",
            "Epoch 8: 64 -> Training Loss 0.006119072437286377 Validation Loss 0.006146625615656376\n",
            "Epoch 8: 68 -> Training Loss 0.0061188386753201485 Validation Loss 0.006137075833976269\n",
            "Epoch 8: 72 -> Training Loss 0.0060276067815721035 Validation Loss 0.006127535365521908\n",
            "Epoch 8: 76 -> Training Loss 0.006025475915521383 Validation Loss 0.0061180139891803265\n",
            "Epoch 8: 80 -> Training Loss 0.006111782044172287 Validation Loss 0.006108512170612812\n",
            "Epoch 8: 84 -> Training Loss 0.006111549213528633 Validation Loss 0.006099021062254906\n",
            "Epoch 8: 88 -> Training Loss 0.0060123843140900135 Validation Loss 0.006089537404477596\n",
            "Epoch 8: 92 -> Training Loss 0.00602930411696434 Validation Loss 0.006080068182200193\n",
            "Epoch 8: 96 -> Training Loss 0.00603269599378109 Validation Loss 0.0060706110671162605\n",
            "Epoch 9: 0 -> Training Loss 0.0060346247628331184 Validation Loss 0.006061166524887085\n",
            "Epoch 9: 4 -> Training Loss 0.0060187336057424545 Validation Loss 0.006051725707948208\n",
            "Epoch 9: 8 -> Training Loss 0.00596257857978344 Validation Loss 0.006042296066880226\n",
            "Epoch 9: 12 -> Training Loss 0.005949090700596571 Validation Loss 0.006032879464328289\n",
            "Epoch 9: 16 -> Training Loss 0.006003538612276316 Validation Loss 0.006023474037647247\n",
            "Epoch 9: 20 -> Training Loss 0.005960715468972921 Validation Loss 0.006014076992869377\n",
            "Epoch 9: 24 -> Training Loss 0.005979827605187893 Validation Loss 0.006004692986607552\n",
            "Epoch 9: 28 -> Training Loss 0.005934477783739567 Validation Loss 0.0059953173622488976\n",
            "Epoch 9: 32 -> Training Loss 0.005978059023618698 Validation Loss 0.005985954310745001\n",
            "Epoch 9: 36 -> Training Loss 0.005914284382015467 Validation Loss 0.00597659545019269\n",
            "Epoch 9: 40 -> Training Loss 0.005914594046771526 Validation Loss 0.005967245437204838\n",
            "Epoch 9: 44 -> Training Loss 0.00595211423933506 Validation Loss 0.005957907065749168\n",
            "Epoch 9: 48 -> Training Loss 0.005912809632718563 Validation Loss 0.005948582664132118\n",
            "Epoch 9: 52 -> Training Loss 0.0059247128665447235 Validation Loss 0.005939263850450516\n",
            "Epoch 9: 56 -> Training Loss 0.005908354185521603 Validation Loss 0.005929948762059212\n",
            "Epoch 9: 60 -> Training Loss 0.005904208868741989 Validation Loss 0.005920641589909792\n",
            "Epoch 9: 64 -> Training Loss 0.005870223511010408 Validation Loss 0.005911343730986118\n",
            "Epoch 9: 68 -> Training Loss 0.005824688822031021 Validation Loss 0.00590206403285265\n",
            "Epoch 9: 72 -> Training Loss 0.0058357929810881615 Validation Loss 0.005892799235880375\n",
            "Epoch 9: 76 -> Training Loss 0.00581600284203887 Validation Loss 0.005883547477424145\n",
            "Epoch 9: 80 -> Training Loss 0.005887752864509821 Validation Loss 0.0058743152767419815\n",
            "Epoch 9: 84 -> Training Loss 0.0058462717570364475 Validation Loss 0.005865088663995266\n",
            "Epoch 9: 88 -> Training Loss 0.005774260964244604 Validation Loss 0.005855871364474297\n",
            "Epoch 9: 92 -> Training Loss 0.005846061743795872 Validation Loss 0.005846666172146797\n",
            "Epoch 9: 96 -> Training Loss 0.005783731117844582 Validation Loss 0.005837468430399895\n",
            "Epoch 10: 0 -> Training Loss 0.005819323938339949 Validation Loss 0.005828283727169037\n",
            "Epoch 10: 4 -> Training Loss 0.005782948806881905 Validation Loss 0.005819106940180063\n",
            "Epoch 10: 8 -> Training Loss 0.005791419185698032 Validation Loss 0.0058099390007555485\n",
            "Epoch 10: 12 -> Training Loss 0.00579865463078022 Validation Loss 0.005800777580589056\n",
            "Epoch 10: 16 -> Training Loss 0.005790666677057743 Validation Loss 0.005791625939309597\n",
            "Epoch 10: 20 -> Training Loss 0.005694069433957338 Validation Loss 0.005782485939562321\n",
            "Epoch 10: 24 -> Training Loss 0.0058067976497113705 Validation Loss 0.005773365031927824\n",
            "Epoch 10: 28 -> Training Loss 0.0057352338917553425 Validation Loss 0.005764249712228775\n",
            "Epoch 10: 32 -> Training Loss 0.00572943314909935 Validation Loss 0.005755146034061909\n",
            "Epoch 10: 36 -> Training Loss 0.005681502632796764 Validation Loss 0.005746050737798214\n",
            "Epoch 10: 40 -> Training Loss 0.005701730959117413 Validation Loss 0.005736972205340862\n",
            "Epoch 10: 44 -> Training Loss 0.005724530667066574 Validation Loss 0.0057279025204479694\n",
            "Epoch 10: 48 -> Training Loss 0.0057035163044929504 Validation Loss 0.005718843080103397\n",
            "Epoch 10: 52 -> Training Loss 0.005698531400412321 Validation Loss 0.005709795281291008\n",
            "Epoch 10: 56 -> Training Loss 0.005682026036083698 Validation Loss 0.005700756795704365\n",
            "Epoch 10: 60 -> Training Loss 0.0056074378080666065 Validation Loss 0.005691722966730595\n",
            "Epoch 10: 64 -> Training Loss 0.005649236962199211 Validation Loss 0.005682704970240593\n",
            "Epoch 10: 68 -> Training Loss 0.005639496725052595 Validation Loss 0.005673692561686039\n",
            "Epoch 10: 72 -> Training Loss 0.005614677909761667 Validation Loss 0.005664687603712082\n",
            "Epoch 10: 76 -> Training Loss 0.005605530925095081 Validation Loss 0.005655692890286446\n",
            "Epoch 10: 80 -> Training Loss 0.005612977780401707 Validation Loss 0.005646713078022003\n",
            "Epoch 10: 84 -> Training Loss 0.005635051056742668 Validation Loss 0.005637748166918755\n",
            "Epoch 10: 88 -> Training Loss 0.005581713281571865 Validation Loss 0.005628792103379965\n",
            "Epoch 10: 92 -> Training Loss 0.0055732824839651585 Validation Loss 0.005619847681373358\n",
            "Epoch 10: 96 -> Training Loss 0.005632978864014149 Validation Loss 0.005610917694866657\n",
            "Epoch 11: 0 -> Training Loss 0.005544801242649555 Validation Loss 0.005601995158940554\n",
            "Epoch 11: 4 -> Training Loss 0.005560848396271467 Validation Loss 0.005593081936240196\n",
            "Epoch 11: 8 -> Training Loss 0.005538684315979481 Validation Loss 0.005584174767136574\n",
            "Epoch 11: 12 -> Training Loss 0.0055169956758618355 Validation Loss 0.005575279705226421\n",
            "Epoch 11: 16 -> Training Loss 0.0055428314954042435 Validation Loss 0.005566396750509739\n",
            "Epoch 11: 20 -> Training Loss 0.005523690953850746 Validation Loss 0.005557519383728504\n",
            "Epoch 11: 24 -> Training Loss 0.005485178902745247 Validation Loss 0.005548650398850441\n",
            "Epoch 11: 28 -> Training Loss 0.00553808594122529 Validation Loss 0.00553979491814971\n",
            "Epoch 11: 32 -> Training Loss 0.00543733686208725 Validation Loss 0.005530950613319874\n",
            "Epoch 11: 36 -> Training Loss 0.005473054479807615 Validation Loss 0.005522122140973806\n",
            "Epoch 11: 40 -> Training Loss 0.005494748707860708 Validation Loss 0.005513306707143784\n",
            "Epoch 11: 44 -> Training Loss 0.005499670282006264 Validation Loss 0.005504498723894358\n",
            "Epoch 11: 48 -> Training Loss 0.005481341853737831 Validation Loss 0.0054956963285803795\n",
            "Epoch 11: 52 -> Training Loss 0.005474218167364597 Validation Loss 0.005486908368766308\n",
            "Epoch 11: 56 -> Training Loss 0.005412505939602852 Validation Loss 0.00547813531011343\n",
            "Epoch 11: 60 -> Training Loss 0.005436818115413189 Validation Loss 0.005469373427331448\n",
            "Epoch 11: 64 -> Training Loss 0.005394493229687214 Validation Loss 0.005460615269839764\n",
            "Epoch 11: 68 -> Training Loss 0.00534545723348856 Validation Loss 0.00545186223462224\n",
            "Epoch 11: 72 -> Training Loss 0.005429415497928858 Validation Loss 0.005443125031888485\n",
            "Epoch 11: 76 -> Training Loss 0.005390883423388004 Validation Loss 0.005434395745396614\n",
            "Epoch 11: 80 -> Training Loss 0.0053889392875134945 Validation Loss 0.005425678566098213\n",
            "Epoch 11: 84 -> Training Loss 0.005433995276689529 Validation Loss 0.005416973028331995\n",
            "Epoch 11: 88 -> Training Loss 0.005425527226179838 Validation Loss 0.005408270284533501\n",
            "Epoch 11: 92 -> Training Loss 0.005401027388870716 Validation Loss 0.0053995754569768906\n",
            "Epoch 11: 96 -> Training Loss 0.005360395647585392 Validation Loss 0.0053908913396298885\n",
            "Epoch 12: 0 -> Training Loss 0.005355614237487316 Validation Loss 0.005382216535508633\n",
            "Epoch 12: 4 -> Training Loss 0.005341659765690565 Validation Loss 0.0053735533729195595\n",
            "Epoch 12: 8 -> Training Loss 0.0053357211872935295 Validation Loss 0.0053648995235562325\n",
            "Epoch 12: 12 -> Training Loss 0.005331911146640778 Validation Loss 0.005356254521757364\n",
            "Epoch 12: 16 -> Training Loss 0.005344502627849579 Validation Loss 0.005347619764506817\n",
            "Epoch 12: 20 -> Training Loss 0.005266841500997543 Validation Loss 0.005338996648788452\n",
            "Epoch 12: 24 -> Training Loss 0.005242840386927128 Validation Loss 0.00533038517460227\n",
            "Epoch 12: 28 -> Training Loss 0.005303175188601017 Validation Loss 0.005321785341948271\n",
            "Epoch 12: 32 -> Training Loss 0.005247257184237242 Validation Loss 0.005313193425536156\n",
            "Epoch 12: 36 -> Training Loss 0.005255014635622501 Validation Loss 0.005304610822349787\n",
            "Epoch 12: 40 -> Training Loss 0.00529628386721015 Validation Loss 0.005296037066727877\n",
            "Epoch 12: 44 -> Training Loss 0.0052320207469165325 Validation Loss 0.005287472624331713\n",
            "Epoch 12: 48 -> Training Loss 0.005294597242027521 Validation Loss 0.005278917960822582\n",
            "Epoch 12: 52 -> Training Loss 0.005261099897325039 Validation Loss 0.005270367953926325\n",
            "Epoch 12: 56 -> Training Loss 0.005231956019997597 Validation Loss 0.00526182446628809\n",
            "Epoch 12: 60 -> Training Loss 0.0052921222522854805 Validation Loss 0.0052532898262143135\n",
            "Epoch 12: 64 -> Training Loss 0.005179057363420725 Validation Loss 0.005244761239737272\n",
            "Epoch 12: 68 -> Training Loss 0.005225745495408773 Validation Loss 0.005236244760453701\n",
            "Epoch 12: 72 -> Training Loss 0.005204002372920513 Validation Loss 0.005227734334766865\n",
            "Epoch 12: 76 -> Training Loss 0.005156703293323517 Validation Loss 0.005219233222305775\n",
            "Epoch 12: 80 -> Training Loss 0.0052309585735201836 Validation Loss 0.00521075027063489\n",
            "Epoch 12: 84 -> Training Loss 0.0051887985318899155 Validation Loss 0.005202272441238165\n",
            "Epoch 12: 88 -> Training Loss 0.005152932368218899 Validation Loss 0.005193803459405899\n",
            "Epoch 12: 92 -> Training Loss 0.005121015943586826 Validation Loss 0.005185343325138092\n",
            "Epoch 12: 96 -> Training Loss 0.005118600558489561 Validation Loss 0.005176894832402468\n",
            "Epoch 13: 0 -> Training Loss 0.005115033593028784 Validation Loss 0.005168459378182888\n",
            "Epoch 13: 4 -> Training Loss 0.005119792185723782 Validation Loss 0.0051600332371890545\n",
            "Epoch 13: 8 -> Training Loss 0.005137299187481403 Validation Loss 0.005151615012437105\n",
            "Epoch 13: 12 -> Training Loss 0.005086335353553295 Validation Loss 0.00514320470392704\n",
            "Epoch 13: 16 -> Training Loss 0.005096412263810635 Validation Loss 0.0051348041743040085\n",
            "Epoch 13: 20 -> Training Loss 0.005059150978922844 Validation Loss 0.005126412026584148\n",
            "Epoch 13: 24 -> Training Loss 0.005067542195320129 Validation Loss 0.00511802826076746\n",
            "Epoch 13: 28 -> Training Loss 0.0050804754719138145 Validation Loss 0.005109651945531368\n",
            "Epoch 13: 32 -> Training Loss 0.005047038663178682 Validation Loss 0.005101285874843597\n",
            "Epoch 13: 36 -> Training Loss 0.005041422788053751 Validation Loss 0.005092931911349297\n",
            "Epoch 13: 40 -> Training Loss 0.005026619881391525 Validation Loss 0.005084588658064604\n",
            "Epoch 13: 44 -> Training Loss 0.005018962547183037 Validation Loss 0.005076253321021795\n",
            "Epoch 13: 48 -> Training Loss 0.0050483583472669125 Validation Loss 0.005067928694188595\n",
            "Epoch 13: 52 -> Training Loss 0.004983832128345966 Validation Loss 0.005059609189629555\n",
            "Epoch 13: 56 -> Training Loss 0.005012900568544865 Validation Loss 0.005051302723586559\n",
            "Epoch 13: 60 -> Training Loss 0.005018325522542 Validation Loss 0.005043006967753172\n",
            "Epoch 13: 64 -> Training Loss 0.0050225043669342995 Validation Loss 0.005034720525145531\n",
            "Epoch 13: 68 -> Training Loss 0.005019717384129763 Validation Loss 0.005026438273489475\n",
            "Epoch 13: 72 -> Training Loss 0.00498182000592351 Validation Loss 0.005018162075430155\n",
            "Epoch 13: 76 -> Training Loss 0.004972632043063641 Validation Loss 0.005009894724935293\n",
            "Epoch 13: 80 -> Training Loss 0.00499370601028204 Validation Loss 0.005001632962375879\n",
            "Epoch 13: 84 -> Training Loss 0.004889507777988911 Validation Loss 0.004993376322090626\n",
            "Epoch 13: 88 -> Training Loss 0.004987180233001709 Validation Loss 0.004985134582966566\n",
            "Epoch 13: 92 -> Training Loss 0.004922627471387386 Validation Loss 0.004976900294423103\n",
            "Epoch 13: 96 -> Training Loss 0.004952821880578995 Validation Loss 0.004968677647411823\n",
            "Epoch 14: 0 -> Training Loss 0.004939299076795578 Validation Loss 0.00496046245098114\n",
            "Epoch 14: 4 -> Training Loss 0.00493654515594244 Validation Loss 0.004952257499098778\n",
            "Epoch 14: 8 -> Training Loss 0.0048879897221922874 Validation Loss 0.004944059997797012\n",
            "Epoch 14: 12 -> Training Loss 0.004952761344611645 Validation Loss 0.004935872741043568\n",
            "Epoch 14: 16 -> Training Loss 0.0048860665410757065 Validation Loss 0.0049276938661932945\n",
            "Epoch 14: 20 -> Training Loss 0.004917303565889597 Validation Loss 0.004919524770230055\n",
            "Epoch 14: 24 -> Training Loss 0.004889436066150665 Validation Loss 0.004911363124847412\n",
            "Epoch 14: 28 -> Training Loss 0.004852278158068657 Validation Loss 0.004903208930045366\n",
            "Epoch 14: 32 -> Training Loss 0.004869842901825905 Validation Loss 0.004895065911114216\n",
            "Epoch 14: 36 -> Training Loss 0.004824960604310036 Validation Loss 0.00488693080842495\n",
            "Epoch 14: 40 -> Training Loss 0.004773640539497137 Validation Loss 0.0048788064159452915\n",
            "Epoch 14: 44 -> Training Loss 0.0048294672742486 Validation Loss 0.004870700184255838\n",
            "Epoch 14: 48 -> Training Loss 0.004829132929444313 Validation Loss 0.004862603731453419\n",
            "Epoch 14: 52 -> Training Loss 0.004777390509843826 Validation Loss 0.004854514263570309\n",
            "Epoch 14: 56 -> Training Loss 0.0048270453698933125 Validation Loss 0.004846435971558094\n",
            "Epoch 14: 60 -> Training Loss 0.004809074103832245 Validation Loss 0.004838365130126476\n",
            "Epoch 14: 64 -> Training Loss 0.004786188248544931 Validation Loss 0.004830303136259317\n",
            "Epoch 14: 68 -> Training Loss 0.004763319157063961 Validation Loss 0.00482224952429533\n",
            "Epoch 14: 72 -> Training Loss 0.004786270670592785 Validation Loss 0.004814206622540951\n",
            "Epoch 14: 76 -> Training Loss 0.004788493737578392 Validation Loss 0.004806175362318754\n",
            "Epoch 14: 80 -> Training Loss 0.004826836287975311 Validation Loss 0.0047981468960642815\n",
            "Epoch 14: 84 -> Training Loss 0.004783493001013994 Validation Loss 0.004790120758116245\n",
            "Epoch 14: 88 -> Training Loss 0.004749830812215805 Validation Loss 0.004782105330377817\n",
            "Epoch 14: 92 -> Training Loss 0.004721738398075104 Validation Loss 0.004774100612848997\n",
            "Epoch 14: 96 -> Training Loss 0.0047295656986534595 Validation Loss 0.004766106605529785\n",
            "Epoch 15: 0 -> Training Loss 0.004734845366328955 Validation Loss 0.004758120514452457\n",
            "Epoch 15: 4 -> Training Loss 0.004751985892653465 Validation Loss 0.004750147927552462\n",
            "Epoch 15: 8 -> Training Loss 0.00469109695404768 Validation Loss 0.004742186516523361\n",
            "Epoch 15: 12 -> Training Loss 0.004687578417360783 Validation Loss 0.004734233487397432\n",
            "Epoch 15: 16 -> Training Loss 0.004708774853497744 Validation Loss 0.004726286046206951\n",
            "Epoch 15: 20 -> Training Loss 0.0047009773552417755 Validation Loss 0.004718346521258354\n",
            "Epoch 15: 24 -> Training Loss 0.004681825637817383 Validation Loss 0.004710412584245205\n",
            "Epoch 15: 28 -> Training Loss 0.004660024307668209 Validation Loss 0.004702490754425526\n",
            "Epoch 15: 32 -> Training Loss 0.004701422061771154 Validation Loss 0.004694577772170305\n",
            "Epoch 15: 36 -> Training Loss 0.0046714432537555695 Validation Loss 0.004686668515205383\n",
            "Epoch 15: 40 -> Training Loss 0.004706650506705046 Validation Loss 0.004678765311837196\n",
            "Epoch 15: 44 -> Training Loss 0.004617725498974323 Validation Loss 0.004670870490372181\n",
            "Epoch 15: 48 -> Training Loss 0.004587873816490173 Validation Loss 0.0046629891730844975\n",
            "Epoch 15: 52 -> Training Loss 0.004655314609408379 Validation Loss 0.004655125550925732\n",
            "Epoch 15: 56 -> Training Loss 0.004604949615895748 Validation Loss 0.004647266585379839\n",
            "Epoch 15: 60 -> Training Loss 0.004592440091073513 Validation Loss 0.004639417864382267\n",
            "Epoch 15: 64 -> Training Loss 0.004585084971040487 Validation Loss 0.004631584510207176\n",
            "Epoch 15: 68 -> Training Loss 0.004580116830766201 Validation Loss 0.0046237618662416935\n",
            "Epoch 15: 72 -> Training Loss 0.004561242647469044 Validation Loss 0.004615942016243935\n",
            "Epoch 15: 76 -> Training Loss 0.004582821857184172 Validation Loss 0.004608131945133209\n",
            "Epoch 15: 80 -> Training Loss 0.00457889586687088 Validation Loss 0.004600326996296644\n",
            "Epoch 15: 84 -> Training Loss 0.004551880992949009 Validation Loss 0.004592531360685825\n",
            "Epoch 15: 88 -> Training Loss 0.004545724950730801 Validation Loss 0.004584749694913626\n",
            "Epoch 15: 92 -> Training Loss 0.004514167085289955 Validation Loss 0.004576980601996183\n",
            "Epoch 15: 96 -> Training Loss 0.004542192909866571 Validation Loss 0.004569219425320625\n",
            "Epoch 16: 0 -> Training Loss 0.00449437415227294 Validation Loss 0.004561463836580515\n",
            "Epoch 16: 4 -> Training Loss 0.0045694089494645596 Validation Loss 0.004553716164082289\n",
            "Epoch 16: 8 -> Training Loss 0.004493389278650284 Validation Loss 0.004545972682535648\n",
            "Epoch 16: 12 -> Training Loss 0.0045275078155100346 Validation Loss 0.0045382361859083176\n",
            "Epoch 16: 16 -> Training Loss 0.0045070955529809 Validation Loss 0.004530502483248711\n",
            "Epoch 16: 20 -> Training Loss 0.004505774937570095 Validation Loss 0.004522782750427723\n",
            "Epoch 16: 24 -> Training Loss 0.004483058117330074 Validation Loss 0.0045150755904614925\n",
            "Epoch 16: 28 -> Training Loss 0.004464170895516872 Validation Loss 0.004507380537688732\n",
            "Epoch 16: 32 -> Training Loss 0.0044644540175795555 Validation Loss 0.0044996971264481544\n",
            "Epoch 16: 36 -> Training Loss 0.004467798862606287 Validation Loss 0.00449201837182045\n",
            "Epoch 16: 40 -> Training Loss 0.004451345652341843 Validation Loss 0.004484351258724928\n",
            "Epoch 16: 44 -> Training Loss 0.0044747935608029366 Validation Loss 0.004476701375097036\n",
            "Epoch 16: 48 -> Training Loss 0.004443494603037834 Validation Loss 0.004469061270356178\n",
            "Epoch 16: 52 -> Training Loss 0.004423558712005615 Validation Loss 0.004461431875824928\n",
            "Epoch 16: 56 -> Training Loss 0.004421791061758995 Validation Loss 0.004453804809600115\n",
            "Epoch 16: 60 -> Training Loss 0.0044221533462405205 Validation Loss 0.004446184262633324\n",
            "Epoch 16: 64 -> Training Loss 0.004425491206347942 Validation Loss 0.004438572097569704\n",
            "Epoch 16: 68 -> Training Loss 0.004371587187051773 Validation Loss 0.004430968314409256\n",
            "Epoch 16: 72 -> Training Loss 0.004421211779117584 Validation Loss 0.004423375241458416\n",
            "Epoch 16: 76 -> Training Loss 0.00435870885848999 Validation Loss 0.004415792413055897\n",
            "Epoch 16: 80 -> Training Loss 0.004369702190160751 Validation Loss 0.004408224485814571\n",
            "Epoch 16: 84 -> Training Loss 0.004338688217103481 Validation Loss 0.004400662146508694\n",
            "Epoch 16: 88 -> Training Loss 0.00435012299567461 Validation Loss 0.0043931081891059875\n",
            "Epoch 16: 92 -> Training Loss 0.004394957795739174 Validation Loss 0.0043855588883161545\n",
            "Epoch 16: 96 -> Training Loss 0.0043457080610096455 Validation Loss 0.004378015175461769\n",
            "Epoch 17: 0 -> Training Loss 0.004350939765572548 Validation Loss 0.004370485432446003\n",
            "Epoch 17: 4 -> Training Loss 0.004329144489020109 Validation Loss 0.004362959414720535\n",
            "Epoch 17: 8 -> Training Loss 0.004264572635293007 Validation Loss 0.0043554408475756645\n",
            "Epoch 17: 12 -> Training Loss 0.0043325722217559814 Validation Loss 0.004347938112914562\n",
            "Epoch 17: 16 -> Training Loss 0.004257409833371639 Validation Loss 0.004340441897511482\n",
            "Epoch 17: 20 -> Training Loss 0.004329785238951445 Validation Loss 0.0043329596519470215\n",
            "Epoch 17: 24 -> Training Loss 0.004312184639275074 Validation Loss 0.004325483459979296\n",
            "Epoch 17: 28 -> Training Loss 0.004254532977938652 Validation Loss 0.004318013787269592\n",
            "Epoch 17: 32 -> Training Loss 0.004281205125153065 Validation Loss 0.00431055435910821\n",
            "Epoch 17: 36 -> Training Loss 0.004258886910974979 Validation Loss 0.0043031079694628716\n",
            "Epoch 17: 40 -> Training Loss 0.004224679432809353 Validation Loss 0.004295675083994865\n",
            "Epoch 17: 44 -> Training Loss 0.004289447795599699 Validation Loss 0.004288254305720329\n",
            "Epoch 17: 48 -> Training Loss 0.004277010913938284 Validation Loss 0.0042808400467038155\n",
            "Epoch 17: 52 -> Training Loss 0.0042679160833358765 Validation Loss 0.004273431375622749\n",
            "Epoch 17: 56 -> Training Loss 0.0042136432603001595 Validation Loss 0.004266023635864258\n",
            "Epoch 17: 60 -> Training Loss 0.004230125807225704 Validation Loss 0.0042586238123476505\n",
            "Epoch 17: 64 -> Training Loss 0.004204743076115847 Validation Loss 0.004251234233379364\n",
            "Epoch 17: 68 -> Training Loss 0.004225657321512699 Validation Loss 0.004243859089910984\n",
            "Epoch 17: 72 -> Training Loss 0.004220397211611271 Validation Loss 0.004236497916281223\n",
            "Epoch 17: 76 -> Training Loss 0.004210927523672581 Validation Loss 0.004229139536619186\n",
            "Epoch 17: 80 -> Training Loss 0.004220812581479549 Validation Loss 0.004221789538860321\n",
            "Epoch 17: 84 -> Training Loss 0.004147868603467941 Validation Loss 0.004214451648294926\n",
            "Epoch 17: 88 -> Training Loss 0.004121345933526754 Validation Loss 0.0042071291245520115\n",
            "Epoch 17: 92 -> Training Loss 0.004177509807050228 Validation Loss 0.004199818708002567\n",
            "Epoch 17: 96 -> Training Loss 0.004158245399594307 Validation Loss 0.004192514345049858\n",
            "Epoch 18: 0 -> Training Loss 0.004152866080403328 Validation Loss 0.004185216501355171\n",
            "Epoch 18: 4 -> Training Loss 0.004135288298130035 Validation Loss 0.00417792284861207\n",
            "Epoch 18: 8 -> Training Loss 0.004142472520470619 Validation Loss 0.004170636180788279\n",
            "Epoch 18: 12 -> Training Loss 0.004131878726184368 Validation Loss 0.004163356963545084\n",
            "Epoch 18: 16 -> Training Loss 0.0041177100501954556 Validation Loss 0.004156086593866348\n",
            "Epoch 18: 20 -> Training Loss 0.004087724722921848 Validation Loss 0.004148827400058508\n",
            "Epoch 18: 24 -> Training Loss 0.004105055704712868 Validation Loss 0.004141580313444138\n",
            "Epoch 18: 28 -> Training Loss 0.0041468203999102116 Validation Loss 0.004134343937039375\n",
            "Epoch 18: 32 -> Training Loss 0.004074783995747566 Validation Loss 0.004127113614231348\n",
            "Epoch 18: 36 -> Training Loss 0.004096950404345989 Validation Loss 0.0041198935359716415\n",
            "Epoch 18: 40 -> Training Loss 0.004087521694600582 Validation Loss 0.0041126771830022335\n",
            "Epoch 18: 44 -> Training Loss 0.004137957002967596 Validation Loss 0.004105464089661837\n",
            "Epoch 18: 48 -> Training Loss 0.004081294871866703 Validation Loss 0.004098251461982727\n",
            "Epoch 18: 52 -> Training Loss 0.004087095148861408 Validation Loss 0.004091051872819662\n",
            "Epoch 18: 56 -> Training Loss 0.00406076991930604 Validation Loss 0.004083867184817791\n",
            "Epoch 18: 60 -> Training Loss 0.0040747844614088535 Validation Loss 0.004076692275702953\n",
            "Epoch 18: 64 -> Training Loss 0.004041563719511032 Validation Loss 0.004069525748491287\n",
            "Epoch 18: 68 -> Training Loss 0.004016750957816839 Validation Loss 0.004062371328473091\n",
            "Epoch 18: 72 -> Training Loss 0.003975111525505781 Validation Loss 0.0040552252903580666\n",
            "Epoch 18: 76 -> Training Loss 0.004015210084617138 Validation Loss 0.004048090428113937\n",
            "Epoch 18: 80 -> Training Loss 0.0039795199409127235 Validation Loss 0.0040409620851278305\n",
            "Epoch 18: 84 -> Training Loss 0.00397850014269352 Validation Loss 0.004033845383673906\n",
            "Epoch 18: 88 -> Training Loss 0.004049157723784447 Validation Loss 0.004026737995445728\n",
            "Epoch 18: 92 -> Training Loss 0.004029940813779831 Validation Loss 0.0040196324698626995\n",
            "Epoch 18: 96 -> Training Loss 0.004013441037386656 Validation Loss 0.004012531600892544\n",
            "Epoch 19: 0 -> Training Loss 0.003967528231441975 Validation Loss 0.004005437716841698\n",
            "Epoch 19: 4 -> Training Loss 0.003929535858333111 Validation Loss 0.003998357802629471\n",
            "Epoch 19: 8 -> Training Loss 0.003973961807787418 Validation Loss 0.003991290926933289\n",
            "Epoch 19: 12 -> Training Loss 0.003983768168836832 Validation Loss 0.003984232433140278\n",
            "Epoch 19: 16 -> Training Loss 0.00391467846930027 Validation Loss 0.0039771790616214275\n",
            "Epoch 19: 20 -> Training Loss 0.003967258147895336 Validation Loss 0.003970138728618622\n",
            "Epoch 19: 24 -> Training Loss 0.003921521361917257 Validation Loss 0.003963104449212551\n",
            "Epoch 19: 28 -> Training Loss 0.003930310253053904 Validation Loss 0.0039560748264193535\n",
            "Epoch 19: 32 -> Training Loss 0.0038987742736935616 Validation Loss 0.003949057310819626\n",
            "Epoch 19: 36 -> Training Loss 0.0038823606446385384 Validation Loss 0.0039420523680746555\n",
            "Epoch 19: 40 -> Training Loss 0.0039019156247377396 Validation Loss 0.00393506046384573\n",
            "Epoch 19: 44 -> Training Loss 0.0038846172392368317 Validation Loss 0.003928074613213539\n",
            "Epoch 19: 48 -> Training Loss 0.003852008143439889 Validation Loss 0.003921099007129669\n",
            "Epoch 19: 52 -> Training Loss 0.003917446825653315 Validation Loss 0.0039141299203038216\n",
            "Epoch 19: 56 -> Training Loss 0.003883915953338146 Validation Loss 0.003907160833477974\n",
            "Epoch 19: 60 -> Training Loss 0.003862022189423442 Validation Loss 0.003900202689692378\n",
            "Epoch 19: 64 -> Training Loss 0.0038733957335352898 Validation Loss 0.0038932585157454014\n",
            "Epoch 19: 68 -> Training Loss 0.0038835229352116585 Validation Loss 0.0038863210938870907\n",
            "Epoch 19: 72 -> Training Loss 0.003803035244345665 Validation Loss 0.0038793927524238825\n",
            "Epoch 19: 76 -> Training Loss 0.003862274345010519 Validation Loss 0.0038724772166460752\n",
            "Epoch 19: 80 -> Training Loss 0.0038620138075202703 Validation Loss 0.0038655665703117847\n",
            "Epoch 19: 84 -> Training Loss 0.0038317865692079067 Validation Loss 0.003858662210404873\n",
            "Epoch 19: 88 -> Training Loss 0.003808597568422556 Validation Loss 0.0038517690263688564\n",
            "Epoch 19: 92 -> Training Loss 0.003800408449023962 Validation Loss 0.0038448870182037354\n",
            "Epoch 19: 96 -> Training Loss 0.003787682857364416 Validation Loss 0.003838007804006338\n",
            "Epoch 20: 0 -> Training Loss 0.003778161248192191 Validation Loss 0.0038311355747282505\n",
            "Epoch 20: 4 -> Training Loss 0.0038018773775547743 Validation Loss 0.0038242675364017487\n",
            "Epoch 20: 8 -> Training Loss 0.0037765142042189837 Validation Loss 0.0038174071814864874\n",
            "Epoch 20: 12 -> Training Loss 0.003773775650188327 Validation Loss 0.003810563124716282\n",
            "Epoch 20: 16 -> Training Loss 0.0037682601250708103 Validation Loss 0.003803727449849248\n",
            "Epoch 20: 20 -> Training Loss 0.003827025881037116 Validation Loss 0.00379689852707088\n",
            "Epoch 20: 24 -> Training Loss 0.0037501626648008823 Validation Loss 0.003790074959397316\n",
            "Epoch 20: 28 -> Training Loss 0.003750128671526909 Validation Loss 0.0037832646630704403\n",
            "Epoch 20: 32 -> Training Loss 0.0037643546238541603 Validation Loss 0.0037764625158160925\n",
            "Epoch 20: 36 -> Training Loss 0.003729859134182334 Validation Loss 0.003769665490835905\n",
            "Epoch 20: 40 -> Training Loss 0.0037632607854902744 Validation Loss 0.0037628752179443836\n",
            "Epoch 20: 44 -> Training Loss 0.003757860977202654 Validation Loss 0.003756089136004448\n",
            "Epoch 20: 48 -> Training Loss 0.0036721834912896156 Validation Loss 0.0037493128329515457\n",
            "Epoch 20: 52 -> Training Loss 0.0037200888618826866 Validation Loss 0.003742555622011423\n",
            "Epoch 20: 56 -> Training Loss 0.0036659426987171173 Validation Loss 0.003735802136361599\n",
            "Epoch 20: 60 -> Training Loss 0.0036843009293079376 Validation Loss 0.003729056566953659\n",
            "Epoch 20: 64 -> Training Loss 0.003679185640066862 Validation Loss 0.0037223179824650288\n",
            "Epoch 20: 68 -> Training Loss 0.003699478693306446 Validation Loss 0.0037155896425247192\n",
            "Epoch 20: 72 -> Training Loss 0.0037313371431082487 Validation Loss 0.003708869218826294\n",
            "Epoch 20: 76 -> Training Loss 0.003627768252044916 Validation Loss 0.0037021487951278687\n",
            "Epoch 20: 80 -> Training Loss 0.0037052491679787636 Validation Loss 0.0036954437382519245\n",
            "Epoch 20: 84 -> Training Loss 0.003680533729493618 Validation Loss 0.0036887419410049915\n",
            "Epoch 20: 88 -> Training Loss 0.00364976329728961 Validation Loss 0.0036820489913225174\n",
            "Epoch 20: 92 -> Training Loss 0.0036897382233291864 Validation Loss 0.003675363725051284\n",
            "Epoch 20: 96 -> Training Loss 0.0036606602370738983 Validation Loss 0.0036686831153929234\n",
            "Epoch 21: 0 -> Training Loss 0.0036223512142896652 Validation Loss 0.003662009024992585\n",
            "Epoch 21: 4 -> Training Loss 0.003625571494922042 Validation Loss 0.0036553468089550734\n",
            "Epoch 21: 8 -> Training Loss 0.0036174915730953217 Validation Loss 0.003648699726909399\n",
            "Epoch 21: 12 -> Training Loss 0.0035888124257326126 Validation Loss 0.003642063122242689\n",
            "Epoch 21: 16 -> Training Loss 0.0036017377860844135 Validation Loss 0.0036354362964630127\n",
            "Epoch 21: 20 -> Training Loss 0.0036071590147912502 Validation Loss 0.0036288134288042784\n",
            "Epoch 21: 24 -> Training Loss 0.0036421199329197407 Validation Loss 0.003622196614742279\n",
            "Epoch 21: 28 -> Training Loss 0.00352594000287354 Validation Loss 0.0036155818961560726\n",
            "Epoch 21: 32 -> Training Loss 0.00357327563688159 Validation Loss 0.0036089790519326925\n",
            "Epoch 21: 36 -> Training Loss 0.003567888867110014 Validation Loss 0.0036023808643221855\n",
            "Epoch 21: 40 -> Training Loss 0.0035607698373496532 Validation Loss 0.0035957880318164825\n",
            "Epoch 21: 44 -> Training Loss 0.0035905642434954643 Validation Loss 0.0035892059095203876\n",
            "Epoch 21: 48 -> Training Loss 0.0035937766078859568 Validation Loss 0.003582631703466177\n",
            "Epoch 21: 52 -> Training Loss 0.003552292939275503 Validation Loss 0.0035760621540248394\n",
            "Epoch 21: 56 -> Training Loss 0.0035334955900907516 Validation Loss 0.0035695042461156845\n",
            "Epoch 21: 60 -> Training Loss 0.0034968010149896145 Validation Loss 0.0035629570484161377\n",
            "Epoch 21: 64 -> Training Loss 0.003483921056613326 Validation Loss 0.00355642125941813\n",
            "Epoch 21: 68 -> Training Loss 0.0035143797285854816 Validation Loss 0.0035498980432748795\n",
            "Epoch 21: 72 -> Training Loss 0.003533430863171816 Validation Loss 0.003543376922607422\n",
            "Epoch 21: 76 -> Training Loss 0.003563887206837535 Validation Loss 0.003536864649504423\n",
            "Epoch 21: 80 -> Training Loss 0.003497176105156541 Validation Loss 0.003530357964336872\n",
            "Epoch 21: 84 -> Training Loss 0.0035188905894756317 Validation Loss 0.0035238596610724926\n",
            "Epoch 21: 88 -> Training Loss 0.0034894600976258516 Validation Loss 0.0035173678770661354\n",
            "Epoch 21: 92 -> Training Loss 0.003469458781182766 Validation Loss 0.0035108844749629498\n",
            "Epoch 21: 96 -> Training Loss 0.0034451254177838564 Validation Loss 0.00350440526381135\n",
            "Epoch 22: 0 -> Training Loss 0.003492054995149374 Validation Loss 0.003497937461361289\n",
            "Epoch 22: 4 -> Training Loss 0.003473294898867607 Validation Loss 0.003491479903459549\n",
            "Epoch 22: 8 -> Training Loss 0.003449483076110482 Validation Loss 0.003485032357275486\n",
            "Epoch 22: 12 -> Training Loss 0.003392755286768079 Validation Loss 0.003478597616776824\n",
            "Epoch 22: 16 -> Training Loss 0.003420294728130102 Validation Loss 0.00347217358648777\n",
            "Epoch 22: 20 -> Training Loss 0.003449763637036085 Validation Loss 0.0034657600335776806\n",
            "Epoch 22: 24 -> Training Loss 0.00338210491463542 Validation Loss 0.003459356026723981\n",
            "Epoch 22: 28 -> Training Loss 0.0034331073984503746 Validation Loss 0.003452960168942809\n",
            "Epoch 22: 32 -> Training Loss 0.0034320305567234755 Validation Loss 0.00344656640663743\n",
            "Epoch 22: 36 -> Training Loss 0.0034633479081094265 Validation Loss 0.0034401779994368553\n",
            "Epoch 22: 40 -> Training Loss 0.003381387796252966 Validation Loss 0.0034337909892201424\n",
            "Epoch 22: 44 -> Training Loss 0.003458131570369005 Validation Loss 0.0034274193458259106\n",
            "Epoch 22: 48 -> Training Loss 0.0034006149508059025 Validation Loss 0.0034210493322461843\n",
            "Epoch 22: 52 -> Training Loss 0.0033589445520192385 Validation Loss 0.0034146890975534916\n",
            "Epoch 22: 56 -> Training Loss 0.0033708796836435795 Validation Loss 0.0034083425998687744\n",
            "Epoch 22: 60 -> Training Loss 0.0033466112799942493 Validation Loss 0.0034020012244582176\n",
            "Epoch 22: 64 -> Training Loss 0.003359964583069086 Validation Loss 0.0033956728875637054\n",
            "Epoch 22: 68 -> Training Loss 0.0033541955053806305 Validation Loss 0.0033893571235239506\n",
            "Epoch 22: 72 -> Training Loss 0.003349361242726445 Validation Loss 0.0033830488100647926\n",
            "Epoch 22: 76 -> Training Loss 0.003279605647549033 Validation Loss 0.0033767460845410824\n",
            "Epoch 22: 80 -> Training Loss 0.003373369574546814 Validation Loss 0.003370458958670497\n",
            "Epoch 22: 84 -> Training Loss 0.003307499224320054 Validation Loss 0.0033641729969531298\n",
            "Epoch 22: 88 -> Training Loss 0.003323338460177183 Validation Loss 0.003357889596372843\n",
            "Epoch 22: 92 -> Training Loss 0.0033164136111736298 Validation Loss 0.0033516110852360725\n",
            "Epoch 22: 96 -> Training Loss 0.003340901806950569 Validation Loss 0.003345339559018612\n",
            "Epoch 23: 0 -> Training Loss 0.0033172734547406435 Validation Loss 0.003339072922244668\n",
            "Epoch 23: 4 -> Training Loss 0.003273886628448963 Validation Loss 0.0033328193239867687\n",
            "Epoch 23: 8 -> Training Loss 0.003284851089119911 Validation Loss 0.003326575504615903\n",
            "Epoch 23: 12 -> Training Loss 0.0033066850155591965 Validation Loss 0.0033203386701643467\n",
            "Epoch 23: 16 -> Training Loss 0.003290910506621003 Validation Loss 0.0033141090534627438\n",
            "Epoch 23: 20 -> Training Loss 0.003300015814602375 Validation Loss 0.0033078882843255997\n",
            "Epoch 23: 24 -> Training Loss 0.0032296760473400354 Validation Loss 0.0033016738016158342\n",
            "Epoch 23: 28 -> Training Loss 0.003282752353698015 Validation Loss 0.0032954695634543896\n",
            "Epoch 23: 32 -> Training Loss 0.003297381103038788 Validation Loss 0.0032892716117203236\n",
            "Epoch 23: 36 -> Training Loss 0.003238117787986994 Validation Loss 0.0032830804120749235\n",
            "Epoch 23: 40 -> Training Loss 0.003275699447840452 Validation Loss 0.003276902250945568\n",
            "Epoch 23: 44 -> Training Loss 0.0032108575105667114 Validation Loss 0.003270723856985569\n",
            "Epoch 23: 48 -> Training Loss 0.003239390207454562 Validation Loss 0.0032645557075738907\n",
            "Epoch 23: 52 -> Training Loss 0.0032554795034229755 Validation Loss 0.0032583950087428093\n",
            "Epoch 23: 56 -> Training Loss 0.003203155007213354 Validation Loss 0.0032522398978471756\n",
            "Epoch 23: 60 -> Training Loss 0.003207739442586899 Validation Loss 0.0032460978254675865\n",
            "Epoch 23: 64 -> Training Loss 0.003205927787348628 Validation Loss 0.003239959944039583\n",
            "Epoch 23: 68 -> Training Loss 0.003253929316997528 Validation Loss 0.003233828581869602\n",
            "Epoch 23: 72 -> Training Loss 0.003202735912054777 Validation Loss 0.003227698151022196\n",
            "Epoch 23: 76 -> Training Loss 0.0031657505314797163 Validation Loss 0.003221582155674696\n",
            "Epoch 23: 80 -> Training Loss 0.0031908643431961536 Validation Loss 0.0032154808286577463\n",
            "Epoch 23: 84 -> Training Loss 0.0031775659881532192 Validation Loss 0.0032093836925923824\n",
            "Epoch 23: 88 -> Training Loss 0.0032090172171592712 Validation Loss 0.003203292842954397\n",
            "Epoch 23: 92 -> Training Loss 0.003152434714138508 Validation Loss 0.003197205252945423\n",
            "Epoch 23: 96 -> Training Loss 0.003127939533442259 Validation Loss 0.00319112790748477\n",
            "Epoch 24: 0 -> Training Loss 0.003160740016028285 Validation Loss 0.0031850647646933794\n",
            "Epoch 24: 4 -> Training Loss 0.0031304126605391502 Validation Loss 0.003179009072482586\n",
            "Epoch 24: 8 -> Training Loss 0.003127405419945717 Validation Loss 0.0031729629263281822\n",
            "Epoch 24: 12 -> Training Loss 0.0031519881449639797 Validation Loss 0.003166923299431801\n",
            "Epoch 24: 16 -> Training Loss 0.003152105025947094 Validation Loss 0.00316088879480958\n",
            "Epoch 24: 20 -> Training Loss 0.0031234431080520153 Validation Loss 0.0031548638362437487\n",
            "Epoch 24: 24 -> Training Loss 0.003065361175686121 Validation Loss 0.0031488505192101\n",
            "Epoch 24: 28 -> Training Loss 0.003083294490352273 Validation Loss 0.0031428481452167034\n",
            "Epoch 24: 32 -> Training Loss 0.003109107492491603 Validation Loss 0.0031368532218039036\n",
            "Epoch 24: 36 -> Training Loss 0.0031164917163550854 Validation Loss 0.0031308638863265514\n",
            "Epoch 24: 40 -> Training Loss 0.0031201140955090523 Validation Loss 0.003124877344816923\n",
            "Epoch 24: 44 -> Training Loss 0.003112527308985591 Validation Loss 0.0031188956927508116\n",
            "Epoch 24: 48 -> Training Loss 0.003094283863902092 Validation Loss 0.0031129210256040096\n",
            "Epoch 24: 52 -> Training Loss 0.003060893854126334 Validation Loss 0.0031069559045135975\n",
            "Epoch 24: 56 -> Training Loss 0.0030287341214716434 Validation Loss 0.0031010054517537355\n",
            "Epoch 24: 60 -> Training Loss 0.0030237899627536535 Validation Loss 0.003095061983913183\n",
            "Epoch 24: 64 -> Training Loss 0.0030344550032168627 Validation Loss 0.003089125966653228\n",
            "Epoch 24: 68 -> Training Loss 0.003071350511163473 Validation Loss 0.0030831985641270876\n",
            "Epoch 24: 72 -> Training Loss 0.003052180167287588 Validation Loss 0.003077274188399315\n",
            "Epoch 24: 76 -> Training Loss 0.003031850326806307 Validation Loss 0.0030713574960827827\n",
            "Epoch 24: 80 -> Training Loss 0.0030092094093561172 Validation Loss 0.0030654477886855602\n",
            "Epoch 24: 84 -> Training Loss 0.003005261765792966 Validation Loss 0.003059545997530222\n",
            "Epoch 24: 88 -> Training Loss 0.002997888019308448 Validation Loss 0.0030536523554474115\n",
            "Epoch 24: 92 -> Training Loss 0.003015511203557253 Validation Loss 0.0030477670952677727\n",
            "Epoch 24: 96 -> Training Loss 0.003022408578544855 Validation Loss 0.0030418913811445236\n",
            "Epoch 25: 0 -> Training Loss 0.0030263736844062805 Validation Loss 0.0030360217206180096\n",
            "Epoch 25: 4 -> Training Loss 0.003003929741680622 Validation Loss 0.003030152525752783\n",
            "Epoch 25: 8 -> Training Loss 0.0029931662138551474 Validation Loss 0.0030242896173149347\n",
            "Epoch 25: 12 -> Training Loss 0.0029979348182678223 Validation Loss 0.0030184355564415455\n",
            "Epoch 25: 16 -> Training Loss 0.002927550580352545 Validation Loss 0.0030125887133181095\n",
            "Epoch 25: 20 -> Training Loss 0.002991459099575877 Validation Loss 0.0030067525804042816\n",
            "Epoch 25: 24 -> Training Loss 0.002977169118821621 Validation Loss 0.0030009185429662466\n",
            "Epoch 25: 28 -> Training Loss 0.0029309820383787155 Validation Loss 0.0029950900934636593\n",
            "Epoch 25: 32 -> Training Loss 0.0029363613575696945 Validation Loss 0.0029892718885093927\n",
            "Epoch 25: 36 -> Training Loss 0.0029590348713099957 Validation Loss 0.002983463928103447\n",
            "Epoch 25: 40 -> Training Loss 0.0029230720829218626 Validation Loss 0.002977664116770029\n",
            "Epoch 25: 44 -> Training Loss 0.002920342143625021 Validation Loss 0.00297187315300107\n",
            "Epoch 25: 48 -> Training Loss 0.002957059070467949 Validation Loss 0.002966091502457857\n",
            "Epoch 25: 52 -> Training Loss 0.00291835842654109 Validation Loss 0.0029603159055113792\n",
            "Epoch 25: 56 -> Training Loss 0.002906165085732937 Validation Loss 0.0029545470606535673\n",
            "Epoch 25: 60 -> Training Loss 0.0029710130766034126 Validation Loss 0.0029487842693924904\n",
            "Epoch 25: 64 -> Training Loss 0.0029451262671500444 Validation Loss 0.0029430235736072063\n",
            "Epoch 25: 68 -> Training Loss 0.0029081758111715317 Validation Loss 0.0029372689314186573\n",
            "Epoch 25: 72 -> Training Loss 0.002947065979242325 Validation Loss 0.002931523136794567\n",
            "Epoch 25: 76 -> Training Loss 0.0028851996175944805 Validation Loss 0.0029257815331220627\n",
            "Epoch 25: 80 -> Training Loss 0.0028774337843060493 Validation Loss 0.00292004831135273\n",
            "Epoch 25: 84 -> Training Loss 0.002898161532357335 Validation Loss 0.002914323005825281\n",
            "Epoch 25: 88 -> Training Loss 0.002848842879757285 Validation Loss 0.002908602822571993\n",
            "Epoch 25: 92 -> Training Loss 0.0028356099501252174 Validation Loss 0.0029028933495283127\n",
            "Epoch 25: 96 -> Training Loss 0.0028970700222998857 Validation Loss 0.0028971980791538954\n",
            "Epoch 26: 0 -> Training Loss 0.0028911251574754715 Validation Loss 0.002891508862376213\n",
            "Epoch 26: 4 -> Training Loss 0.0028437671717256308 Validation Loss 0.0028858226723968983\n",
            "Epoch 26: 8 -> Training Loss 0.0028693792410194874 Validation Loss 0.0028801457956433296\n",
            "Epoch 26: 12 -> Training Loss 0.0028445653151720762 Validation Loss 0.002874475670978427\n",
            "Epoch 26: 16 -> Training Loss 0.0028471602126955986 Validation Loss 0.0028688120655715466\n",
            "Epoch 26: 20 -> Training Loss 0.0027928680647164583 Validation Loss 0.0028631570748984814\n",
            "Epoch 26: 24 -> Training Loss 0.0028424528427422047 Validation Loss 0.0028575104661285877\n",
            "Epoch 26: 28 -> Training Loss 0.0028566045220941305 Validation Loss 0.002851864555850625\n",
            "Epoch 26: 32 -> Training Loss 0.0028067994862794876 Validation Loss 0.002846219576895237\n",
            "Epoch 26: 36 -> Training Loss 0.0028351652435958385 Validation Loss 0.002840586705133319\n",
            "Epoch 26: 40 -> Training Loss 0.0028039137832820415 Validation Loss 0.0028349636122584343\n",
            "Epoch 26: 44 -> Training Loss 0.0028156437911093235 Validation Loss 0.002829349134117365\n",
            "Epoch 26: 48 -> Training Loss 0.0027699549682438374 Validation Loss 0.002823743037879467\n",
            "Epoch 26: 52 -> Training Loss 0.0027982424944639206 Validation Loss 0.002818142995238304\n",
            "Epoch 26: 56 -> Training Loss 0.0027976895216852427 Validation Loss 0.002812543883919716\n",
            "Epoch 26: 60 -> Training Loss 0.002795260399580002 Validation Loss 0.00280695129185915\n",
            "Epoch 26: 64 -> Training Loss 0.002781616523861885 Validation Loss 0.00280136545188725\n",
            "Epoch 26: 68 -> Training Loss 0.002781661693006754 Validation Loss 0.0027957879938185215\n",
            "Epoch 26: 72 -> Training Loss 0.002757319947704673 Validation Loss 0.0027902182191610336\n",
            "Epoch 26: 76 -> Training Loss 0.002772551728412509 Validation Loss 0.002784662414342165\n",
            "Epoch 26: 80 -> Training Loss 0.002763096708804369 Validation Loss 0.002779114292934537\n",
            "Epoch 26: 84 -> Training Loss 0.002755550667643547 Validation Loss 0.0027735750190913677\n",
            "Epoch 26: 88 -> Training Loss 0.0027587066870182753 Validation Loss 0.002768040867522359\n",
            "Epoch 26: 92 -> Training Loss 0.002770138205960393 Validation Loss 0.0027625123038887978\n",
            "Epoch 26: 96 -> Training Loss 0.0027204621583223343 Validation Loss 0.002756985370069742\n",
            "Epoch 27: 0 -> Training Loss 0.0027323076501488686 Validation Loss 0.0027514700777828693\n",
            "Epoch 27: 4 -> Training Loss 0.002714554313570261 Validation Loss 0.002745963167399168\n",
            "Epoch 27: 8 -> Training Loss 0.0026452532038092613 Validation Loss 0.002740461379289627\n",
            "Epoch 27: 12 -> Training Loss 0.002717628376558423 Validation Loss 0.002734971232712269\n",
            "Epoch 27: 16 -> Training Loss 0.0026931948959827423 Validation Loss 0.0027294885367155075\n",
            "Epoch 27: 20 -> Training Loss 0.00268126861192286 Validation Loss 0.0027240151539444923\n",
            "Epoch 27: 24 -> Training Loss 0.0026472387835383415 Validation Loss 0.0027185475919395685\n",
            "Epoch 27: 28 -> Training Loss 0.0027110471855849028 Validation Loss 0.0027130860835313797\n",
            "Epoch 27: 32 -> Training Loss 0.0026631327345967293 Validation Loss 0.0027076341211795807\n",
            "Epoch 27: 36 -> Training Loss 0.002697072457522154 Validation Loss 0.0027021951973438263\n",
            "Epoch 27: 40 -> Training Loss 0.002662204671651125 Validation Loss 0.0026967586018145084\n",
            "Epoch 27: 44 -> Training Loss 0.0026483519468456507 Validation Loss 0.00269132643006742\n",
            "Epoch 27: 48 -> Training Loss 0.0026838514022529125 Validation Loss 0.002685903338715434\n",
            "Epoch 27: 52 -> Training Loss 0.0026684210170060396 Validation Loss 0.0026804842054843903\n",
            "Epoch 27: 56 -> Training Loss 0.002621591556817293 Validation Loss 0.002675070893019438\n",
            "Epoch 27: 60 -> Training Loss 0.0026334095746278763 Validation Loss 0.0026696682907640934\n",
            "Epoch 27: 64 -> Training Loss 0.0026063015684485435 Validation Loss 0.0026642680168151855\n",
            "Epoch 27: 68 -> Training Loss 0.0026129717007279396 Validation Loss 0.0026588779874145985\n",
            "Epoch 27: 72 -> Training Loss 0.002622530795633793 Validation Loss 0.002653496339917183\n",
            "Epoch 27: 76 -> Training Loss 0.0026076617650687695 Validation Loss 0.0026481219101697206\n",
            "Epoch 27: 80 -> Training Loss 0.0026176548562943935 Validation Loss 0.0026427535340189934\n",
            "Epoch 27: 84 -> Training Loss 0.0025748107582330704 Validation Loss 0.002637390047311783\n",
            "Epoch 27: 88 -> Training Loss 0.0025850466918200254 Validation Loss 0.0026320340111851692\n",
            "Epoch 27: 92 -> Training Loss 0.0026024971157312393 Validation Loss 0.00262668589130044\n",
            "Epoch 27: 96 -> Training Loss 0.002573669422417879 Validation Loss 0.0026213484816253185\n",
            "Epoch 28: 0 -> Training Loss 0.0026227429043501616 Validation Loss 0.002616021316498518\n",
            "Epoch 28: 4 -> Training Loss 0.0025688474997878075 Validation Loss 0.0026106981094926596\n",
            "Epoch 28: 8 -> Training Loss 0.0025782031007111073 Validation Loss 0.0026053839828819036\n",
            "Epoch 28: 12 -> Training Loss 0.002611000556498766 Validation Loss 0.0026000780053436756\n",
            "Epoch 28: 16 -> Training Loss 0.002554511884227395 Validation Loss 0.0025947741232812405\n",
            "Epoch 28: 20 -> Training Loss 0.002560556400567293 Validation Loss 0.0025894807185977697\n",
            "Epoch 28: 24 -> Training Loss 0.0025129951536655426 Validation Loss 0.0025841989554464817\n",
            "Epoch 28: 28 -> Training Loss 0.0025458524469286203 Validation Loss 0.0025789286009967327\n",
            "Epoch 28: 32 -> Training Loss 0.0025500499177724123 Validation Loss 0.0025736610405147076\n",
            "Epoch 28: 36 -> Training Loss 0.002548803575336933 Validation Loss 0.002568398602306843\n",
            "Epoch 28: 40 -> Training Loss 0.0025421997997909784 Validation Loss 0.002563142217695713\n",
            "Epoch 28: 44 -> Training Loss 0.002515518106520176 Validation Loss 0.0025578890927135944\n",
            "Epoch 28: 48 -> Training Loss 0.0025271696504205465 Validation Loss 0.0025526448152959347\n",
            "Epoch 28: 52 -> Training Loss 0.0025387033820152283 Validation Loss 0.002547402633354068\n",
            "Epoch 28: 56 -> Training Loss 0.0025426975917071104 Validation Loss 0.0025421660393476486\n",
            "Epoch 28: 60 -> Training Loss 0.0024959330912679434 Validation Loss 0.002536941319704056\n",
            "Epoch 28: 64 -> Training Loss 0.002506156684830785 Validation Loss 0.0025317282415926456\n",
            "Epoch 28: 68 -> Training Loss 0.0025023892521858215 Validation Loss 0.0025265212170779705\n",
            "Epoch 28: 72 -> Training Loss 0.002513614483177662 Validation Loss 0.002521321177482605\n",
            "Epoch 28: 76 -> Training Loss 0.002468450926244259 Validation Loss 0.002516130916774273\n",
            "Epoch 28: 80 -> Training Loss 0.002492303494364023 Validation Loss 0.002510952763259411\n",
            "Epoch 28: 84 -> Training Loss 0.0025005715433508158 Validation Loss 0.002505776472389698\n",
            "Epoch 28: 88 -> Training Loss 0.002482960233464837 Validation Loss 0.0025006055366247892\n",
            "Epoch 28: 92 -> Training Loss 0.002462692093104124 Validation Loss 0.002495442982763052\n",
            "Epoch 28: 96 -> Training Loss 0.0024588839150965214 Validation Loss 0.002490291139110923\n",
            "Epoch 29: 0 -> Training Loss 0.002484831726178527 Validation Loss 0.0024851453490555286\n",
            "Epoch 29: 4 -> Training Loss 0.002444938290864229 Validation Loss 0.0024800049141049385\n",
            "Epoch 29: 8 -> Training Loss 0.002469509607180953 Validation Loss 0.0024748733267188072\n",
            "Epoch 29: 12 -> Training Loss 0.002452841028571129 Validation Loss 0.002469751052558422\n",
            "Epoch 29: 16 -> Training Loss 0.0024397512897849083 Validation Loss 0.002464636228978634\n",
            "Epoch 29: 20 -> Training Loss 0.002409870270639658 Validation Loss 0.002459525130689144\n",
            "Epoch 29: 24 -> Training Loss 0.00241809687577188 Validation Loss 0.002454421017318964\n",
            "Epoch 29: 28 -> Training Loss 0.00244249333627522 Validation Loss 0.002449324354529381\n",
            "Epoch 29: 32 -> Training Loss 0.0024395128712058067 Validation Loss 0.002444232814013958\n",
            "Epoch 29: 36 -> Training Loss 0.0024734449107199907 Validation Loss 0.002439146162942052\n",
            "Epoch 29: 40 -> Training Loss 0.0023940326645970345 Validation Loss 0.002434058813378215\n",
            "Epoch 29: 44 -> Training Loss 0.0024299798533320427 Validation Loss 0.0024289812427014112\n",
            "Epoch 29: 48 -> Training Loss 0.002403317019343376 Validation Loss 0.0024239132180809975\n",
            "Epoch 29: 52 -> Training Loss 0.0023917765356600285 Validation Loss 0.00241885450668633\n",
            "Epoch 29: 56 -> Training Loss 0.002408161759376526 Validation Loss 0.0024138069711625576\n",
            "Epoch 29: 60 -> Training Loss 0.0023961965925991535 Validation Loss 0.00240876292809844\n",
            "Epoch 29: 64 -> Training Loss 0.0023578708060085773 Validation Loss 0.002403722144663334\n",
            "Epoch 29: 68 -> Training Loss 0.0023743619676679373 Validation Loss 0.002398689277470112\n",
            "Epoch 29: 72 -> Training Loss 0.0023726867511868477 Validation Loss 0.002393665025010705\n",
            "Epoch 29: 76 -> Training Loss 0.002348181791603565 Validation Loss 0.0023886454291641712\n",
            "Epoch 29: 80 -> Training Loss 0.00236727693118155 Validation Loss 0.002383634215220809\n",
            "Epoch 29: 84 -> Training Loss 0.002371361246332526 Validation Loss 0.002378630917519331\n",
            "Epoch 29: 88 -> Training Loss 0.002375618787482381 Validation Loss 0.0023736334405839443\n",
            "Epoch 29: 92 -> Training Loss 0.0023225611075758934 Validation Loss 0.002368638990446925\n",
            "Epoch 29: 96 -> Training Loss 0.0023261122405529022 Validation Loss 0.0023636529222130775\n",
            "Epoch 30: 0 -> Training Loss 0.0023210386279970407 Validation Loss 0.002358675003051758\n",
            "Epoch 30: 4 -> Training Loss 0.0023306594230234623 Validation Loss 0.002353707328438759\n",
            "Epoch 30: 8 -> Training Loss 0.0023036955390125513 Validation Loss 0.002348748967051506\n",
            "Epoch 30: 12 -> Training Loss 0.0023363446816802025 Validation Loss 0.0023438003845512867\n",
            "Epoch 30: 16 -> Training Loss 0.0022784254979342222 Validation Loss 0.002338854596018791\n",
            "Epoch 30: 20 -> Training Loss 0.002325269393622875 Validation Loss 0.0023339218460023403\n",
            "Epoch 30: 24 -> Training Loss 0.002317539183422923 Validation Loss 0.0023289944510906935\n",
            "Epoch 30: 28 -> Training Loss 0.002278928877785802 Validation Loss 0.002324075438082218\n",
            "Epoch 30: 32 -> Training Loss 0.0022950423881411552 Validation Loss 0.0023191659711301327\n",
            "Epoch 30: 36 -> Training Loss 0.0023200176656246185 Validation Loss 0.0023142597638070583\n",
            "Epoch 30: 40 -> Training Loss 0.0022522667422890663 Validation Loss 0.0023093577474355698\n",
            "Epoch 30: 44 -> Training Loss 0.0022786413319408894 Validation Loss 0.00230446457862854\n",
            "Epoch 30: 48 -> Training Loss 0.002303824760019779 Validation Loss 0.0022995793260633945\n",
            "Epoch 30: 52 -> Training Loss 0.0022761672735214233 Validation Loss 0.002294698264449835\n",
            "Epoch 30: 56 -> Training Loss 0.002298797247931361 Validation Loss 0.002289821160957217\n",
            "Epoch 30: 60 -> Training Loss 0.002289053751155734 Validation Loss 0.0022849473170936108\n",
            "Epoch 30: 64 -> Training Loss 0.0022765009198337793 Validation Loss 0.002280081622302532\n",
            "Epoch 30: 68 -> Training Loss 0.0022423500195145607 Validation Loss 0.0022752233780920506\n",
            "Epoch 30: 72 -> Training Loss 0.0022331932559609413 Validation Loss 0.002270372351631522\n",
            "Epoch 30: 76 -> Training Loss 0.002262987196445465 Validation Loss 0.002265532035380602\n",
            "Epoch 30: 80 -> Training Loss 0.0022408387158066034 Validation Loss 0.0022606963757425547\n",
            "Epoch 30: 84 -> Training Loss 0.00223812204785645 Validation Loss 0.0022558686323463917\n",
            "Epoch 30: 88 -> Training Loss 0.002264037262648344 Validation Loss 0.002251049503684044\n",
            "Epoch 30: 92 -> Training Loss 0.0022078899201005697 Validation Loss 0.002246232470497489\n",
            "Epoch 30: 96 -> Training Loss 0.0022133057937026024 Validation Loss 0.0022414226550608873\n",
            "Epoch 31: 0 -> Training Loss 0.0022026672959327698 Validation Loss 0.0022366163320839405\n",
            "Epoch 31: 4 -> Training Loss 0.0021881461143493652 Validation Loss 0.0022318155970424414\n",
            "Epoch 31: 8 -> Training Loss 0.0021950630471110344 Validation Loss 0.0022270248737186193\n",
            "Epoch 31: 12 -> Training Loss 0.002221891190856695 Validation Loss 0.002222242299467325\n",
            "Epoch 31: 16 -> Training Loss 0.0021943403407931328 Validation Loss 0.002217466477304697\n",
            "Epoch 31: 20 -> Training Loss 0.002210398903116584 Validation Loss 0.0022126995027065277\n",
            "Epoch 31: 24 -> Training Loss 0.002200817223638296 Validation Loss 0.002207939513027668\n",
            "Epoch 31: 28 -> Training Loss 0.00218855869024992 Validation Loss 0.00220318790525198\n",
            "Epoch 31: 32 -> Training Loss 0.0021552739199250937 Validation Loss 0.002198442816734314\n",
            "Epoch 31: 36 -> Training Loss 0.0021652360446751118 Validation Loss 0.002193709369748831\n",
            "Epoch 31: 40 -> Training Loss 0.002152056433260441 Validation Loss 0.0021889815106987953\n",
            "Epoch 31: 44 -> Training Loss 0.002134779468178749 Validation Loss 0.002184262266382575\n",
            "Epoch 31: 48 -> Training Loss 0.0021890588104724884 Validation Loss 0.0021795532666146755\n",
            "Epoch 31: 52 -> Training Loss 0.0021619214676320553 Validation Loss 0.0021748470608145\n",
            "Epoch 31: 56 -> Training Loss 0.0021552713587880135 Validation Loss 0.002170150401070714\n",
            "Epoch 31: 60 -> Training Loss 0.0021361876279115677 Validation Loss 0.0021654602605849504\n",
            "Epoch 31: 64 -> Training Loss 0.002125075552612543 Validation Loss 0.0021607785020023584\n",
            "Epoch 31: 68 -> Training Loss 0.0021107555367052555 Validation Loss 0.0021561034955084324\n",
            "Epoch 31: 72 -> Training Loss 0.0021385159343481064 Validation Loss 0.002151430817320943\n",
            "Epoch 31: 76 -> Training Loss 0.00212874636054039 Validation Loss 0.002146762330085039\n",
            "Epoch 31: 80 -> Training Loss 0.0021361280232667923 Validation Loss 0.002142100129276514\n",
            "Epoch 31: 84 -> Training Loss 0.0021209518890827894 Validation Loss 0.002137443982064724\n",
            "Epoch 31: 88 -> Training Loss 0.0021146240178495646 Validation Loss 0.002132792491465807\n",
            "Epoch 31: 92 -> Training Loss 0.002079937607049942 Validation Loss 0.0021281465888023376\n",
            "Epoch 31: 96 -> Training Loss 0.002098417840898037 Validation Loss 0.0021235093008726835\n",
            "Epoch 32: 0 -> Training Loss 0.002107240492478013 Validation Loss 0.0021188801620155573\n",
            "Epoch 32: 4 -> Training Loss 0.00203983997926116 Validation Loss 0.0021142547484487295\n",
            "Epoch 32: 8 -> Training Loss 0.002079836092889309 Validation Loss 0.002109638648107648\n",
            "Epoch 32: 12 -> Training Loss 0.0020629679784178734 Validation Loss 0.0021050292998552322\n",
            "Epoch 32: 16 -> Training Loss 0.0020769608672708273 Validation Loss 0.0021004262380301952\n",
            "Epoch 32: 20 -> Training Loss 0.002072644652798772 Validation Loss 0.002095830161124468\n",
            "Epoch 32: 24 -> Training Loss 0.0020644827745854855 Validation Loss 0.002091239672154188\n",
            "Epoch 32: 28 -> Training Loss 0.0020621097646653652 Validation Loss 0.002086654771119356\n",
            "Epoch 32: 32 -> Training Loss 0.002078589517623186 Validation Loss 0.0020820810459554195\n",
            "Epoch 32: 36 -> Training Loss 0.0019967108964920044 Validation Loss 0.0020775143057107925\n",
            "Epoch 32: 40 -> Training Loss 0.0020266869105398655 Validation Loss 0.0020729582756757736\n",
            "Epoch 32: 44 -> Training Loss 0.00206430209800601 Validation Loss 0.0020684064365923405\n",
            "Epoch 32: 48 -> Training Loss 0.0020109398756176233 Validation Loss 0.002063858788460493\n",
            "Epoch 32: 52 -> Training Loss 0.0020211138762533665 Validation Loss 0.0020593199878931046\n",
            "Epoch 32: 56 -> Training Loss 0.002009666757658124 Validation Loss 0.0020547877065837383\n",
            "Epoch 32: 60 -> Training Loss 0.002034964971244335 Validation Loss 0.00205026101320982\n",
            "Epoch 32: 64 -> Training Loss 0.0020238496363162994 Validation Loss 0.0020457387436181307\n",
            "Epoch 32: 68 -> Training Loss 0.0020148588810116053 Validation Loss 0.0020412197336554527\n",
            "Epoch 32: 72 -> Training Loss 0.0020009358413517475 Validation Loss 0.002036706544458866\n",
            "Epoch 32: 76 -> Training Loss 0.0019690408371388912 Validation Loss 0.0020322005730122328\n",
            "Epoch 32: 80 -> Training Loss 0.002027633134275675 Validation Loss 0.002027700189501047\n",
            "Epoch 32: 84 -> Training Loss 0.0020152186043560505 Validation Loss 0.0020232032984495163\n",
            "Epoch 32: 88 -> Training Loss 0.001983871217817068 Validation Loss 0.0020187124609947205\n",
            "Epoch 32: 92 -> Training Loss 0.001981689827516675 Validation Loss 0.0020142304711043835\n",
            "Epoch 32: 96 -> Training Loss 0.0019506998360157013 Validation Loss 0.0020097580272704363\n",
            "Epoch 33: 0 -> Training Loss 0.0020080730319023132 Validation Loss 0.002005293732509017\n",
            "Epoch 33: 4 -> Training Loss 0.001969308825209737 Validation Loss 0.002000831998884678\n",
            "Epoch 33: 8 -> Training Loss 0.001977535430341959 Validation Loss 0.0019963784143328667\n",
            "Epoch 33: 12 -> Training Loss 0.0019606712739914656 Validation Loss 0.001991931116208434\n",
            "Epoch 33: 16 -> Training Loss 0.0019436568254604936 Validation Loss 0.001987491501495242\n",
            "Epoch 33: 20 -> Training Loss 0.001959246816113591 Validation Loss 0.001983059337362647\n",
            "Epoch 33: 24 -> Training Loss 0.0019482455682009459 Validation Loss 0.001978632528334856\n",
            "Epoch 33: 28 -> Training Loss 0.0019632666371762753 Validation Loss 0.0019742120057344437\n",
            "Epoch 33: 32 -> Training Loss 0.001959742046892643 Validation Loss 0.0019697989337146282\n",
            "Epoch 33: 36 -> Training Loss 0.0019507352262735367 Validation Loss 0.001965387025848031\n",
            "Epoch 33: 40 -> Training Loss 0.001956104300916195 Validation Loss 0.0019609734881669283\n",
            "Epoch 33: 44 -> Training Loss 0.0019253517966717482 Validation Loss 0.001956566935405135\n",
            "Epoch 33: 48 -> Training Loss 0.0019274596124887466 Validation Loss 0.0019521729554980993\n",
            "Epoch 33: 52 -> Training Loss 0.0019417532021179795 Validation Loss 0.001947787357494235\n",
            "Epoch 33: 56 -> Training Loss 0.0019232557388022542 Validation Loss 0.0019434061832726002\n",
            "Epoch 33: 60 -> Training Loss 0.0019260324770584702 Validation Loss 0.0019390340894460678\n",
            "Epoch 33: 64 -> Training Loss 0.0019149044528603554 Validation Loss 0.001934670377522707\n",
            "Epoch 33: 68 -> Training Loss 0.001889134175144136 Validation Loss 0.001930313534103334\n",
            "Epoch 33: 72 -> Training Loss 0.0018868343904614449 Validation Loss 0.0019259677501395345\n",
            "Epoch 33: 76 -> Training Loss 0.0019108986016362906 Validation Loss 0.0019216301152482629\n",
            "Epoch 33: 80 -> Training Loss 0.0018968419171869755 Validation Loss 0.001917295390740037\n",
            "Epoch 33: 84 -> Training Loss 0.0018789605237543583 Validation Loss 0.0019129663705825806\n",
            "Epoch 33: 88 -> Training Loss 0.0018888479098677635 Validation Loss 0.0019086441025137901\n",
            "Epoch 33: 92 -> Training Loss 0.0019076673779636621 Validation Loss 0.0019043275387957692\n",
            "Epoch 33: 96 -> Training Loss 0.0018628824036568403 Validation Loss 0.0019000133033841848\n",
            "Epoch 34: 0 -> Training Loss 0.0018652374856173992 Validation Loss 0.0018957068677991629\n",
            "Epoch 34: 4 -> Training Loss 0.0018703967798501253 Validation Loss 0.0018914071843028069\n",
            "Epoch 34: 8 -> Training Loss 0.0018581213662400842 Validation Loss 0.001887112739495933\n",
            "Epoch 34: 12 -> Training Loss 0.0018415618687868118 Validation Loss 0.0018828288884833455\n",
            "Epoch 34: 16 -> Training Loss 0.0018589308019727468 Validation Loss 0.0018785522552207112\n",
            "Epoch 34: 20 -> Training Loss 0.0018568672239780426 Validation Loss 0.0018742799293249846\n",
            "Epoch 34: 24 -> Training Loss 0.0018657025648280978 Validation Loss 0.0018700177315622568\n",
            "Epoch 34: 28 -> Training Loss 0.0018583617638796568 Validation Loss 0.0018657635664567351\n",
            "Epoch 34: 32 -> Training Loss 0.0018487197812646627 Validation Loss 0.0018615119624882936\n",
            "Epoch 34: 36 -> Training Loss 0.0018467485206201673 Validation Loss 0.0018572642002254725\n",
            "Epoch 34: 40 -> Training Loss 0.0018356973305344582 Validation Loss 0.001853021327406168\n",
            "Epoch 34: 44 -> Training Loss 0.001831292873248458 Validation Loss 0.0018487854395061731\n",
            "Epoch 34: 48 -> Training Loss 0.0018085401970893145 Validation Loss 0.0018445533933117986\n",
            "Epoch 34: 52 -> Training Loss 0.001816349453292787 Validation Loss 0.001840328797698021\n",
            "Epoch 34: 56 -> Training Loss 0.001797253848053515 Validation Loss 0.0018361153779551387\n",
            "Epoch 34: 60 -> Training Loss 0.0017816945910453796 Validation Loss 0.0018319095252081752\n",
            "Epoch 34: 64 -> Training Loss 0.0017851232551038265 Validation Loss 0.0018277140334248543\n",
            "Epoch 34: 68 -> Training Loss 0.0018024889286607504 Validation Loss 0.0018235233146697283\n",
            "Epoch 34: 72 -> Training Loss 0.0017888423753902316 Validation Loss 0.0018193365540355444\n",
            "Epoch 34: 76 -> Training Loss 0.0018268418498337269 Validation Loss 0.0018151567783206701\n",
            "Epoch 34: 80 -> Training Loss 0.0017485299613326788 Validation Loss 0.0018109773518517613\n",
            "Epoch 34: 84 -> Training Loss 0.0017711508553475142 Validation Loss 0.0018068088684231043\n",
            "Epoch 34: 88 -> Training Loss 0.0017665625782683492 Validation Loss 0.0018026470206677914\n",
            "Epoch 34: 92 -> Training Loss 0.0017842449015006423 Validation Loss 0.0017984940204769373\n",
            "Epoch 34: 96 -> Training Loss 0.0017705652862787247 Validation Loss 0.0017943482380360365\n",
            "Epoch 35: 0 -> Training Loss 0.0018079322762787342 Validation Loss 0.0017902092076838017\n",
            "Epoch 35: 4 -> Training Loss 0.001777388621121645 Validation Loss 0.001786071341484785\n",
            "Epoch 35: 8 -> Training Loss 0.0017290633404627442 Validation Loss 0.0017819367349147797\n",
            "Epoch 35: 12 -> Training Loss 0.0017530114855617285 Validation Loss 0.001777810975909233\n",
            "Epoch 35: 16 -> Training Loss 0.0017416534246876836 Validation Loss 0.0017736922018229961\n",
            "Epoch 35: 20 -> Training Loss 0.0017593156080693007 Validation Loss 0.0017695790156722069\n",
            "Epoch 35: 24 -> Training Loss 0.0017673238180577755 Validation Loss 0.0017654732801020145\n",
            "Epoch 35: 28 -> Training Loss 0.0017385268583893776 Validation Loss 0.001761372433975339\n",
            "Epoch 35: 32 -> Training Loss 0.0017180661670863628 Validation Loss 0.0017572795040905476\n",
            "Epoch 35: 36 -> Training Loss 0.0017741958145052195 Validation Loss 0.0017531993798911572\n",
            "Epoch 35: 40 -> Training Loss 0.001720750005915761 Validation Loss 0.0017491212347522378\n",
            "Epoch 35: 44 -> Training Loss 0.0017163038719445467 Validation Loss 0.001745048793964088\n",
            "Epoch 35: 48 -> Training Loss 0.0016974847530946136 Validation Loss 0.0017409824067726731\n",
            "Epoch 35: 52 -> Training Loss 0.0017012753523886204 Validation Loss 0.0017369226552546024\n",
            "Epoch 35: 56 -> Training Loss 0.0017395586473867297 Validation Loss 0.0017328668618574739\n",
            "Epoch 35: 60 -> Training Loss 0.0017285688081756234 Validation Loss 0.0017288094386458397\n",
            "Epoch 35: 64 -> Training Loss 0.0016971846343949437 Validation Loss 0.0017247559735551476\n",
            "Epoch 35: 68 -> Training Loss 0.0017190640792250633 Validation Loss 0.001720712287351489\n",
            "Epoch 35: 72 -> Training Loss 0.0016976252663880587 Validation Loss 0.0017166764009743929\n",
            "Epoch 35: 76 -> Training Loss 0.0016754374373704195 Validation Loss 0.0017126459861174226\n",
            "Epoch 35: 80 -> Training Loss 0.0016719239065423608 Validation Loss 0.0017086237203329802\n",
            "Epoch 35: 84 -> Training Loss 0.0016975607722997665 Validation Loss 0.0017046063439920545\n",
            "Epoch 35: 88 -> Training Loss 0.0016987714916467667 Validation Loss 0.0017005892004817724\n",
            "Epoch 35: 92 -> Training Loss 0.001654655090533197 Validation Loss 0.0016965800896286964\n",
            "Epoch 35: 96 -> Training Loss 0.0016939014894887805 Validation Loss 0.0016925808740779757\n",
            "Epoch 36: 0 -> Training Loss 0.0016602324321866035 Validation Loss 0.0016885866643860936\n",
            "Epoch 36: 4 -> Training Loss 0.001664835261180997 Validation Loss 0.001684598159044981\n",
            "Epoch 36: 8 -> Training Loss 0.0016410131938755512 Validation Loss 0.001680614659562707\n",
            "Epoch 36: 12 -> Training Loss 0.0016290433704853058 Validation Loss 0.0016766374465078115\n",
            "Epoch 36: 16 -> Training Loss 0.0016813187394291162 Validation Loss 0.0016726715257391334\n",
            "Epoch 36: 20 -> Training Loss 0.001652103615924716 Validation Loss 0.001668710494413972\n",
            "Epoch 36: 24 -> Training Loss 0.001625914010219276 Validation Loss 0.0016647572629153728\n",
            "Epoch 36: 28 -> Training Loss 0.0016469396650791168 Validation Loss 0.0016608129953965545\n",
            "Epoch 36: 32 -> Training Loss 0.001618948532268405 Validation Loss 0.0016568731516599655\n",
            "Epoch 36: 36 -> Training Loss 0.0016508095432072878 Validation Loss 0.0016529414569959044\n",
            "Epoch 36: 40 -> Training Loss 0.0016198869561776519 Validation Loss 0.0016490097623318434\n",
            "Epoch 36: 44 -> Training Loss 0.0016202307306230068 Validation Loss 0.0016450864495709538\n",
            "Epoch 36: 48 -> Training Loss 0.0016269999323412776 Validation Loss 0.0016411719843745232\n",
            "Epoch 36: 52 -> Training Loss 0.0016371402889490128 Validation Loss 0.0016372633399441838\n",
            "Epoch 36: 56 -> Training Loss 0.001612327410839498 Validation Loss 0.0016333572566509247\n",
            "Epoch 36: 60 -> Training Loss 0.0015997305745258927 Validation Loss 0.001629456877708435\n",
            "Epoch 36: 64 -> Training Loss 0.0016133332392200828 Validation Loss 0.0016255646478384733\n",
            "Epoch 36: 68 -> Training Loss 0.0015997609589248896 Validation Loss 0.0016216770745813847\n",
            "Epoch 36: 72 -> Training Loss 0.0016315167304128408 Validation Loss 0.0016177953220903873\n",
            "Epoch 36: 76 -> Training Loss 0.0015776606742292643 Validation Loss 0.0016139165963977575\n",
            "Epoch 36: 80 -> Training Loss 0.001608554506674409 Validation Loss 0.0016100499778985977\n",
            "Epoch 36: 84 -> Training Loss 0.0015868168557062745 Validation Loss 0.0016061885980889201\n",
            "Epoch 36: 88 -> Training Loss 0.0015693820314481854 Validation Loss 0.0016023304779082537\n",
            "Epoch 36: 92 -> Training Loss 0.0016148043796420097 Validation Loss 0.001598477945663035\n",
            "Epoch 36: 96 -> Training Loss 0.00158252555411309 Validation Loss 0.0015946319326758385\n",
            "Epoch 37: 0 -> Training Loss 0.0015585727524012327 Validation Loss 0.0015907925553619862\n",
            "Epoch 37: 4 -> Training Loss 0.0015762571711093187 Validation Loss 0.0015869608614593744\n",
            "Epoch 37: 8 -> Training Loss 0.0015946213388815522 Validation Loss 0.0015831368509680033\n",
            "Epoch 37: 12 -> Training Loss 0.001569619053043425 Validation Loss 0.0015793138882145286\n",
            "Epoch 37: 16 -> Training Loss 0.0015478209825232625 Validation Loss 0.0015754986088722944\n",
            "Epoch 37: 20 -> Training Loss 0.001552204368636012 Validation Loss 0.001571692992001772\n",
            "Epoch 37: 24 -> Training Loss 0.001565596554428339 Validation Loss 0.0015678915660828352\n",
            "Epoch 37: 28 -> Training Loss 0.0015838074032217264 Validation Loss 0.0015640957280993462\n",
            "Epoch 37: 32 -> Training Loss 0.0015473805833607912 Validation Loss 0.0015603001229465008\n",
            "Epoch 37: 36 -> Training Loss 0.0015003718435764313 Validation Loss 0.0015565097564831376\n",
            "Epoch 37: 40 -> Training Loss 0.0015508331125602126 Validation Loss 0.0015527327777817845\n",
            "Epoch 37: 44 -> Training Loss 0.0015205867821350694 Validation Loss 0.0015489617362618446\n",
            "Epoch 37: 48 -> Training Loss 0.0014898140216246247 Validation Loss 0.001545196631923318\n",
            "Epoch 37: 52 -> Training Loss 0.0014999142149463296 Validation Loss 0.001541439676657319\n",
            "Epoch 37: 56 -> Training Loss 0.0015305066481232643 Validation Loss 0.0015376927331089973\n",
            "Epoch 37: 60 -> Training Loss 0.0015391811029985547 Validation Loss 0.0015339503297582269\n",
            "Epoch 37: 64 -> Training Loss 0.0015193206490948796 Validation Loss 0.0015302103711292148\n",
            "Epoch 37: 68 -> Training Loss 0.0015342960832640529 Validation Loss 0.0015264736721292138\n",
            "Epoch 37: 72 -> Training Loss 0.0014941005501896143 Validation Loss 0.0015227374387905002\n",
            "Epoch 37: 76 -> Training Loss 0.0014916686341166496 Validation Loss 0.0015190097037702799\n",
            "Epoch 37: 80 -> Training Loss 0.0014875708147883415 Validation Loss 0.00151528837159276\n",
            "Epoch 37: 84 -> Training Loss 0.0014793691225349903 Validation Loss 0.0015115751884877682\n",
            "Epoch 37: 88 -> Training Loss 0.0014844144461676478 Validation Loss 0.0015078689903020859\n",
            "Epoch 37: 92 -> Training Loss 0.0014961035922169685 Validation Loss 0.001504169194959104\n",
            "Epoch 37: 96 -> Training Loss 0.0014905435964465141 Validation Loss 0.0015004739398136735\n",
            "Epoch 38: 0 -> Training Loss 0.0014578599948436022 Validation Loss 0.0014967815950512886\n",
            "Epoch 38: 4 -> Training Loss 0.001466106390580535 Validation Loss 0.0014930967008695006\n",
            "Epoch 38: 8 -> Training Loss 0.0014855875633656979 Validation Loss 0.0014894166961312294\n",
            "Epoch 38: 12 -> Training Loss 0.001466242945753038 Validation Loss 0.0014857403002679348\n",
            "Epoch 38: 16 -> Training Loss 0.0014650416560471058 Validation Loss 0.0014820713549852371\n",
            "Epoch 38: 20 -> Training Loss 0.0014498811215162277 Validation Loss 0.0014784078812226653\n",
            "Epoch 38: 24 -> Training Loss 0.001460500992834568 Validation Loss 0.0014747532550245523\n",
            "Epoch 38: 28 -> Training Loss 0.001475258730351925 Validation Loss 0.0014711059629917145\n",
            "Epoch 38: 32 -> Training Loss 0.0014796219766139984 Validation Loss 0.0014674642588943243\n",
            "Epoch 38: 36 -> Training Loss 0.001483612577430904 Validation Loss 0.0014638231368735433\n",
            "Epoch 38: 40 -> Training Loss 0.0014277476584538817 Validation Loss 0.0014601843431591988\n",
            "Epoch 38: 44 -> Training Loss 0.00142768828663975 Validation Loss 0.0014565566089004278\n",
            "Epoch 38: 48 -> Training Loss 0.0014277056325227022 Validation Loss 0.0014529356267303228\n",
            "Epoch 38: 52 -> Training Loss 0.0014399001374840736 Validation Loss 0.0014493209309875965\n",
            "Epoch 38: 56 -> Training Loss 0.0014163977466523647 Validation Loss 0.0014457085635513067\n",
            "Epoch 38: 60 -> Training Loss 0.0014201914891600609 Validation Loss 0.0014421057421714067\n",
            "Epoch 38: 64 -> Training Loss 0.0014451591996476054 Validation Loss 0.00143850885797292\n",
            "Epoch 38: 68 -> Training Loss 0.0014133886434137821 Validation Loss 0.0014349122066050768\n",
            "Epoch 38: 72 -> Training Loss 0.0014097916427999735 Validation Loss 0.0014313219580799341\n",
            "Epoch 38: 76 -> Training Loss 0.0013997736386954784 Validation Loss 0.0014277432346716523\n",
            "Epoch 38: 80 -> Training Loss 0.0014218747382983565 Validation Loss 0.0014241733588278294\n",
            "Epoch 38: 84 -> Training Loss 0.0013941614888608456 Validation Loss 0.0014206029009073973\n",
            "Epoch 38: 88 -> Training Loss 0.0014345053350552917 Validation Loss 0.0014170394279062748\n",
            "Epoch 38: 92 -> Training Loss 0.0013777782442048192 Validation Loss 0.001413480844348669\n",
            "Epoch 38: 96 -> Training Loss 0.001361936330795288 Validation Loss 0.0014099304098635912\n",
            "Epoch 39: 0 -> Training Loss 0.0013596359640359879 Validation Loss 0.0014063897542655468\n",
            "Epoch 39: 4 -> Training Loss 0.0013711045030504465 Validation Loss 0.0014028570149093866\n",
            "Epoch 39: 8 -> Training Loss 0.001388571341522038 Validation Loss 0.001399332657456398\n",
            "Epoch 39: 12 -> Training Loss 0.0013692565262317657 Validation Loss 0.0013958107447251678\n",
            "Epoch 39: 16 -> Training Loss 0.0013573103351518512 Validation Loss 0.0013922934886068106\n",
            "Epoch 39: 20 -> Training Loss 0.0013767320197075605 Validation Loss 0.0013887769309803843\n",
            "Epoch 39: 24 -> Training Loss 0.001359668094664812 Validation Loss 0.0013852663105353713\n",
            "Epoch 39: 28 -> Training Loss 0.0013669232139363885 Validation Loss 0.0013817624421790242\n",
            "Epoch 39: 32 -> Training Loss 0.0013606712454929948 Validation Loss 0.0013782625319436193\n",
            "Epoch 39: 36 -> Training Loss 0.0013609875459223986 Validation Loss 0.0013747734483331442\n",
            "Epoch 39: 40 -> Training Loss 0.001348666730336845 Validation Loss 0.0013712935615330935\n",
            "Epoch 39: 44 -> Training Loss 0.001324102864600718 Validation Loss 0.0013678190298378468\n",
            "Epoch 39: 48 -> Training Loss 0.0013349100481718779 Validation Loss 0.0013643542770296335\n",
            "Epoch 39: 52 -> Training Loss 0.0013169455341994762 Validation Loss 0.0013608972076326609\n",
            "Epoch 39: 56 -> Training Loss 0.0013185169082134962 Validation Loss 0.0013574433978646994\n",
            "Epoch 39: 60 -> Training Loss 0.001336500747129321 Validation Loss 0.0013539972715079784\n",
            "Epoch 39: 64 -> Training Loss 0.0013269624905660748 Validation Loss 0.0013505544047802687\n",
            "Epoch 39: 68 -> Training Loss 0.0013934130547568202 Validation Loss 0.0013471131678670645\n",
            "Epoch 39: 72 -> Training Loss 0.0013147321296855807 Validation Loss 0.0013436696026474237\n",
            "Epoch 39: 76 -> Training Loss 0.001328816288150847 Validation Loss 0.0013402365148067474\n",
            "Epoch 39: 80 -> Training Loss 0.001314819324761629 Validation Loss 0.00133681227453053\n",
            "Epoch 39: 84 -> Training Loss 0.0013279883423820138 Validation Loss 0.0013333961833268404\n",
            "Epoch 39: 88 -> Training Loss 0.0013110970612615347 Validation Loss 0.001329984748736024\n",
            "Epoch 39: 92 -> Training Loss 0.001342564239166677 Validation Loss 0.0013265786692500114\n",
            "Epoch 39: 96 -> Training Loss 0.0012881695292890072 Validation Loss 0.0013231736375018954\n",
            "Epoch 40: 0 -> Training Loss 0.0012939772568643093 Validation Loss 0.00131977500859648\n",
            "Epoch 40: 4 -> Training Loss 0.001311478903517127 Validation Loss 0.0013163824332877994\n",
            "Epoch 40: 8 -> Training Loss 0.0012984778732061386 Validation Loss 0.0013129948638379574\n",
            "Epoch 40: 12 -> Training Loss 0.001291369553655386 Validation Loss 0.001309612882323563\n",
            "Epoch 40: 16 -> Training Loss 0.0013045388041064143 Validation Loss 0.0013062367215752602\n",
            "Epoch 40: 20 -> Training Loss 0.0012774282367900014 Validation Loss 0.0013028655666857958\n",
            "Epoch 40: 24 -> Training Loss 0.001279870280995965 Validation Loss 0.0012995046563446522\n",
            "Epoch 40: 28 -> Training Loss 0.0012625902891159058 Validation Loss 0.0012961532920598984\n",
            "Epoch 40: 32 -> Training Loss 0.0012851665960624814 Validation Loss 0.0012928054202347994\n",
            "Epoch 40: 36 -> Training Loss 0.0013005424989387393 Validation Loss 0.0012894617393612862\n",
            "Epoch 40: 40 -> Training Loss 0.001271296525374055 Validation Loss 0.0012861210852861404\n",
            "Epoch 40: 44 -> Training Loss 0.0012825067387893796 Validation Loss 0.0012827885802835226\n",
            "Epoch 40: 48 -> Training Loss 0.0012642808724194765 Validation Loss 0.0012794591020792723\n",
            "Epoch 40: 52 -> Training Loss 0.00127188372425735 Validation Loss 0.0012761340476572514\n",
            "Epoch 40: 56 -> Training Loss 0.0012820260599255562 Validation Loss 0.0012728115543723106\n",
            "Epoch 40: 60 -> Training Loss 0.0012645046226680279 Validation Loss 0.0012694897595793009\n",
            "Epoch 40: 64 -> Training Loss 0.0012412532232701778 Validation Loss 0.0012661757646128535\n",
            "Epoch 40: 68 -> Training Loss 0.0012672031298279762 Validation Loss 0.0012628710828721523\n",
            "Epoch 40: 72 -> Training Loss 0.00125437555834651 Validation Loss 0.0012595690786838531\n",
            "Epoch 40: 76 -> Training Loss 0.001275357324630022 Validation Loss 0.0012562707997858524\n",
            "Epoch 40: 80 -> Training Loss 0.0012459855061024427 Validation Loss 0.001252979738637805\n",
            "Epoch 40: 84 -> Training Loss 0.0012280911905691028 Validation Loss 0.0012496979907155037\n",
            "Epoch 40: 88 -> Training Loss 0.0012057814747095108 Validation Loss 0.0012464257888495922\n",
            "Epoch 40: 92 -> Training Loss 0.0012026313925161958 Validation Loss 0.0012431603390723467\n",
            "Epoch 40: 96 -> Training Loss 0.001228518784046173 Validation Loss 0.0012399007100611925\n",
            "Epoch 41: 0 -> Training Loss 0.0012307470897212625 Validation Loss 0.0012366474838927388\n",
            "Epoch 41: 4 -> Training Loss 0.0012381256092339754 Validation Loss 0.0012333989143371582\n",
            "Epoch 41: 8 -> Training Loss 0.0012052514357492328 Validation Loss 0.0012301532551646233\n",
            "Epoch 41: 12 -> Training Loss 0.0012103170156478882 Validation Loss 0.001226914580911398\n",
            "Epoch 41: 16 -> Training Loss 0.0012259489158168435 Validation Loss 0.001223684987053275\n",
            "Epoch 41: 20 -> Training Loss 0.001212946604937315 Validation Loss 0.0012204605154693127\n",
            "Epoch 41: 24 -> Training Loss 0.001216721604578197 Validation Loss 0.0012172404676675797\n",
            "Epoch 41: 28 -> Training Loss 0.001183673506602645 Validation Loss 0.0012140236794948578\n",
            "Epoch 41: 32 -> Training Loss 0.001203430350869894 Validation Loss 0.0012108155060559511\n",
            "Epoch 41: 36 -> Training Loss 0.0012211694847792387 Validation Loss 0.0012076094280928373\n",
            "Epoch 41: 40 -> Training Loss 0.0011904584243893623 Validation Loss 0.0012044075410813093\n",
            "Epoch 41: 44 -> Training Loss 0.0011770944111049175 Validation Loss 0.0012012134538963437\n",
            "Epoch 41: 48 -> Training Loss 0.0011588286142796278 Validation Loss 0.0011980263516306877\n",
            "Epoch 41: 52 -> Training Loss 0.0011613692622631788 Validation Loss 0.001194850541651249\n",
            "Epoch 41: 56 -> Training Loss 0.0011826881673187017 Validation Loss 0.0011916829971596599\n",
            "Epoch 41: 60 -> Training Loss 0.0011864162515848875 Validation Loss 0.0011885218555107713\n",
            "Epoch 41: 64 -> Training Loss 0.0011605815961956978 Validation Loss 0.001185367233119905\n",
            "Epoch 41: 68 -> Training Loss 0.0011498138774186373 Validation Loss 0.0011822220403701067\n",
            "Epoch 41: 72 -> Training Loss 0.0011858908692374825 Validation Loss 0.0011790860444307327\n",
            "Epoch 41: 76 -> Training Loss 0.0011600919533520937 Validation Loss 0.001175949815660715\n",
            "Epoch 41: 80 -> Training Loss 0.0011690444080159068 Validation Loss 0.0011728161480277777\n",
            "Epoch 41: 84 -> Training Loss 0.0011397376656532288 Validation Loss 0.0011696857400238514\n",
            "Epoch 41: 88 -> Training Loss 0.0011527396272867918 Validation Loss 0.001166562084108591\n",
            "Epoch 41: 92 -> Training Loss 0.00114335585385561 Validation Loss 0.00116344029083848\n",
            "Epoch 41: 96 -> Training Loss 0.0011588750639930367 Validation Loss 0.0011603247839957476\n",
            "Epoch 42: 0 -> Training Loss 0.001153278280980885 Validation Loss 0.0011572141665965319\n",
            "Epoch 42: 4 -> Training Loss 0.0011395810870453715 Validation Loss 0.0011541058775037527\n",
            "Epoch 42: 8 -> Training Loss 0.0011466343421489 Validation Loss 0.0011510038748383522\n",
            "Epoch 42: 12 -> Training Loss 0.001128742704167962 Validation Loss 0.0011479061795398593\n",
            "Epoch 42: 16 -> Training Loss 0.001132831210270524 Validation Loss 0.0011448166333138943\n",
            "Epoch 42: 20 -> Training Loss 0.0011546174064278603 Validation Loss 0.0011417323257774115\n",
            "Epoch 42: 24 -> Training Loss 0.0011365496320649981 Validation Loss 0.001138649065978825\n",
            "Epoch 42: 28 -> Training Loss 0.001137003069743514 Validation Loss 0.0011355725582689047\n",
            "Epoch 42: 32 -> Training Loss 0.0011306061642244458 Validation Loss 0.0011325007071718574\n",
            "Epoch 42: 36 -> Training Loss 0.001137525076046586 Validation Loss 0.001129436888732016\n",
            "Epoch 42: 40 -> Training Loss 0.0010992189636453986 Validation Loss 0.0011263773776590824\n",
            "Epoch 42: 44 -> Training Loss 0.001121091889217496 Validation Loss 0.0011233241530135274\n",
            "Epoch 42: 48 -> Training Loss 0.0011012401664629579 Validation Loss 0.0011202744208276272\n",
            "Epoch 42: 52 -> Training Loss 0.001102132024243474 Validation Loss 0.0011172335362061858\n",
            "Epoch 42: 56 -> Training Loss 0.00111218576785177 Validation Loss 0.0011141994036734104\n",
            "Epoch 42: 60 -> Training Loss 0.001086496515199542 Validation Loss 0.0011111682979390025\n",
            "Epoch 42: 64 -> Training Loss 0.0010820208117365837 Validation Loss 0.0011081472039222717\n",
            "Epoch 42: 68 -> Training Loss 0.0011016128119081259 Validation Loss 0.0011051320470869541\n",
            "Epoch 42: 72 -> Training Loss 0.0010947172995656729 Validation Loss 0.0011021210812032223\n",
            "Epoch 42: 76 -> Training Loss 0.0010799176525324583 Validation Loss 0.0010991140734404325\n",
            "Epoch 42: 80 -> Training Loss 0.0010719321435317397 Validation Loss 0.001096114981919527\n",
            "Epoch 42: 84 -> Training Loss 0.0010776566341519356 Validation Loss 0.001093124272301793\n",
            "Epoch 42: 88 -> Training Loss 0.0011066922452300787 Validation Loss 0.0010901361238211393\n",
            "Epoch 42: 92 -> Training Loss 0.0010854813735932112 Validation Loss 0.0010871466947719455\n",
            "Epoch 42: 96 -> Training Loss 0.0010634909849613905 Validation Loss 0.001084161689504981\n",
            "Epoch 43: 0 -> Training Loss 0.0010668111499398947 Validation Loss 0.0010811875108629465\n",
            "Epoch 43: 4 -> Training Loss 0.001076413900591433 Validation Loss 0.0010782184544950724\n",
            "Epoch 43: 8 -> Training Loss 0.0010727548506110907 Validation Loss 0.0010752531234174967\n",
            "Epoch 43: 12 -> Training Loss 0.0010641037952154875 Validation Loss 0.001072293147444725\n",
            "Epoch 43: 16 -> Training Loss 0.0010464724618941545 Validation Loss 0.0010693364311009645\n",
            "Epoch 43: 20 -> Training Loss 0.001057005487382412 Validation Loss 0.0010663882130756974\n",
            "Epoch 43: 24 -> Training Loss 0.0010554093169048429 Validation Loss 0.0010634437203407288\n",
            "Epoch 43: 28 -> Training Loss 0.00102945975959301 Validation Loss 0.001060505281202495\n",
            "Epoch 43: 32 -> Training Loss 0.0010489365085959435 Validation Loss 0.0010575721971690655\n",
            "Epoch 43: 36 -> Training Loss 0.0010464650113135576 Validation Loss 0.0010546427220106125\n",
            "Epoch 43: 40 -> Training Loss 0.0010242153657600284 Validation Loss 0.0010517208138480783\n",
            "Epoch 43: 44 -> Training Loss 0.0010324842296540737 Validation Loss 0.0010488093830645084\n",
            "Epoch 43: 48 -> Training Loss 0.00102138367947191 Validation Loss 0.0010459036566317081\n",
            "Epoch 43: 52 -> Training Loss 0.0010191320907324553 Validation Loss 0.0010430049151182175\n",
            "Epoch 43: 56 -> Training Loss 0.001033770153298974 Validation Loss 0.0010401125764474273\n",
            "Epoch 43: 60 -> Training Loss 0.0010116577614098787 Validation Loss 0.0010372247779741883\n",
            "Epoch 43: 64 -> Training Loss 0.0010413739364594221 Validation Loss 0.0010343423346057534\n",
            "Epoch 43: 68 -> Training Loss 0.001003261306323111 Validation Loss 0.0010314590763300657\n",
            "Epoch 43: 72 -> Training Loss 0.0009982045739889145 Validation Loss 0.0010285826865583658\n",
            "Epoch 43: 76 -> Training Loss 0.001012084074318409 Validation Loss 0.0010257165413349867\n",
            "Epoch 43: 80 -> Training Loss 0.0010029016993939877 Validation Loss 0.0010228585451841354\n",
            "Epoch 43: 84 -> Training Loss 0.0009742736583575606 Validation Loss 0.0010200025280937552\n",
            "Epoch 43: 88 -> Training Loss 0.0010207348968833685 Validation Loss 0.001017157337628305\n",
            "Epoch 43: 92 -> Training Loss 0.0010041604982689023 Validation Loss 0.001014312612824142\n",
            "Epoch 43: 96 -> Training Loss 0.0009823722066357732 Validation Loss 0.0010114723118022084\n",
            "Epoch 44: 0 -> Training Loss 0.001009191619232297 Validation Loss 0.0010086419060826302\n",
            "Epoch 44: 4 -> Training Loss 0.0009902280289679766 Validation Loss 0.0010058146435767412\n",
            "Epoch 44: 8 -> Training Loss 0.0009878674754872918 Validation Loss 0.001002992270514369\n",
            "Epoch 44: 12 -> Training Loss 0.001002380158752203 Validation Loss 0.0010001752525568008\n",
            "Epoch 44: 16 -> Training Loss 0.0009795194491744041 Validation Loss 0.000997360097244382\n",
            "Epoch 44: 20 -> Training Loss 0.0009933898691087961 Validation Loss 0.000994550297036767\n",
            "Epoch 44: 24 -> Training Loss 0.0009770988253876567 Validation Loss 0.0009917442221194506\n",
            "Epoch 44: 28 -> Training Loss 0.0009748055599629879 Validation Loss 0.0009889457141980529\n",
            "Epoch 44: 32 -> Training Loss 0.0009731561876833439 Validation Loss 0.0009861569851636887\n",
            "Epoch 44: 36 -> Training Loss 0.0009856167016550899 Validation Loss 0.000983371981419623\n",
            "Epoch 44: 40 -> Training Loss 0.0009624699014239013 Validation Loss 0.0009805905865505338\n",
            "Epoch 44: 44 -> Training Loss 0.0009612290887162089 Validation Loss 0.0009778145467862487\n",
            "Epoch 44: 48 -> Training Loss 0.0009582954226061702 Validation Loss 0.0009750423487275839\n",
            "Epoch 44: 52 -> Training Loss 0.0009478346910327673 Validation Loss 0.0009722771355882287\n",
            "Epoch 44: 56 -> Training Loss 0.0009657192276790738 Validation Loss 0.0009695172775536776\n",
            "Epoch 44: 60 -> Training Loss 0.0009523104527033865 Validation Loss 0.0009667616104707122\n",
            "Epoch 44: 64 -> Training Loss 0.0009666476980783045 Validation Loss 0.0009640082134865224\n",
            "Epoch 44: 68 -> Training Loss 0.0009419473353773355 Validation Loss 0.0009612602880224586\n",
            "Epoch 44: 72 -> Training Loss 0.0009261410450562835 Validation Loss 0.0009585245861671865\n",
            "Epoch 44: 76 -> Training Loss 0.0009692282183095813 Validation Loss 0.0009557978482916951\n",
            "Epoch 44: 80 -> Training Loss 0.0009242102969437838 Validation Loss 0.0009530733805149794\n",
            "Epoch 44: 84 -> Training Loss 0.0009253872558474541 Validation Loss 0.0009503549663349986\n",
            "Epoch 44: 88 -> Training Loss 0.0009381577256135643 Validation Loss 0.0009476457489654422\n",
            "Epoch 44: 92 -> Training Loss 0.0009292005561292171 Validation Loss 0.0009449395583942533\n",
            "Epoch 44: 96 -> Training Loss 0.00091884087305516 Validation Loss 0.0009422381408512592\n",
            "Epoch 45: 0 -> Training Loss 0.0009141307673417032 Validation Loss 0.0009395403321832418\n",
            "Epoch 45: 4 -> Training Loss 0.0009552802075631917 Validation Loss 0.0009368442697450519\n",
            "Epoch 45: 8 -> Training Loss 0.0009226110996678472 Validation Loss 0.0009341505356132984\n",
            "Epoch 45: 12 -> Training Loss 0.0009288317523896694 Validation Loss 0.0009314622730016708\n",
            "Epoch 45: 16 -> Training Loss 0.0009229087736457586 Validation Loss 0.0009287801804021001\n",
            "Epoch 45: 20 -> Training Loss 0.0008892177138477564 Validation Loss 0.0009261051309294999\n",
            "Epoch 45: 24 -> Training Loss 0.0009195447200909257 Validation Loss 0.0009234404424205422\n",
            "Epoch 45: 28 -> Training Loss 0.0009112829575315118 Validation Loss 0.0009207827388308942\n",
            "Epoch 45: 32 -> Training Loss 0.0008940671687014401 Validation Loss 0.0009181296918541193\n",
            "Epoch 45: 36 -> Training Loss 0.0009114028653129935 Validation Loss 0.0009154794970527291\n",
            "Epoch 45: 40 -> Training Loss 0.0008968189358711243 Validation Loss 0.0009128324454650283\n",
            "Epoch 45: 44 -> Training Loss 0.0009026422048918903 Validation Loss 0.0009101909236051142\n",
            "Epoch 45: 48 -> Training Loss 0.0009009717032313347 Validation Loss 0.0009075502166524529\n",
            "Epoch 45: 52 -> Training Loss 0.0008871774189174175 Validation Loss 0.0009049109648913145\n",
            "Epoch 45: 56 -> Training Loss 0.0008877719519659877 Validation Loss 0.0009022805606946349\n",
            "Epoch 45: 60 -> Training Loss 0.0008835855987854302 Validation Loss 0.000899659120477736\n",
            "Epoch 45: 64 -> Training Loss 0.0008870104793459177 Validation Loss 0.0008970405324362218\n",
            "Epoch 45: 68 -> Training Loss 0.0008733610156923532 Validation Loss 0.0008944252040237188\n",
            "Epoch 45: 72 -> Training Loss 0.0008986584725789726 Validation Loss 0.0008918199455365539\n",
            "Epoch 45: 76 -> Training Loss 0.000875163939781487 Validation Loss 0.0008892164332792163\n",
            "Epoch 45: 80 -> Training Loss 0.0008779363124631345 Validation Loss 0.0008866186253726482\n",
            "Epoch 45: 84 -> Training Loss 0.0008589328499510884 Validation Loss 0.0008840268710628152\n",
            "Epoch 45: 88 -> Training Loss 0.0008587574120610952 Validation Loss 0.0008814410539343953\n",
            "Epoch 45: 92 -> Training Loss 0.0008717708406038582 Validation Loss 0.0008788604754954576\n",
            "Epoch 45: 96 -> Training Loss 0.0008394004544243217 Validation Loss 0.0008762863581068814\n",
            "Epoch 46: 0 -> Training Loss 0.0008631938835605979 Validation Loss 0.000873719691298902\n",
            "Epoch 46: 4 -> Training Loss 0.0008419920923188329 Validation Loss 0.0008711543050594628\n",
            "Epoch 46: 8 -> Training Loss 0.0008595709223300219 Validation Loss 0.0008685985812917352\n",
            "Epoch 46: 12 -> Training Loss 0.0008516799425706267 Validation Loss 0.0008660476305522025\n",
            "Epoch 46: 16 -> Training Loss 0.0008405596017837524 Validation Loss 0.000863500579725951\n",
            "Epoch 46: 20 -> Training Loss 0.00084797537419945 Validation Loss 0.0008609609794802964\n",
            "Epoch 46: 24 -> Training Loss 0.0008253534324467182 Validation Loss 0.000858425977639854\n",
            "Epoch 46: 28 -> Training Loss 0.0008535399683751166 Validation Loss 0.0008559005218558013\n",
            "Epoch 46: 32 -> Training Loss 0.0008251522667706013 Validation Loss 0.0008533773361705244\n",
            "Epoch 46: 36 -> Training Loss 0.0008332304423674941 Validation Loss 0.0008508587488904595\n",
            "Epoch 46: 40 -> Training Loss 0.0008539354894310236 Validation Loss 0.0008483442943543196\n",
            "Epoch 46: 44 -> Training Loss 0.0008405284024775028 Validation Loss 0.0008458324009552598\n",
            "Epoch 46: 48 -> Training Loss 0.0008428583387285471 Validation Loss 0.0008433253969997168\n",
            "Epoch 46: 52 -> Training Loss 0.0008252469124272466 Validation Loss 0.0008408236317336559\n",
            "Epoch 46: 56 -> Training Loss 0.0008230365347117186 Validation Loss 0.0008383255917578936\n",
            "Epoch 46: 60 -> Training Loss 0.0008111982024274766 Validation Loss 0.0008358308696188033\n",
            "Epoch 46: 64 -> Training Loss 0.0008211616077460349 Validation Loss 0.0008333426085300744\n",
            "Epoch 46: 68 -> Training Loss 0.0008057029917836189 Validation Loss 0.0008308581891469657\n",
            "Epoch 46: 72 -> Training Loss 0.0008189051877707243 Validation Loss 0.0008283817442134023\n",
            "Epoch 46: 76 -> Training Loss 0.0008107454632408917 Validation Loss 0.0008259122259914875\n",
            "Epoch 46: 80 -> Training Loss 0.0008027360308915377 Validation Loss 0.0008234475390054286\n",
            "Epoch 46: 84 -> Training Loss 0.0007916385075077415 Validation Loss 0.0008209897787310183\n",
            "Epoch 46: 88 -> Training Loss 0.0007822117768228054 Validation Loss 0.0008185416809283197\n",
            "Epoch 46: 92 -> Training Loss 0.0007753055542707443 Validation Loss 0.000816098996438086\n",
            "Epoch 46: 96 -> Training Loss 0.0007777732098475099 Validation Loss 0.0008136627729982138\n",
            "Epoch 47: 0 -> Training Loss 0.0007916342001408339 Validation Loss 0.0008112301584333181\n",
            "Epoch 47: 4 -> Training Loss 0.0008198877912946045 Validation Loss 0.0008087987080216408\n",
            "Epoch 47: 8 -> Training Loss 0.0007894657319411635 Validation Loss 0.0008063659188337624\n",
            "Epoch 47: 12 -> Training Loss 0.0007972648600116372 Validation Loss 0.0008039413369260728\n",
            "Epoch 47: 16 -> Training Loss 0.0007863985374569893 Validation Loss 0.0008015215862542391\n",
            "Epoch 47: 20 -> Training Loss 0.0008003331022337079 Validation Loss 0.0007991059683263302\n",
            "Epoch 47: 24 -> Training Loss 0.0007905528764240444 Validation Loss 0.0007966938428580761\n",
            "Epoch 47: 28 -> Training Loss 0.0007704217568971217 Validation Loss 0.0007942863157950342\n",
            "Epoch 47: 32 -> Training Loss 0.0007737610721960664 Validation Loss 0.0007918860064819455\n",
            "Epoch 47: 36 -> Training Loss 0.0007664175936952233 Validation Loss 0.0007894895388744771\n",
            "Epoch 47: 40 -> Training Loss 0.0007845285581424832 Validation Loss 0.0007870986592024565\n",
            "Epoch 47: 44 -> Training Loss 0.0007927044644020498 Validation Loss 0.0007847092929296196\n",
            "Epoch 47: 48 -> Training Loss 0.0007808966911397874 Validation Loss 0.0007823219639249146\n",
            "Epoch 47: 52 -> Training Loss 0.000748292775824666 Validation Loss 0.0007799409795552492\n",
            "Epoch 47: 56 -> Training Loss 0.0007723644375801086 Validation Loss 0.0007775675039738417\n",
            "Epoch 47: 60 -> Training Loss 0.0007710342179052532 Validation Loss 0.0007751990342512727\n",
            "Epoch 47: 64 -> Training Loss 0.0007790173403918743 Validation Loss 0.0007728351629339159\n",
            "Epoch 47: 68 -> Training Loss 0.0007675327360630035 Validation Loss 0.0007704740855842829\n",
            "Epoch 47: 72 -> Training Loss 0.00075827183900401 Validation Loss 0.0007681153947487473\n",
            "Epoch 47: 76 -> Training Loss 0.0007480435306206346 Validation Loss 0.0007657636888325214\n",
            "Epoch 47: 80 -> Training Loss 0.0007388644735328853 Validation Loss 0.000763420423027128\n",
            "Epoch 47: 84 -> Training Loss 0.0007609751773998141 Validation Loss 0.0007610869943164289\n",
            "Epoch 47: 88 -> Training Loss 0.0007359156152233481 Validation Loss 0.000758754787966609\n",
            "Epoch 47: 92 -> Training Loss 0.000744462595321238 Validation Loss 0.0007564262486994267\n",
            "Epoch 47: 96 -> Training Loss 0.0007519370992667973 Validation Loss 0.0007541020167991519\n",
            "Epoch 48: 0 -> Training Loss 0.0007523062522523105 Validation Loss 0.0007517805206589401\n",
            "Epoch 48: 4 -> Training Loss 0.0007410160615108907 Validation Loss 0.0007494649617001414\n",
            "Epoch 48: 8 -> Training Loss 0.0007150962483137846 Validation Loss 0.000747154641430825\n",
            "Epoch 48: 12 -> Training Loss 0.0007419127505272627 Validation Loss 0.0007448502583429217\n",
            "Epoch 48: 16 -> Training Loss 0.0007472726865671575 Validation Loss 0.0007425507646985352\n",
            "Epoch 48: 20 -> Training Loss 0.0007410625694319606 Validation Loss 0.0007402518531307578\n",
            "Epoch 48: 24 -> Training Loss 0.000715655623935163 Validation Loss 0.0007379545131698251\n",
            "Epoch 48: 28 -> Training Loss 0.000711001455783844 Validation Loss 0.0007356642745435238\n",
            "Epoch 48: 32 -> Training Loss 0.0007215090445242822 Validation Loss 0.0007333813118748367\n",
            "Epoch 48: 36 -> Training Loss 0.0007437750464305282 Validation Loss 0.0007311032386496663\n",
            "Epoch 48: 40 -> Training Loss 0.0007246238528750837 Validation Loss 0.0007288310443982482\n",
            "Epoch 48: 44 -> Training Loss 0.000721029588021338 Validation Loss 0.0007265618769451976\n",
            "Epoch 48: 48 -> Training Loss 0.0007210397161543369 Validation Loss 0.0007242971332743764\n",
            "Epoch 48: 52 -> Training Loss 0.0007274023373611271 Validation Loss 0.0007220373954623938\n",
            "Epoch 48: 56 -> Training Loss 0.0007067631813697517 Validation Loss 0.0007197806844487786\n",
            "Epoch 48: 60 -> Training Loss 0.0007005714578554034 Validation Loss 0.0007175313076004386\n",
            "Epoch 48: 64 -> Training Loss 0.0007102207164280117 Validation Loss 0.0007152872276492417\n",
            "Epoch 48: 68 -> Training Loss 0.0007127862772904336 Validation Loss 0.0007130459416657686\n",
            "Epoch 48: 72 -> Training Loss 0.0006982902996242046 Validation Loss 0.0007108101854100823\n",
            "Epoch 48: 76 -> Training Loss 0.0007015998708084226 Validation Loss 0.0007085812976583838\n",
            "Epoch 48: 80 -> Training Loss 0.0007118430221453309 Validation Loss 0.0007063528173603117\n",
            "Epoch 48: 84 -> Training Loss 0.0006934850243851542 Validation Loss 0.0007041259086690843\n",
            "Epoch 48: 88 -> Training Loss 0.0006836135871708393 Validation Loss 0.0007019061595201492\n",
            "Epoch 48: 92 -> Training Loss 0.0006932333344593644 Validation Loss 0.0006996935117058456\n",
            "Epoch 48: 96 -> Training Loss 0.0006827095639891922 Validation Loss 0.0006974876159802079\n",
            "Epoch 49: 0 -> Training Loss 0.0006928900256752968 Validation Loss 0.0006952897529117763\n",
            "Epoch 49: 4 -> Training Loss 0.0006891575176268816 Validation Loss 0.0006930972449481487\n",
            "Epoch 49: 8 -> Training Loss 0.0006809790502302349 Validation Loss 0.0006909103831276298\n",
            "Epoch 49: 12 -> Training Loss 0.0006835688254795969 Validation Loss 0.0006887266645208001\n",
            "Epoch 49: 16 -> Training Loss 0.0006800389382988214 Validation Loss 0.0006865435861982405\n",
            "Epoch 49: 20 -> Training Loss 0.0006909465882927179 Validation Loss 0.0006843673763796687\n",
            "Epoch 49: 24 -> Training Loss 0.0006559694302268326 Validation Loss 0.0006821957649663091\n",
            "Epoch 49: 28 -> Training Loss 0.0006911990349180996 Validation Loss 0.0006800339906476438\n",
            "Epoch 49: 32 -> Training Loss 0.0006479551084339619 Validation Loss 0.0006778723909519613\n",
            "Epoch 49: 36 -> Training Loss 0.0006537790177389979 Validation Loss 0.0006757183000445366\n",
            "Epoch 49: 40 -> Training Loss 0.0006838997360318899 Validation Loss 0.0006735700881108642\n",
            "Epoch 49: 44 -> Training Loss 0.0006639070343226194 Validation Loss 0.0006714240298606455\n",
            "Epoch 49: 48 -> Training Loss 0.0006311507895588875 Validation Loss 0.0006692844908684492\n",
            "Epoch 49: 52 -> Training Loss 0.0006707193097099662 Validation Loss 0.0006671547307632864\n",
            "Epoch 49: 56 -> Training Loss 0.0006505980854853988 Validation Loss 0.000665027997456491\n",
            "Epoch 49: 60 -> Training Loss 0.0006614997982978821 Validation Loss 0.0006629066774621606\n",
            "Epoch 49: 64 -> Training Loss 0.0006529391976073384 Validation Loss 0.0006607858231291175\n",
            "Epoch 49: 68 -> Training Loss 0.0006477168062701821 Validation Loss 0.0006586703239008784\n",
            "Epoch 49: 72 -> Training Loss 0.000641907739918679 Validation Loss 0.000656565127428621\n",
            "Epoch 49: 76 -> Training Loss 0.0006387050962075591 Validation Loss 0.0006544666830450296\n",
            "Epoch 49: 80 -> Training Loss 0.0006460099248215556 Validation Loss 0.0006523723714053631\n",
            "Epoch 49: 84 -> Training Loss 0.0006267916760407388 Validation Loss 0.000650275032967329\n",
            "Epoch 49: 88 -> Training Loss 0.0006343129789456725 Validation Loss 0.0006481814198195934\n",
            "Epoch 49: 92 -> Training Loss 0.0006428920896723866 Validation Loss 0.0006460970616899431\n",
            "Epoch 49: 96 -> Training Loss 0.000627794477622956 Validation Loss 0.0006440167198888958\n",
            "Epoch 50: 0 -> Training Loss 0.0006381968851201236 Validation Loss 0.0006419409764930606\n",
            "Epoch 50: 4 -> Training Loss 0.0006161581259220839 Validation Loss 0.0006398666882887483\n",
            "Epoch 50: 8 -> Training Loss 0.0006223001983016729 Validation Loss 0.0006378000252880156\n",
            "Epoch 50: 12 -> Training Loss 0.0006296329665929079 Validation Loss 0.0006357389502227306\n",
            "Epoch 50: 16 -> Training Loss 0.0006255392218008637 Validation Loss 0.0006336825317703187\n",
            "Epoch 50: 20 -> Training Loss 0.0006277260836213827 Validation Loss 0.0006316309445537627\n",
            "Epoch 50: 24 -> Training Loss 0.0006249009165912867 Validation Loss 0.0006295825587585568\n",
            "Epoch 50: 28 -> Training Loss 0.0006110711256042123 Validation Loss 0.0006275377236306667\n",
            "Epoch 50: 32 -> Training Loss 0.0006196851027198136 Validation Loss 0.0006254974869079888\n",
            "Epoch 50: 36 -> Training Loss 0.0006163995712995529 Validation Loss 0.0006234612665139139\n",
            "Epoch 50: 40 -> Training Loss 0.0005942367133684456 Validation Loss 0.0006214281311258674\n",
            "Epoch 50: 44 -> Training Loss 0.0006131361587904394 Validation Loss 0.0006194003508426249\n",
            "Epoch 50: 48 -> Training Loss 0.0006111161201260984 Validation Loss 0.0006173764122650027\n",
            "Epoch 50: 52 -> Training Loss 0.0006100558675825596 Validation Loss 0.0006153577705845237\n",
            "Epoch 50: 56 -> Training Loss 0.00059909449191764 Validation Loss 0.0006133420974947512\n",
            "Epoch 50: 60 -> Training Loss 0.000589127535931766 Validation Loss 0.0006113328272476792\n",
            "Epoch 50: 64 -> Training Loss 0.000608922156970948 Validation Loss 0.0006093308329582214\n",
            "Epoch 50: 68 -> Training Loss 0.0005918300012126565 Validation Loss 0.0006073348922654986\n",
            "Epoch 50: 72 -> Training Loss 0.0005873594200238585 Validation Loss 0.0006053434917703271\n",
            "Epoch 50: 76 -> Training Loss 0.0005980540299788117 Validation Loss 0.0006033557001501322\n",
            "Epoch 50: 80 -> Training Loss 0.0005787650588899851 Validation Loss 0.0006013733800500631\n",
            "Epoch 50: 84 -> Training Loss 0.0005943069700151682 Validation Loss 0.0005994010716676712\n",
            "Epoch 50: 88 -> Training Loss 0.0005944377044215798 Validation Loss 0.0005974319647066295\n",
            "Epoch 50: 92 -> Training Loss 0.0005887813167646527 Validation Loss 0.0005954679800197482\n",
            "Epoch 50: 96 -> Training Loss 0.0005781482323072851 Validation Loss 0.0005935078370384872\n",
            "Epoch 51: 0 -> Training Loss 0.0005911896005272865 Validation Loss 0.0005915549700148404\n",
            "Epoch 51: 4 -> Training Loss 0.0005689960671588778 Validation Loss 0.0005896028596907854\n",
            "Epoch 51: 8 -> Training Loss 0.0005804303218610585 Validation Loss 0.0005876552313566208\n",
            "Epoch 51: 12 -> Training Loss 0.0005626354832202196 Validation Loss 0.0005857116775587201\n",
            "Epoch 51: 16 -> Training Loss 0.0005721956258639693 Validation Loss 0.0005837749922648072\n",
            "Epoch 51: 20 -> Training Loss 0.0005610550288110971 Validation Loss 0.0005818437784910202\n",
            "Epoch 51: 24 -> Training Loss 0.0005839136429131031 Validation Loss 0.0005799134378321469\n",
            "Epoch 51: 28 -> Training Loss 0.0005753653822466731 Validation Loss 0.000577982165850699\n",
            "Epoch 51: 32 -> Training Loss 0.0005679287714883685 Validation Loss 0.0005760574713349342\n",
            "Epoch 51: 36 -> Training Loss 0.0005617659771814942 Validation Loss 0.0005741356872022152\n",
            "Epoch 51: 40 -> Training Loss 0.0005882662953808904 Validation Loss 0.0005722178611904383\n",
            "Epoch 51: 44 -> Training Loss 0.0005784762324765325 Validation Loss 0.0005703021306544542\n",
            "Epoch 51: 48 -> Training Loss 0.0005580197321251035 Validation Loss 0.0005683923372998834\n",
            "Epoch 51: 52 -> Training Loss 0.0005672904662787914 Validation Loss 0.0005664892378263175\n",
            "Epoch 51: 56 -> Training Loss 0.00056278589181602 Validation Loss 0.0005645910277962685\n",
            "Epoch 51: 60 -> Training Loss 0.0005496842786669731 Validation Loss 0.0005626976490020752\n",
            "Epoch 51: 64 -> Training Loss 0.0005610196385532618 Validation Loss 0.0005608117207884789\n",
            "Epoch 51: 68 -> Training Loss 0.0005555048119276762 Validation Loss 0.0005589289357885718\n",
            "Epoch 51: 72 -> Training Loss 0.0005614210967905819 Validation Loss 0.0005570510402321815\n",
            "Epoch 51: 76 -> Training Loss 0.000555508304387331 Validation Loss 0.0005551803042180836\n",
            "Epoch 51: 80 -> Training Loss 0.0005245276261121035 Validation Loss 0.0005533151561394334\n",
            "Epoch 51: 84 -> Training Loss 0.0005273936549201608 Validation Loss 0.0005514555959962308\n",
            "Epoch 51: 88 -> Training Loss 0.000535353203304112 Validation Loss 0.000549600925296545\n",
            "Epoch 51: 92 -> Training Loss 0.000537998741492629 Validation Loss 0.0005477502709254622\n",
            "Epoch 51: 96 -> Training Loss 0.0005510447081178427 Validation Loss 0.000545904622413218\n",
            "Epoch 52: 0 -> Training Loss 0.0005277226446196437 Validation Loss 0.0005440600798465312\n",
            "Epoch 52: 4 -> Training Loss 0.000540134496986866 Validation Loss 0.000542221125215292\n",
            "Epoch 52: 8 -> Training Loss 0.0005425477284006774 Validation Loss 0.0005403825198300183\n",
            "Epoch 52: 12 -> Training Loss 0.0005250649992376566 Validation Loss 0.0005385472904890776\n",
            "Epoch 52: 16 -> Training Loss 0.0005481004482135177 Validation Loss 0.0005367214325815439\n",
            "Epoch 52: 20 -> Training Loss 0.0005199664738029242 Validation Loss 0.000534897786565125\n",
            "Epoch 52: 24 -> Training Loss 0.0005326076643541455 Validation Loss 0.0005330807762220502\n",
            "Epoch 52: 28 -> Training Loss 0.0005203943001106381 Validation Loss 0.0005312667926773429\n",
            "Epoch 52: 32 -> Training Loss 0.0005231115501374006 Validation Loss 0.0005294602597132325\n",
            "Epoch 52: 36 -> Training Loss 0.0005272431881166995 Validation Loss 0.0005276594310998917\n",
            "Epoch 52: 40 -> Training Loss 0.0005014543421566486 Validation Loss 0.0005258601740933955\n",
            "Epoch 52: 44 -> Training Loss 0.0005180048756301403 Validation Loss 0.0005240667378529906\n",
            "Epoch 52: 48 -> Training Loss 0.00051835086196661 Validation Loss 0.0005222787149250507\n",
            "Epoch 52: 52 -> Training Loss 0.0005115468520671129 Validation Loss 0.000520496629178524\n",
            "Epoch 52: 56 -> Training Loss 0.0005130915669724345 Validation Loss 0.000518720131367445\n",
            "Epoch 52: 60 -> Training Loss 0.0004987866268493235 Validation Loss 0.000516947009600699\n",
            "Epoch 52: 64 -> Training Loss 0.0004946512635797262 Validation Loss 0.0005151756340637803\n",
            "Epoch 52: 68 -> Training Loss 0.0005043103592470288 Validation Loss 0.0005134091479703784\n",
            "Epoch 52: 72 -> Training Loss 0.00048615928972139955 Validation Loss 0.0005116469692438841\n",
            "Epoch 52: 76 -> Training Loss 0.0004921548534184694 Validation Loss 0.0005098921246826649\n",
            "Epoch 52: 80 -> Training Loss 0.0004968446446582675 Validation Loss 0.000508143741171807\n",
            "Epoch 52: 84 -> Training Loss 0.0005147465271875262 Validation Loss 0.0005063991993665695\n",
            "Epoch 52: 88 -> Training Loss 0.0005012225592508912 Validation Loss 0.0005046526202932\n",
            "Epoch 52: 92 -> Training Loss 0.0004796456196345389 Validation Loss 0.0005029122694395483\n",
            "Epoch 52: 96 -> Training Loss 0.0005121852736920118 Validation Loss 0.0005011773318983614\n",
            "Epoch 53: 0 -> Training Loss 0.0004984593251720071 Validation Loss 0.0004994433256797493\n",
            "Epoch 53: 4 -> Training Loss 0.0004911426221951842 Validation Loss 0.0004977105418220162\n",
            "Epoch 53: 8 -> Training Loss 0.00047599506797268987 Validation Loss 0.0004959800280630589\n",
            "Epoch 53: 12 -> Training Loss 0.0004832889826502651 Validation Loss 0.0004942594678141177\n",
            "Epoch 53: 16 -> Training Loss 0.00048618490109220147 Validation Loss 0.0004925433313474059\n",
            "Epoch 53: 20 -> Training Loss 0.0004838286549784243 Validation Loss 0.0004908270202577114\n",
            "Epoch 53: 24 -> Training Loss 0.0004783067852258682 Validation Loss 0.0004891145508736372\n",
            "Epoch 53: 28 -> Training Loss 0.00047045672545209527 Validation Loss 0.00048740554484538734\n",
            "Epoch 53: 32 -> Training Loss 0.0004844533104915172 Validation Loss 0.0004857027670368552\n",
            "Epoch 53: 36 -> Training Loss 0.0004705853934865445 Validation Loss 0.0004840039473492652\n",
            "Epoch 53: 40 -> Training Loss 0.000477938971016556 Validation Loss 0.00048230821266770363\n",
            "Epoch 53: 44 -> Training Loss 0.0004782979958690703 Validation Loss 0.00048061576671898365\n",
            "Epoch 53: 48 -> Training Loss 0.00048149318899959326 Validation Loss 0.000478931498946622\n",
            "Epoch 53: 52 -> Training Loss 0.000462217052699998 Validation Loss 0.00047725363401696086\n",
            "Epoch 53: 56 -> Training Loss 0.0004803120391443372 Validation Loss 0.0004755813570227474\n",
            "Epoch 53: 60 -> Training Loss 0.0004688992630690336 Validation Loss 0.00047391155385412276\n",
            "Epoch 53: 64 -> Training Loss 0.0004623325075954199 Validation Loss 0.0004722480080090463\n",
            "Epoch 53: 68 -> Training Loss 0.00048189162043854594 Validation Loss 0.000470590079203248\n",
            "Epoch 53: 72 -> Training Loss 0.0004290480283088982 Validation Loss 0.0004689347115345299\n",
            "Epoch 53: 76 -> Training Loss 0.0004479550407268107 Validation Loss 0.0004672858340200037\n",
            "Epoch 53: 80 -> Training Loss 0.00046269784797914326 Validation Loss 0.0004656395176425576\n",
            "Epoch 53: 84 -> Training Loss 0.00045640638563781977 Validation Loss 0.00046399759594351053\n",
            "Epoch 53: 88 -> Training Loss 0.00046808295883238316 Validation Loss 0.00046236059279181063\n",
            "Epoch 53: 92 -> Training Loss 0.00045501007116399705 Validation Loss 0.00046072847908362746\n",
            "Epoch 53: 96 -> Training Loss 0.0004556710191536695 Validation Loss 0.0004590967728290707\n",
            "Epoch 54: 0 -> Training Loss 0.0004435009614098817 Validation Loss 0.0004574681515805423\n",
            "Epoch 54: 4 -> Training Loss 0.0004518814675975591 Validation Loss 0.00045584666077047586\n",
            "Epoch 54: 8 -> Training Loss 0.0004470527346711606 Validation Loss 0.0004542295937426388\n",
            "Epoch 54: 12 -> Training Loss 0.00044607202289626 Validation Loss 0.0004526145348791033\n",
            "Epoch 54: 16 -> Training Loss 0.00045295804738998413 Validation Loss 0.00045100439456291497\n",
            "Epoch 54: 20 -> Training Loss 0.0004390066023916006 Validation Loss 0.00044939626241102815\n",
            "Epoch 54: 24 -> Training Loss 0.00045473428326658905 Validation Loss 0.000447793718194589\n",
            "Epoch 54: 28 -> Training Loss 0.0004453923611436039 Validation Loss 0.00044619248365052044\n",
            "Epoch 54: 32 -> Training Loss 0.00043895712587982416 Validation Loss 0.00044459576020017266\n",
            "Epoch 54: 36 -> Training Loss 0.00042879389366135 Validation Loss 0.00044300378067418933\n",
            "Epoch 54: 40 -> Training Loss 0.00043399629066698253 Validation Loss 0.00044141768012195826\n",
            "Epoch 54: 44 -> Training Loss 0.0004222182324156165 Validation Loss 0.000439835072029382\n",
            "Epoch 54: 48 -> Training Loss 0.0004308934148866683 Validation Loss 0.00043825930333696306\n",
            "Epoch 54: 52 -> Training Loss 0.0004308183561079204 Validation Loss 0.00043668458238244057\n",
            "Epoch 54: 56 -> Training Loss 0.0004254118539392948 Validation Loss 0.0004351151583250612\n",
            "Epoch 54: 60 -> Training Loss 0.00042122084414586425 Validation Loss 0.00043355231173336506\n",
            "Epoch 54: 64 -> Training Loss 0.0004289013158995658 Validation Loss 0.00043199374340474606\n",
            "Epoch 54: 68 -> Training Loss 0.00042190158274024725 Validation Loss 0.00043043511686846614\n",
            "Epoch 54: 72 -> Training Loss 0.0004097822238691151 Validation Loss 0.0004288790514692664\n",
            "Epoch 54: 76 -> Training Loss 0.0004204304132144898 Validation Loss 0.0004273302620276809\n",
            "Epoch 54: 80 -> Training Loss 0.00041314359987154603 Validation Loss 0.0004257871478330344\n",
            "Epoch 54: 84 -> Training Loss 0.00041873392183333635 Validation Loss 0.0004242517170496285\n",
            "Epoch 54: 88 -> Training Loss 0.0004060067585669458 Validation Loss 0.00042271707206964493\n",
            "Epoch 54: 92 -> Training Loss 0.0004142890975344926 Validation Loss 0.00042118411511182785\n",
            "Epoch 54: 96 -> Training Loss 0.00042470888001844287 Validation Loss 0.00041965479613281786\n",
            "Epoch 55: 0 -> Training Loss 0.0004173671477474272 Validation Loss 0.0004181282129138708\n",
            "Epoch 55: 4 -> Training Loss 0.00040676892967894673 Validation Loss 0.00041660599526949227\n",
            "Epoch 55: 8 -> Training Loss 0.000415976217482239 Validation Loss 0.00041508584399707615\n",
            "Epoch 55: 12 -> Training Loss 0.0003897547139786184 Validation Loss 0.0004135690978728235\n",
            "Epoch 55: 16 -> Training Loss 0.0004003008652944118 Validation Loss 0.0004120604135096073\n",
            "Epoch 55: 20 -> Training Loss 0.00040676433127373457 Validation Loss 0.00041055589099414647\n",
            "Epoch 55: 24 -> Training Loss 0.0004191936750430614 Validation Loss 0.00040905564674176276\n",
            "Epoch 55: 28 -> Training Loss 0.00039522891165688634 Validation Loss 0.00040755729423835874\n",
            "Epoch 55: 32 -> Training Loss 0.00039052008651196957 Validation Loss 0.000406065140850842\n",
            "Epoch 55: 36 -> Training Loss 0.00039812782779335976 Validation Loss 0.00040457688737660646\n",
            "Epoch 55: 40 -> Training Loss 0.00040769827319309115 Validation Loss 0.0004030931740999222\n",
            "Epoch 55: 44 -> Training Loss 0.0003866826882585883 Validation Loss 0.00040161143988370895\n",
            "Epoch 55: 48 -> Training Loss 0.0003960447502322495 Validation Loss 0.00040013663237914443\n",
            "Epoch 55: 52 -> Training Loss 0.0003847103798761964 Validation Loss 0.0003986652591265738\n",
            "Epoch 55: 56 -> Training Loss 0.00038858933839946985 Validation Loss 0.00039719833876006305\n",
            "Epoch 55: 60 -> Training Loss 0.00038271970697678626 Validation Loss 0.0003957353183068335\n",
            "Epoch 55: 64 -> Training Loss 0.0004022646462544799 Validation Loss 0.0003942775074392557\n",
            "Epoch 55: 68 -> Training Loss 0.00037619390059262514 Validation Loss 0.00039282115176320076\n",
            "Epoch 55: 72 -> Training Loss 0.00038108398439362645 Validation Loss 0.00039136933628469706\n",
            "Epoch 55: 76 -> Training Loss 0.00038769067032262683 Validation Loss 0.0003899210714735091\n",
            "Epoch 55: 80 -> Training Loss 0.00036544096656143665 Validation Loss 0.00038847647374495864\n",
            "Epoch 55: 84 -> Training Loss 0.0003736458020284772 Validation Loss 0.0003870380751322955\n",
            "Epoch 55: 88 -> Training Loss 0.0003808844485320151 Validation Loss 0.0003856024704873562\n",
            "Epoch 55: 92 -> Training Loss 0.00037912020343355834 Validation Loss 0.0003841682046186179\n",
            "Epoch 55: 96 -> Training Loss 0.00037613423774018884 Validation Loss 0.00038274144753813744\n",
            "Epoch 56: 0 -> Training Loss 0.0003764991997741163 Validation Loss 0.0003813189105130732\n",
            "Epoch 56: 4 -> Training Loss 0.0003747377486433834 Validation Loss 0.0003798968391492963\n",
            "Epoch 56: 8 -> Training Loss 0.00036091936635784805 Validation Loss 0.00037847808562219143\n",
            "Epoch 56: 12 -> Training Loss 0.00036162923788651824 Validation Loss 0.0003770642215386033\n",
            "Epoch 56: 16 -> Training Loss 0.0003614606976043433 Validation Loss 0.00037565286038443446\n",
            "Epoch 56: 20 -> Training Loss 0.0003722540568560362 Validation Loss 0.0003742445260286331\n",
            "Epoch 56: 24 -> Training Loss 0.0003725978313013911 Validation Loss 0.0003728388692252338\n",
            "Epoch 56: 28 -> Training Loss 0.0003562614438124001 Validation Loss 0.0003714386257342994\n",
            "Epoch 56: 32 -> Training Loss 0.000367715023458004 Validation Loss 0.0003700455417856574\n",
            "Epoch 56: 36 -> Training Loss 0.0003525418578647077 Validation Loss 0.00036865699803456664\n",
            "Epoch 56: 40 -> Training Loss 0.00035238300915807486 Validation Loss 0.0003672711900435388\n",
            "Epoch 56: 44 -> Training Loss 0.00036179577000439167 Validation Loss 0.0003658910281956196\n",
            "Epoch 56: 48 -> Training Loss 0.00035883934469893575 Validation Loss 0.00036450978950597346\n",
            "Epoch 56: 52 -> Training Loss 0.00034885972854681313 Validation Loss 0.00036313364398665726\n",
            "Epoch 56: 56 -> Training Loss 0.00034681285615079105 Validation Loss 0.0003617648617364466\n",
            "Epoch 56: 60 -> Training Loss 0.0003562900237739086 Validation Loss 0.0003604013181757182\n",
            "Epoch 56: 64 -> Training Loss 0.0003541118639986962 Validation Loss 0.00035904074320569634\n",
            "Epoch 56: 68 -> Training Loss 0.0003617820329964161 Validation Loss 0.00035768630914390087\n",
            "Epoch 56: 72 -> Training Loss 0.0003444096655584872 Validation Loss 0.00035633554216474295\n",
            "Epoch 56: 76 -> Training Loss 0.00033661763882264495 Validation Loss 0.00035498914076015353\n",
            "Epoch 56: 80 -> Training Loss 0.00035104062408208847 Validation Loss 0.00035364460200071335\n",
            "Epoch 56: 84 -> Training Loss 0.00035189156187698245 Validation Loss 0.00035230250796303153\n",
            "Epoch 56: 88 -> Training Loss 0.0003418870037421584 Validation Loss 0.0003509628586471081\n",
            "Epoch 56: 92 -> Training Loss 0.0003363896976225078 Validation Loss 0.00034962710924446583\n",
            "Epoch 56: 96 -> Training Loss 0.00033814951893873513 Validation Loss 0.0003482953761704266\n",
            "Epoch 57: 0 -> Training Loss 0.00034209145815111697 Validation Loss 0.0003469642251729965\n",
            "Epoch 57: 4 -> Training Loss 0.00033207167871296406 Validation Loss 0.0003456354606896639\n",
            "Epoch 57: 8 -> Training Loss 0.0003450278309173882 Validation Loss 0.0003443100140430033\n",
            "Epoch 57: 12 -> Training Loss 0.0003381936112418771 Validation Loss 0.00034298287937417626\n",
            "Epoch 57: 16 -> Training Loss 0.0003338721871841699 Validation Loss 0.00034166048862971365\n",
            "Epoch 57: 20 -> Training Loss 0.0003380341804586351 Validation Loss 0.00034034348209388554\n",
            "Epoch 57: 24 -> Training Loss 0.0003385095333214849 Validation Loss 0.0003390359925106168\n",
            "Epoch 57: 28 -> Training Loss 0.0003322750562801957 Validation Loss 0.00033773499308153987\n",
            "Epoch 57: 32 -> Training Loss 0.00031711976043879986 Validation Loss 0.00033643728238530457\n",
            "Epoch 57: 36 -> Training Loss 0.0003279990633018315 Validation Loss 0.0003351469640620053\n",
            "Epoch 57: 40 -> Training Loss 0.00032489729346707463 Validation Loss 0.0003338618262205273\n",
            "Epoch 57: 44 -> Training Loss 0.00030950334621593356 Validation Loss 0.0003325840807519853\n",
            "Epoch 57: 48 -> Training Loss 0.00033037213142961264 Validation Loss 0.0003313141642138362\n",
            "Epoch 57: 52 -> Training Loss 0.00032118993112817407 Validation Loss 0.00033004325814545155\n",
            "Epoch 57: 56 -> Training Loss 0.00031247289734892547 Validation Loss 0.00032877380726858974\n",
            "Epoch 57: 60 -> Training Loss 0.0003187174443155527 Validation Loss 0.0003275094204582274\n",
            "Epoch 57: 64 -> Training Loss 0.0003250074223615229 Validation Loss 0.00032624704181216657\n",
            "Epoch 57: 68 -> Training Loss 0.0003116682346444577 Validation Loss 0.0003249870496802032\n",
            "Epoch 57: 72 -> Training Loss 0.0003102235496044159 Validation Loss 0.0003237318014726043\n",
            "Epoch 57: 76 -> Training Loss 0.0003223475650884211 Validation Loss 0.000322480162139982\n",
            "Epoch 57: 80 -> Training Loss 0.0003251682792324573 Validation Loss 0.00032122927950695157\n",
            "Epoch 57: 84 -> Training Loss 0.0003294526832178235 Validation Loss 0.00031997906626202166\n",
            "Epoch 57: 88 -> Training Loss 0.00030396427609957755 Validation Loss 0.0003187350230291486\n",
            "Epoch 57: 92 -> Training Loss 0.00030764006078243256 Validation Loss 0.00031750049674883485\n",
            "Epoch 57: 96 -> Training Loss 0.00029671486117877066 Validation Loss 0.00031627051066607237\n",
            "Epoch 58: 0 -> Training Loss 0.00031106857932172716 Validation Loss 0.0003150452103000134\n",
            "Epoch 58: 4 -> Training Loss 0.0003140737535431981 Validation Loss 0.00031382194720208645\n",
            "Epoch 58: 8 -> Training Loss 0.0003111821715719998 Validation Loss 0.00031259888783097267\n",
            "Epoch 58: 12 -> Training Loss 0.0002984708407893777 Validation Loss 0.0003113792627118528\n",
            "Epoch 58: 16 -> Training Loss 0.0003088530502282083 Validation Loss 0.0003101626061834395\n",
            "Epoch 58: 20 -> Training Loss 0.00030254627927206457 Validation Loss 0.0003089485689997673\n",
            "Epoch 58: 24 -> Training Loss 0.0003002122975885868 Validation Loss 0.00030773604521527886\n",
            "Epoch 58: 28 -> Training Loss 0.0002851827011909336 Validation Loss 0.0003065310884267092\n",
            "Epoch 58: 32 -> Training Loss 0.00029446001281030476 Validation Loss 0.0003053327091038227\n",
            "Epoch 58: 36 -> Training Loss 0.00030048395274206996 Validation Loss 0.0003041354939341545\n",
            "Epoch 58: 40 -> Training Loss 0.00029792444547638297 Validation Loss 0.000302942527923733\n",
            "Epoch 58: 44 -> Training Loss 0.0003000030992552638 Validation Loss 0.00030175215215422213\n",
            "Epoch 58: 48 -> Training Loss 0.00029447878478094935 Validation Loss 0.00030056407558731735\n",
            "Epoch 58: 52 -> Training Loss 0.00028994824970141053 Validation Loss 0.0002993791422341019\n",
            "Epoch 58: 56 -> Training Loss 0.00028139070491306484 Validation Loss 0.0002981960424222052\n",
            "Epoch 58: 60 -> Training Loss 0.00030486381729133427 Validation Loss 0.00029701879248023033\n",
            "Epoch 58: 64 -> Training Loss 0.0002914685755968094 Validation Loss 0.0002958421828225255\n",
            "Epoch 58: 68 -> Training Loss 0.00029834386077709496 Validation Loss 0.00029466720297932625\n",
            "Epoch 58: 72 -> Training Loss 0.0003039618895854801 Validation Loss 0.0002934941730927676\n",
            "Epoch 58: 76 -> Training Loss 0.00028890324756503105 Validation Loss 0.0002923240535892546\n",
            "Epoch 58: 80 -> Training Loss 0.00029177567921578884 Validation Loss 0.00029115850338712335\n",
            "Epoch 58: 84 -> Training Loss 0.0002782242663670331 Validation Loss 0.00028999726055189967\n",
            "Epoch 58: 88 -> Training Loss 0.00027426157612353563 Validation Loss 0.00028884445782750845\n",
            "Epoch 58: 92 -> Training Loss 0.0002823036047630012 Validation Loss 0.0002876950893551111\n",
            "Epoch 58: 96 -> Training Loss 0.0002907845482695848 Validation Loss 0.0002865504357032478\n",
            "Epoch 59: 0 -> Training Loss 0.00026997365057468414 Validation Loss 0.0002854105259757489\n",
            "Epoch 59: 4 -> Training Loss 0.00028760870918631554 Validation Loss 0.000284275010926649\n",
            "Epoch 59: 8 -> Training Loss 0.00027542407042346895 Validation Loss 0.0002831406600307673\n",
            "Epoch 59: 12 -> Training Loss 0.00028336385730654 Validation Loss 0.00028201128588989377\n",
            "Epoch 59: 16 -> Training Loss 0.0002700772020034492 Validation Loss 0.00028088552062399685\n",
            "Epoch 59: 20 -> Training Loss 0.00027321770903654397 Validation Loss 0.00027976182173006237\n",
            "Epoch 59: 24 -> Training Loss 0.0002771726285573095 Validation Loss 0.00027864208095707\n",
            "Epoch 59: 28 -> Training Loss 0.0002672509872354567 Validation Loss 0.0002775254542939365\n",
            "Epoch 59: 32 -> Training Loss 0.000269971409579739 Validation Loss 0.0002764103701338172\n",
            "Epoch 59: 36 -> Training Loss 0.0002792940940707922 Validation Loss 0.000275302940281108\n",
            "Epoch 59: 40 -> Training Loss 0.00027043651789426804 Validation Loss 0.00027420069091022015\n",
            "Epoch 59: 44 -> Training Loss 0.00025856425054371357 Validation Loss 0.0002731019339989871\n",
            "Epoch 59: 48 -> Training Loss 0.0002593034878373146 Validation Loss 0.00027200535987503827\n",
            "Epoch 59: 52 -> Training Loss 0.00026159518165513873 Validation Loss 0.00027091510128229856\n",
            "Epoch 59: 56 -> Training Loss 0.0002740143099799752 Validation Loss 0.00026982996496371925\n",
            "Epoch 59: 60 -> Training Loss 0.0002559675194788724 Validation Loss 0.0002687455853447318\n",
            "Epoch 59: 64 -> Training Loss 0.00027010065969079733 Validation Loss 0.0002676663571037352\n",
            "Epoch 59: 68 -> Training Loss 0.00026631972286850214 Validation Loss 0.0002665882930159569\n",
            "Epoch 59: 72 -> Training Loss 0.0002557294792495668 Validation Loss 0.0002655121497809887\n",
            "Epoch 59: 76 -> Training Loss 0.0002525400777813047 Validation Loss 0.0002644385094754398\n",
            "Epoch 59: 80 -> Training Loss 0.0002655086573213339 Validation Loss 0.00026337115559726954\n",
            "Epoch 59: 84 -> Training Loss 0.0002493257634341717 Validation Loss 0.0002623066247906536\n",
            "Epoch 59: 88 -> Training Loss 0.0002628746151458472 Validation Loss 0.00026124774012714624\n",
            "Epoch 59: 92 -> Training Loss 0.00025984179228544235 Validation Loss 0.0002601895248517394\n",
            "Epoch 59: 96 -> Training Loss 0.00024323332763742656 Validation Loss 0.00025913259014487267\n",
            "Epoch 60: 0 -> Training Loss 0.0002465887228026986 Validation Loss 0.0002580833388492465\n",
            "Epoch 60: 4 -> Training Loss 0.00025442062178626657 Validation Loss 0.00025703758001327515\n",
            "Epoch 60: 8 -> Training Loss 0.000244691880652681 Validation Loss 0.00025599414948374033\n",
            "Epoch 60: 12 -> Training Loss 0.0002497641835361719 Validation Loss 0.0002549568307586014\n",
            "Epoch 60: 16 -> Training Loss 0.00025879021268337965 Validation Loss 0.00025392300449311733\n",
            "Epoch 60: 20 -> Training Loss 0.0002475669316481799 Validation Loss 0.000252889993134886\n",
            "Epoch 60: 24 -> Training Loss 0.00024447700707241893 Validation Loss 0.00025185890262946486\n",
            "Epoch 60: 28 -> Training Loss 0.000246113573666662 Validation Loss 0.0002508325269445777\n",
            "Epoch 60: 32 -> Training Loss 0.00023981454432941973 Validation Loss 0.00024981010938063264\n",
            "Epoch 60: 36 -> Training Loss 0.00024420779664069414 Validation Loss 0.0002487869933247566\n",
            "Epoch 60: 40 -> Training Loss 0.0002484160359017551 Validation Loss 0.00024776486679911613\n",
            "Epoch 60: 44 -> Training Loss 0.0002390472509432584 Validation Loss 0.00024674710584804416\n",
            "Epoch 60: 48 -> Training Loss 0.00024279732315335423 Validation Loss 0.0002457373484503478\n",
            "Epoch 60: 52 -> Training Loss 0.00024098031281027943 Validation Loss 0.0002447327715344727\n",
            "Epoch 60: 56 -> Training Loss 0.0002386357809882611 Validation Loss 0.00024373149790335447\n",
            "Epoch 60: 60 -> Training Loss 0.00023577193496748805 Validation Loss 0.000242734415223822\n",
            "Epoch 60: 64 -> Training Loss 0.00023604571470059454 Validation Loss 0.00024173677957151085\n",
            "Epoch 60: 68 -> Training Loss 0.00023614872770849615 Validation Loss 0.0002407451393082738\n",
            "Epoch 60: 72 -> Training Loss 0.0002354691387154162 Validation Loss 0.0002397561474936083\n",
            "Epoch 60: 76 -> Training Loss 0.00023169862106442451 Validation Loss 0.00023877067724242806\n",
            "Epoch 60: 80 -> Training Loss 0.00023770707775838673 Validation Loss 0.00023778967442922294\n",
            "Epoch 60: 84 -> Training Loss 0.00023372047871816903 Validation Loss 0.00023681198945268989\n",
            "Epoch 60: 88 -> Training Loss 0.00023136867093853652 Validation Loss 0.0002358376805204898\n",
            "Epoch 60: 92 -> Training Loss 0.00023842189693823457 Validation Loss 0.00023486338614020497\n",
            "Epoch 60: 96 -> Training Loss 0.00023652850359212607 Validation Loss 0.0002338857448194176\n",
            "Epoch 61: 0 -> Training Loss 0.00022973580053076148 Validation Loss 0.00023291113029699773\n",
            "Epoch 61: 4 -> Training Loss 0.00022473563149105757 Validation Loss 0.00023194430104922503\n",
            "Epoch 61: 8 -> Training Loss 0.00022553325106855482 Validation Loss 0.00023098153178580105\n",
            "Epoch 61: 12 -> Training Loss 0.00022152649762574583 Validation Loss 0.00023002244415692985\n",
            "Epoch 61: 16 -> Training Loss 0.00022427042131312191 Validation Loss 0.00022906760568730533\n",
            "Epoch 61: 20 -> Training Loss 0.00021844057482667267 Validation Loss 0.0002281151246279478\n",
            "Epoch 61: 24 -> Training Loss 0.00022204595734365284 Validation Loss 0.0002271644480060786\n",
            "Epoch 61: 28 -> Training Loss 0.00022873787384014577 Validation Loss 0.00022621653624810278\n",
            "Epoch 61: 32 -> Training Loss 0.00022272435307968408 Validation Loss 0.00022527211694978178\n",
            "Epoch 61: 36 -> Training Loss 0.00021689011191483587 Validation Loss 0.0002243328490294516\n",
            "Epoch 61: 40 -> Training Loss 0.00021403803839348257 Validation Loss 0.00022339826682582498\n",
            "Epoch 61: 44 -> Training Loss 0.00021192565327510238 Validation Loss 0.0002224678755737841\n",
            "Epoch 61: 48 -> Training Loss 0.00021342889522202313 Validation Loss 0.00022154241742100567\n",
            "Epoch 61: 52 -> Training Loss 0.0002217572182416916 Validation Loss 0.00022062065545469522\n",
            "Epoch 61: 56 -> Training Loss 0.00021034566452726722 Validation Loss 0.00021970170200802386\n",
            "Epoch 61: 60 -> Training Loss 0.00021255467436276376 Validation Loss 0.00021878725965507329\n",
            "Epoch 61: 64 -> Training Loss 0.00019758884445764124 Validation Loss 0.00021787485457025468\n",
            "Epoch 61: 68 -> Training Loss 0.00020773342112079263 Validation Loss 0.00021696514158975333\n",
            "Epoch 61: 72 -> Training Loss 0.00021446618484333158 Validation Loss 0.00021605659276247025\n",
            "Epoch 61: 76 -> Training Loss 0.0002034102799370885 Validation Loss 0.00021514938271138817\n",
            "Epoch 61: 80 -> Training Loss 0.0002078838733723387 Validation Loss 0.00021424655278678983\n",
            "Epoch 61: 84 -> Training Loss 0.0002064375876216218 Validation Loss 0.0002133490052074194\n",
            "Epoch 61: 88 -> Training Loss 0.00019969523418694735 Validation Loss 0.00021245519747026265\n",
            "Epoch 61: 92 -> Training Loss 0.00021298440697137266 Validation Loss 0.0002115643728757277\n",
            "Epoch 61: 96 -> Training Loss 0.00020487414440140128 Validation Loss 0.0002106716128764674\n",
            "Epoch 62: 0 -> Training Loss 0.00019854947458952665 Validation Loss 0.00020978430984541774\n",
            "Epoch 62: 4 -> Training Loss 0.00021094101248309016 Validation Loss 0.00020890167797915637\n",
            "Epoch 62: 8 -> Training Loss 0.00020354789739940315 Validation Loss 0.00020802281505893916\n",
            "Epoch 62: 12 -> Training Loss 0.00019235259969718754 Validation Loss 0.00020714860875159502\n",
            "Epoch 62: 16 -> Training Loss 0.00019663253624457866 Validation Loss 0.00020627996127586812\n",
            "Epoch 62: 20 -> Training Loss 0.00019178056390956044 Validation Loss 0.00020541732374113053\n",
            "Epoch 62: 24 -> Training Loss 0.00020201278675813228 Validation Loss 0.00020455860067158937\n",
            "Epoch 62: 28 -> Training Loss 0.00020075647626072168 Validation Loss 0.0002036987862084061\n",
            "Epoch 62: 32 -> Training Loss 0.0001974339975276962 Validation Loss 0.00020284023776184767\n",
            "Epoch 62: 36 -> Training Loss 0.00018964865012094378 Validation Loss 0.00020198390120640397\n",
            "Epoch 62: 40 -> Training Loss 0.00018996704602614045 Validation Loss 0.00020113079517614096\n",
            "Epoch 62: 44 -> Training Loss 0.0001941142836585641 Validation Loss 0.00020028077415190637\n",
            "Epoch 62: 48 -> Training Loss 0.0001893689186545089 Validation Loss 0.00019943395454902202\n",
            "Epoch 62: 52 -> Training Loss 0.00019622982654254884 Validation Loss 0.0001985924318432808\n",
            "Epoch 62: 56 -> Training Loss 0.00018573852139525115 Validation Loss 0.00019775357213802636\n",
            "Epoch 62: 60 -> Training Loss 0.00019384146435186267 Validation Loss 0.00019691996567416936\n",
            "Epoch 62: 64 -> Training Loss 0.00019231243641115725 Validation Loss 0.0001960899098776281\n",
            "Epoch 62: 68 -> Training Loss 0.00020289499661885202 Validation Loss 0.0001952578459167853\n",
            "Epoch 62: 72 -> Training Loss 0.00019148529099766165 Validation Loss 0.0001944262912729755\n",
            "Epoch 62: 76 -> Training Loss 0.00018268560233991593 Validation Loss 0.0001935992913786322\n",
            "Epoch 62: 80 -> Training Loss 0.00019606556452345103 Validation Loss 0.00019277489627711475\n",
            "Epoch 62: 84 -> Training Loss 0.00019279299885965884 Validation Loss 0.00019195140339434147\n",
            "Epoch 62: 88 -> Training Loss 0.00018573462148196995 Validation Loss 0.0001911296130856499\n",
            "Epoch 62: 92 -> Training Loss 0.0001794458512449637 Validation Loss 0.00019031210104003549\n",
            "Epoch 62: 96 -> Training Loss 0.00018581833865027875 Validation Loss 0.00018950080266222358\n",
            "Epoch 63: 0 -> Training Loss 0.000191501370863989 Validation Loss 0.00018869279301725328\n",
            "Epoch 63: 4 -> Training Loss 0.00016551464796066284 Validation Loss 0.00018788495799526572\n",
            "Epoch 63: 8 -> Training Loss 0.00018243618251290172 Validation Loss 0.00018708486459217966\n",
            "Epoch 63: 12 -> Training Loss 0.0001773890689946711 Validation Loss 0.0001862882636487484\n",
            "Epoch 63: 16 -> Training Loss 0.00017857860075309873 Validation Loss 0.00018549188098404557\n",
            "Epoch 63: 20 -> Training Loss 0.00017207779455929995 Validation Loss 0.0001846994855441153\n",
            "Epoch 63: 24 -> Training Loss 0.00017001834930852056 Validation Loss 0.00018390972400084138\n",
            "Epoch 63: 28 -> Training Loss 0.00017549589392729104 Validation Loss 0.00018312485190108418\n",
            "Epoch 63: 32 -> Training Loss 0.0001707777555566281 Validation Loss 0.00018234489834867418\n",
            "Epoch 63: 36 -> Training Loss 0.0001719634310575202 Validation Loss 0.00018156638543587178\n",
            "Epoch 63: 40 -> Training Loss 0.00017744419164955616 Validation Loss 0.0001807900407584384\n",
            "Epoch 63: 44 -> Training Loss 0.0001778065343387425 Validation Loss 0.00018001528223976493\n",
            "Epoch 63: 48 -> Training Loss 0.00017425650730729103 Validation Loss 0.00017924248822964728\n",
            "Epoch 63: 52 -> Training Loss 0.00017186097102239728 Validation Loss 0.00017847097478806973\n",
            "Epoch 63: 56 -> Training Loss 0.00017393659800291061 Validation Loss 0.00017770261911209673\n",
            "Epoch 63: 60 -> Training Loss 0.00017111122724600136 Validation Loss 0.00017693785775918514\n",
            "Epoch 63: 64 -> Training Loss 0.00017644507170189172 Validation Loss 0.0001761758467182517\n",
            "Epoch 63: 68 -> Training Loss 0.00017013659817166626 Validation Loss 0.00017541753186378628\n",
            "Epoch 63: 72 -> Training Loss 0.00017146908794529736 Validation Loss 0.0001746639027260244\n",
            "Epoch 63: 76 -> Training Loss 0.0001680008281255141 Validation Loss 0.00017391229630447924\n",
            "Epoch 63: 80 -> Training Loss 0.00016099362983368337 Validation Loss 0.00017316384764853865\n",
            "Epoch 63: 84 -> Training Loss 0.00016427581431344151 Validation Loss 0.00017242078320123255\n",
            "Epoch 63: 88 -> Training Loss 0.00017514845239929855 Validation Loss 0.00017168116755783558\n",
            "Epoch 63: 92 -> Training Loss 0.00016471350681968033 Validation Loss 0.00017094046052079648\n",
            "Epoch 63: 96 -> Training Loss 0.00017127813771367073 Validation Loss 0.00017020103405229747\n",
            "Epoch 64: 0 -> Training Loss 0.00016339107241947204 Validation Loss 0.0001694641396170482\n",
            "Epoch 64: 4 -> Training Loss 0.000161540592671372 Validation Loss 0.00016873388085514307\n",
            "Epoch 64: 8 -> Training Loss 0.0001651552738621831 Validation Loss 0.00016800846788100898\n",
            "Epoch 64: 12 -> Training Loss 0.00016545278776902705 Validation Loss 0.0001672858343226835\n",
            "Epoch 64: 16 -> Training Loss 0.0001596395595697686 Validation Loss 0.000166563899256289\n",
            "Epoch 64: 20 -> Training Loss 0.00016280556155834347 Validation Loss 0.0001658447872614488\n",
            "Epoch 64: 24 -> Training Loss 0.00016259014955721796 Validation Loss 0.000165125064086169\n",
            "Epoch 64: 28 -> Training Loss 0.00015969108790159225 Validation Loss 0.00016440550098195672\n",
            "Epoch 64: 32 -> Training Loss 0.00016792523092590272 Validation Loss 0.00016368849901482463\n",
            "Epoch 64: 36 -> Training Loss 0.0001719956926535815 Validation Loss 0.00016297661932185292\n",
            "Epoch 64: 40 -> Training Loss 0.00015732436440885067 Validation Loss 0.00016226786829065531\n",
            "Epoch 64: 44 -> Training Loss 0.00016238813987001777 Validation Loss 0.00016156307538039982\n",
            "Epoch 64: 48 -> Training Loss 0.0001556071947561577 Validation Loss 0.00016086184768937528\n",
            "Epoch 64: 52 -> Training Loss 0.00015571007679682225 Validation Loss 0.00016016350127756596\n",
            "Epoch 64: 56 -> Training Loss 0.00015257293125614524 Validation Loss 0.0001594672939972952\n",
            "Epoch 64: 60 -> Training Loss 0.00016327682533301413 Validation Loss 0.0001587769074831158\n",
            "Epoch 64: 64 -> Training Loss 0.00014675084094051272 Validation Loss 0.0001580886309966445\n",
            "Epoch 64: 68 -> Training Loss 0.00014856526104267687 Validation Loss 0.00015740306116640568\n",
            "Epoch 64: 72 -> Training Loss 0.0001600816030986607 Validation Loss 0.00015671839355491102\n",
            "Epoch 64: 76 -> Training Loss 0.00014671444660052657 Validation Loss 0.00015603218344040215\n",
            "Epoch 64: 80 -> Training Loss 0.00015740605886094272 Validation Loss 0.00015535163402091712\n",
            "Epoch 64: 84 -> Training Loss 0.00014197187556419522 Validation Loss 0.00015467521734535694\n",
            "Epoch 64: 88 -> Training Loss 0.0001498150813858956 Validation Loss 0.00015400274423882365\n",
            "Epoch 64: 92 -> Training Loss 0.00014412602467928082 Validation Loss 0.00015333463670685887\n",
            "Epoch 64: 96 -> Training Loss 0.0001525427505839616 Validation Loss 0.00015266900300048292\n",
            "Epoch 65: 0 -> Training Loss 0.00015017451369203627 Validation Loss 0.0001520020014140755\n",
            "Epoch 65: 4 -> Training Loss 0.00014433760952670127 Validation Loss 0.00015133543638512492\n",
            "Epoch 65: 8 -> Training Loss 0.00015267400885932148 Validation Loss 0.00015067440108396113\n",
            "Epoch 65: 12 -> Training Loss 0.00014566618483513594 Validation Loss 0.00015001663996372372\n",
            "Epoch 65: 16 -> Training Loss 0.0001453472359571606 Validation Loss 0.00014936336083337665\n",
            "Epoch 65: 20 -> Training Loss 0.00015077571151778102 Validation Loss 0.00014871425810270011\n",
            "Epoch 65: 24 -> Training Loss 0.0001449710107408464 Validation Loss 0.00014806709077674896\n",
            "Epoch 65: 28 -> Training Loss 0.00014237008872441947 Validation Loss 0.00014742261555511504\n",
            "Epoch 65: 32 -> Training Loss 0.0001411350240232423 Validation Loss 0.000146780745126307\n",
            "Epoch 65: 36 -> Training Loss 0.00014182644372340292 Validation Loss 0.00014614241081289947\n",
            "Epoch 65: 40 -> Training Loss 0.00014751640264876187 Validation Loss 0.0001455050805816427\n",
            "Epoch 65: 44 -> Training Loss 0.0001401301269652322 Validation Loss 0.0001448689727112651\n",
            "Epoch 65: 48 -> Training Loss 0.00014009090955369174 Validation Loss 0.00014423535321839154\n",
            "Epoch 65: 52 -> Training Loss 0.0001446570677217096 Validation Loss 0.0001436074380762875\n",
            "Epoch 65: 56 -> Training Loss 0.00014058429223950952 Validation Loss 0.00014298511086963117\n",
            "Epoch 65: 60 -> Training Loss 0.00013920324272476137 Validation Loss 0.00014236314746085554\n",
            "Epoch 65: 64 -> Training Loss 0.00013415407738648355 Validation Loss 0.0001417450257577002\n",
            "Epoch 65: 68 -> Training Loss 0.00013665040023624897 Validation Loss 0.00014112956705503166\n",
            "Epoch 65: 72 -> Training Loss 0.00014274846762418747 Validation Loss 0.00014051649486646056\n",
            "Epoch 65: 76 -> Training Loss 0.00012305547716096044 Validation Loss 0.00013990348088555038\n",
            "Epoch 65: 80 -> Training Loss 0.00013941992074251175 Validation Loss 0.000139293450047262\n",
            "Epoch 65: 84 -> Training Loss 0.0001342675241176039 Validation Loss 0.00013868379755876958\n",
            "Epoch 65: 88 -> Training Loss 0.00013171366299502552 Validation Loss 0.00013807840878143907\n",
            "Epoch 65: 92 -> Training Loss 0.00012938342115376145 Validation Loss 0.0001374765415675938\n",
            "Epoch 65: 96 -> Training Loss 0.0001303800381720066 Validation Loss 0.00013687860337086022\n",
            "Epoch 66: 0 -> Training Loss 0.00012898040586151183 Validation Loss 0.00013628185843117535\n",
            "Epoch 66: 4 -> Training Loss 0.00014151030336506665 Validation Loss 0.00013568790745921433\n",
            "Epoch 66: 8 -> Training Loss 0.0001331336097791791 Validation Loss 0.00013509232667274773\n",
            "Epoch 66: 12 -> Training Loss 0.00012763007543981075 Validation Loss 0.00013449561083689332\n",
            "Epoch 66: 16 -> Training Loss 0.00013184787530917674 Validation Loss 0.00013390323147177696\n",
            "Epoch 66: 20 -> Training Loss 0.00012653597514145076 Validation Loss 0.0001333132677245885\n",
            "Epoch 66: 24 -> Training Loss 0.0001326761848758906 Validation Loss 0.00013272676733322442\n",
            "Epoch 66: 28 -> Training Loss 0.00012688546848949045 Validation Loss 0.00013214515638537705\n",
            "Epoch 66: 32 -> Training Loss 0.00013104709796607494 Validation Loss 0.00013156869681552052\n",
            "Epoch 66: 36 -> Training Loss 0.0001251941721420735 Validation Loss 0.00013099487114232033\n",
            "Epoch 66: 40 -> Training Loss 0.00012616711319424212 Validation Loss 0.000130425178213045\n",
            "Epoch 66: 44 -> Training Loss 0.00012107990914955735 Validation Loss 0.00012985659122932702\n",
            "Epoch 66: 48 -> Training Loss 0.00012045484618283808 Validation Loss 0.00012929043441545218\n",
            "Epoch 66: 52 -> Training Loss 0.00012345841969363391 Validation Loss 0.00012872580555267632\n",
            "Epoch 66: 56 -> Training Loss 0.00012873734522145241 Validation Loss 0.00012816226808354259\n",
            "Epoch 66: 60 -> Training Loss 0.000131182765471749 Validation Loss 0.00012760161189362407\n",
            "Epoch 66: 64 -> Training Loss 0.0001272032968699932 Validation Loss 0.00012703843822237104\n",
            "Epoch 66: 68 -> Training Loss 0.0001184357242891565 Validation Loss 0.00012647801486309618\n",
            "Epoch 66: 72 -> Training Loss 0.00012212205911055207 Validation Loss 0.00012592259736265987\n",
            "Epoch 66: 76 -> Training Loss 0.00012423476437106729 Validation Loss 0.000125370075693354\n",
            "Epoch 66: 80 -> Training Loss 0.00011541697313077748 Validation Loss 0.00012481879093684256\n",
            "Epoch 66: 84 -> Training Loss 0.00011726704542525113 Validation Loss 0.00012427112960722297\n",
            "Epoch 66: 88 -> Training Loss 0.00011361316137481481 Validation Loss 0.00012372595665510744\n",
            "Epoch 66: 92 -> Training Loss 0.00012133604468544945 Validation Loss 0.0001231823698617518\n",
            "Epoch 66: 96 -> Training Loss 0.00011298956087557599 Validation Loss 0.0001226418826263398\n",
            "Epoch 67: 0 -> Training Loss 0.00011343357618898153 Validation Loss 0.0001221044803969562\n",
            "Epoch 67: 4 -> Training Loss 0.00011840707156807184 Validation Loss 0.00012156972661614418\n",
            "Epoch 67: 8 -> Training Loss 0.0001234298397321254 Validation Loss 0.00012103548215236515\n",
            "Epoch 67: 12 -> Training Loss 0.00011654839181574062 Validation Loss 0.00012050347868353128\n",
            "Epoch 67: 16 -> Training Loss 0.0001099717992474325 Validation Loss 0.00011997345427516848\n",
            "Epoch 67: 20 -> Training Loss 0.00011959391122218221 Validation Loss 0.00011944553261855617\n",
            "Epoch 67: 24 -> Training Loss 0.00012081171735189855 Validation Loss 0.00011891921167261899\n",
            "Epoch 67: 28 -> Training Loss 0.00011814692697953433 Validation Loss 0.00011839323997264728\n",
            "Epoch 67: 32 -> Training Loss 0.000118069197924342 Validation Loss 0.00011786958202719688\n",
            "Epoch 67: 36 -> Training Loss 0.00011290395923424512 Validation Loss 0.00011734951112885028\n",
            "Epoch 67: 40 -> Training Loss 0.00011269404785707593 Validation Loss 0.00011683259799610823\n",
            "Epoch 67: 44 -> Training Loss 0.00011111788626294583 Validation Loss 0.00011631855159066617\n",
            "Epoch 67: 48 -> Training Loss 0.00011330186680424958 Validation Loss 0.00011580598948057741\n",
            "Epoch 67: 52 -> Training Loss 0.00010196949006058276 Validation Loss 0.00011529615585459396\n",
            "Epoch 67: 56 -> Training Loss 0.00011574634845601395 Validation Loss 0.00011479268141556531\n",
            "Epoch 67: 60 -> Training Loss 0.00011327753600198776 Validation Loss 0.0001142917390097864\n",
            "Epoch 67: 64 -> Training Loss 0.00010636076331138611 Validation Loss 0.00011379279021639377\n",
            "Epoch 67: 68 -> Training Loss 0.0001164204441010952 Validation Loss 0.00011329555127304047\n",
            "Epoch 67: 72 -> Training Loss 0.00011077076487708837 Validation Loss 0.00011279936006758362\n",
            "Epoch 67: 76 -> Training Loss 0.00010436079173814505 Validation Loss 0.00011230642849113792\n",
            "Epoch 67: 80 -> Training Loss 0.00011031479516532272 Validation Loss 0.00011181629815837368\n",
            "Epoch 67: 84 -> Training Loss 0.0001033897278830409 Validation Loss 0.00011132690269732848\n",
            "Epoch 67: 88 -> Training Loss 0.00010851632396224886 Validation Loss 0.00011084169091191143\n",
            "Epoch 67: 92 -> Training Loss 0.00011301673657726496 Validation Loss 0.00011035891657229513\n",
            "Epoch 67: 96 -> Training Loss 0.00011343001096975058 Validation Loss 0.00010987652058247477\n",
            "Epoch 68: 0 -> Training Loss 0.00010858732275664806 Validation Loss 0.00010939377534668893\n",
            "Epoch 68: 4 -> Training Loss 0.00010670609481167048 Validation Loss 0.00010891271813306957\n",
            "Epoch 68: 8 -> Training Loss 0.0001118529326049611 Validation Loss 0.00010843337804544717\n",
            "Epoch 68: 12 -> Training Loss 0.00010019040200859308 Validation Loss 0.00010795616253744811\n",
            "Epoch 68: 16 -> Training Loss 0.0001012878492474556 Validation Loss 0.0001074843094102107\n",
            "Epoch 68: 20 -> Training Loss 9.833381045609713e-05 Validation Loss 0.00010701642895583063\n",
            "Epoch 68: 24 -> Training Loss 0.00010664024011930451 Validation Loss 0.00010654951620381325\n",
            "Epoch 68: 28 -> Training Loss 0.00010176848445553333 Validation Loss 0.00010608165757730603\n",
            "Epoch 68: 32 -> Training Loss 0.00010949999705189839 Validation Loss 0.00010561887756921351\n",
            "Epoch 68: 36 -> Training Loss 0.00010448507964611053 Validation Loss 0.00010515449685044587\n",
            "Epoch 68: 40 -> Training Loss 0.00010244078293908387 Validation Loss 0.00010469411790836602\n",
            "Epoch 68: 44 -> Training Loss 0.00010215143993264064 Validation Loss 0.0001042376970872283\n",
            "Epoch 68: 48 -> Training Loss 9.952165419235826e-05 Validation Loss 0.00010378270235378295\n",
            "Epoch 68: 52 -> Training Loss 9.341305121779442e-05 Validation Loss 0.00010333067621104419\n",
            "Epoch 68: 56 -> Training Loss 0.00010123959509655833 Validation Loss 0.00010288132034474984\n",
            "Epoch 68: 60 -> Training Loss 9.81107004918158e-05 Validation Loss 0.00010243523865938187\n",
            "Epoch 68: 64 -> Training Loss 0.0001010537234833464 Validation Loss 0.0001019928022287786\n",
            "Epoch 68: 68 -> Training Loss 9.825119195738807e-05 Validation Loss 0.00010155211930396035\n",
            "Epoch 68: 72 -> Training Loss 9.676713671069592e-05 Validation Loss 0.00010111219307873398\n",
            "Epoch 68: 76 -> Training Loss 9.582308121025562e-05 Validation Loss 0.00010067311814054847\n",
            "Epoch 68: 80 -> Training Loss 9.933285764418542e-05 Validation Loss 0.00010023578215623274\n",
            "Epoch 68: 84 -> Training Loss 9.855096141109243e-05 Validation Loss 9.979853348340839e-05\n",
            "Epoch 68: 88 -> Training Loss 9.657591726863757e-05 Validation Loss 9.936340211424977e-05\n",
            "Epoch 68: 92 -> Training Loss 9.68855747487396e-05 Validation Loss 9.893208334688097e-05\n",
            "Epoch 68: 96 -> Training Loss 9.788840543478727e-05 Validation Loss 9.85014921752736e-05\n",
            "Epoch 69: 0 -> Training Loss 0.00010419271711725742 Validation Loss 9.807485912460834e-05\n",
            "Epoch 69: 4 -> Training Loss 9.03952750377357e-05 Validation Loss 9.765024151420221e-05\n",
            "Epoch 69: 8 -> Training Loss 8.879258530214429e-05 Validation Loss 9.723088442115113e-05\n",
            "Epoch 69: 12 -> Training Loss 8.578032429795712e-05 Validation Loss 9.681342635303736e-05\n",
            "Epoch 69: 16 -> Training Loss 0.00010342583118472248 Validation Loss 9.639727068133652e-05\n",
            "Epoch 69: 20 -> Training Loss 9.633850277168676e-05 Validation Loss 9.597770986147225e-05\n",
            "Epoch 69: 24 -> Training Loss 8.819761569611728e-05 Validation Loss 9.55611903918907e-05\n",
            "Epoch 69: 28 -> Training Loss 9.655338362790644e-05 Validation Loss 9.514958946965635e-05\n",
            "Epoch 69: 32 -> Training Loss 9.278595098294318e-05 Validation Loss 9.473870159126818e-05\n",
            "Epoch 69: 36 -> Training Loss 9.095378482015803e-05 Validation Loss 9.433063678443432e-05\n",
            "Epoch 69: 40 -> Training Loss 9.577262972015887e-05 Validation Loss 9.392486390424892e-05\n",
            "Epoch 69: 44 -> Training Loss 8.310770499520004e-05 Validation Loss 9.352042980026454e-05\n",
            "Epoch 69: 48 -> Training Loss 8.856561908032745e-05 Validation Loss 9.311984467785805e-05\n",
            "Epoch 69: 52 -> Training Loss 9.145571675617248e-05 Validation Loss 9.272112947655842e-05\n",
            "Epoch 69: 56 -> Training Loss 8.891004108591005e-05 Validation Loss 9.232430602423847e-05\n",
            "Epoch 69: 60 -> Training Loss 8.65622641867958e-05 Validation Loss 9.192951984005049e-05\n",
            "Epoch 69: 64 -> Training Loss 9.196021710522473e-05 Validation Loss 9.153781138593331e-05\n",
            "Epoch 69: 68 -> Training Loss 8.75102705322206e-05 Validation Loss 9.114837303059176e-05\n",
            "Epoch 69: 72 -> Training Loss 8.53791061672382e-05 Validation Loss 9.076086280401796e-05\n",
            "Epoch 69: 76 -> Training Loss 8.964034350356087e-05 Validation Loss 9.037601557793096e-05\n",
            "Epoch 69: 80 -> Training Loss 8.796947804512456e-05 Validation Loss 8.998972771223634e-05\n",
            "Epoch 69: 84 -> Training Loss 8.575526589993387e-05 Validation Loss 8.960362174548209e-05\n",
            "Epoch 69: 88 -> Training Loss 7.920127973193303e-05 Validation Loss 8.92188836587593e-05\n",
            "Epoch 69: 92 -> Training Loss 8.4907456766814e-05 Validation Loss 8.883897680789232e-05\n",
            "Epoch 69: 96 -> Training Loss 8.230483217630535e-05 Validation Loss 8.845973934512585e-05\n",
            "Epoch 70: 0 -> Training Loss 8.841535600367934e-05 Validation Loss 8.808257553027943e-05\n",
            "Epoch 70: 4 -> Training Loss 8.820138464216143e-05 Validation Loss 8.770482963882387e-05\n",
            "Epoch 70: 8 -> Training Loss 8.017061918508261e-05 Validation Loss 8.733055437915027e-05\n",
            "Epoch 70: 12 -> Training Loss 8.197811985155568e-05 Validation Loss 8.696259465068579e-05\n",
            "Epoch 70: 16 -> Training Loss 8.144172898028046e-05 Validation Loss 8.659805462229997e-05\n",
            "Epoch 70: 20 -> Training Loss 8.7623848230578e-05 Validation Loss 8.623483881819993e-05\n",
            "Epoch 70: 24 -> Training Loss 8.323349175043404e-05 Validation Loss 8.587370393797755e-05\n",
            "Epoch 70: 28 -> Training Loss 8.676316065248102e-05 Validation Loss 8.55132529977709e-05\n",
            "Epoch 70: 32 -> Training Loss 8.186134800780565e-05 Validation Loss 8.515150693710893e-05\n",
            "Epoch 70: 36 -> Training Loss 8.105665619950742e-05 Validation Loss 8.479275129502639e-05\n",
            "Epoch 70: 40 -> Training Loss 8.157498086802661e-05 Validation Loss 8.443682600045577e-05\n",
            "Epoch 70: 44 -> Training Loss 7.599632226629183e-05 Validation Loss 8.408246503677219e-05\n",
            "Epoch 70: 48 -> Training Loss 8.054346835706383e-05 Validation Loss 8.373198943445459e-05\n",
            "Epoch 70: 52 -> Training Loss 8.527586032869294e-05 Validation Loss 8.338237967109308e-05\n",
            "Epoch 70: 56 -> Training Loss 8.355380850844085e-05 Validation Loss 8.303405775222927e-05\n",
            "Epoch 70: 60 -> Training Loss 8.115069795167074e-05 Validation Loss 8.26880568638444e-05\n",
            "Epoch 70: 64 -> Training Loss 7.667418685741723e-05 Validation Loss 8.234488632297143e-05\n",
            "Epoch 70: 68 -> Training Loss 8.609250653535128e-05 Validation Loss 8.200443699024618e-05\n",
            "Epoch 70: 72 -> Training Loss 8.213934779632837e-05 Validation Loss 8.166389307007194e-05\n",
            "Epoch 70: 76 -> Training Loss 6.989307439653203e-05 Validation Loss 8.132438233587891e-05\n",
            "Epoch 70: 80 -> Training Loss 7.788420043652877e-05 Validation Loss 8.098781108856201e-05\n",
            "Epoch 70: 84 -> Training Loss 7.833162817405537e-05 Validation Loss 8.065235306276008e-05\n",
            "Epoch 70: 88 -> Training Loss 7.217223173938692e-05 Validation Loss 8.03188668214716e-05\n",
            "Epoch 70: 92 -> Training Loss 7.404888310702518e-05 Validation Loss 7.999153604032472e-05\n",
            "Epoch 70: 96 -> Training Loss 8.410266309510916e-05 Validation Loss 7.966542762005702e-05\n",
            "Epoch 71: 0 -> Training Loss 7.883850776124746e-05 Validation Loss 7.933841698104516e-05\n",
            "Epoch 71: 4 -> Training Loss 8.180455188266933e-05 Validation Loss 7.901107892394066e-05\n",
            "Epoch 71: 8 -> Training Loss 7.860463665565476e-05 Validation Loss 7.868336979299784e-05\n",
            "Epoch 71: 12 -> Training Loss 8.22557631181553e-05 Validation Loss 7.835739233996719e-05\n",
            "Epoch 71: 16 -> Training Loss 7.88822362665087e-05 Validation Loss 7.803417975082994e-05\n",
            "Epoch 71: 20 -> Training Loss 7.649003237020224e-05 Validation Loss 7.771146920276806e-05\n",
            "Epoch 71: 24 -> Training Loss 7.280116551555693e-05 Validation Loss 7.738965359749272e-05\n",
            "Epoch 71: 28 -> Training Loss 7.168417505454272e-05 Validation Loss 7.707100303377956e-05\n",
            "Epoch 71: 32 -> Training Loss 8.158131095115095e-05 Validation Loss 7.675531378481537e-05\n",
            "Epoch 71: 36 -> Training Loss 7.8958721132949e-05 Validation Loss 7.643969001946971e-05\n",
            "Epoch 71: 40 -> Training Loss 7.512455340474844e-05 Validation Loss 7.612657645950094e-05\n",
            "Epoch 71: 44 -> Training Loss 7.319260475924239e-05 Validation Loss 7.581544923596084e-05\n",
            "Epoch 71: 48 -> Training Loss 7.597890362376347e-05 Validation Loss 7.550809095846489e-05\n",
            "Epoch 71: 52 -> Training Loss 7.252437353599817e-05 Validation Loss 7.520447252318263e-05\n",
            "Epoch 71: 56 -> Training Loss 7.370724779320881e-05 Validation Loss 7.490177813451737e-05\n",
            "Epoch 71: 60 -> Training Loss 7.148350414354354e-05 Validation Loss 7.46000005165115e-05\n",
            "Epoch 71: 64 -> Training Loss 7.572784670628607e-05 Validation Loss 7.430277764797211e-05\n",
            "Epoch 71: 68 -> Training Loss 7.03898404026404e-05 Validation Loss 7.400459435302764e-05\n",
            "Epoch 71: 72 -> Training Loss 7.157816435210407e-05 Validation Loss 7.370917592197657e-05\n",
            "Epoch 71: 76 -> Training Loss 6.831107020843774e-05 Validation Loss 7.341544551309198e-05\n",
            "Epoch 71: 80 -> Training Loss 6.803913129260764e-05 Validation Loss 7.312191155506298e-05\n",
            "Epoch 71: 84 -> Training Loss 7.095831824699417e-05 Validation Loss 7.282862497959286e-05\n",
            "Epoch 71: 88 -> Training Loss 6.86377752572298e-05 Validation Loss 7.253509829752147e-05\n",
            "Epoch 71: 92 -> Training Loss 7.586091669509187e-05 Validation Loss 7.22435288480483e-05\n",
            "Epoch 71: 96 -> Training Loss 6.945464701857418e-05 Validation Loss 7.195315265562385e-05\n",
            "Epoch 72: 0 -> Training Loss 7.20535172149539e-05 Validation Loss 7.166406430769712e-05\n",
            "Epoch 72: 4 -> Training Loss 6.836732063675299e-05 Validation Loss 7.137634383980185e-05\n",
            "Epoch 72: 8 -> Training Loss 6.324733112705871e-05 Validation Loss 7.109012949513271e-05\n",
            "Epoch 72: 12 -> Training Loss 7.179501699283719e-05 Validation Loss 7.08071020198986e-05\n",
            "Epoch 72: 16 -> Training Loss 6.764735735487193e-05 Validation Loss 7.052454020595178e-05\n",
            "Epoch 72: 20 -> Training Loss 6.801896233810112e-05 Validation Loss 7.024459773674607e-05\n",
            "Epoch 72: 24 -> Training Loss 6.82587269693613e-05 Validation Loss 6.996879528742284e-05\n",
            "Epoch 72: 28 -> Training Loss 6.560622568940744e-05 Validation Loss 6.96943752700463e-05\n",
            "Epoch 72: 32 -> Training Loss 6.939542799955234e-05 Validation Loss 6.942207255633548e-05\n",
            "Epoch 72: 36 -> Training Loss 7.012848800513893e-05 Validation Loss 6.915261474205181e-05\n",
            "Epoch 72: 40 -> Training Loss 6.428774213418365e-05 Validation Loss 6.888378266012296e-05\n",
            "Epoch 72: 44 -> Training Loss 6.068199945730157e-05 Validation Loss 6.861678411951289e-05\n",
            "Epoch 72: 48 -> Training Loss 7.255226955749094e-05 Validation Loss 6.835315434727818e-05\n",
            "Epoch 72: 52 -> Training Loss 6.951563409529626e-05 Validation Loss 6.808817124692723e-05\n",
            "Epoch 72: 56 -> Training Loss 6.367068999679759e-05 Validation Loss 6.782534183003008e-05\n",
            "Epoch 72: 60 -> Training Loss 6.665353430435061e-05 Validation Loss 6.756457150913775e-05\n",
            "Epoch 72: 64 -> Training Loss 6.858629785710946e-05 Validation Loss 6.730634777341038e-05\n",
            "Epoch 72: 68 -> Training Loss 6.414054951164871e-05 Validation Loss 6.704715633532032e-05\n",
            "Epoch 72: 72 -> Training Loss 6.544520147144794e-05 Validation Loss 6.678893987555057e-05\n",
            "Epoch 72: 76 -> Training Loss 6.170088454382494e-05 Validation Loss 6.653157470282167e-05\n",
            "Epoch 72: 80 -> Training Loss 6.514654523925856e-05 Validation Loss 6.627888069488108e-05\n",
            "Epoch 72: 84 -> Training Loss 6.465156911872327e-05 Validation Loss 6.60269070067443e-05\n",
            "Epoch 72: 88 -> Training Loss 6.186935934238136e-05 Validation Loss 6.577791646122932e-05\n",
            "Epoch 72: 92 -> Training Loss 6.793155625928193e-05 Validation Loss 6.5532440203242e-05\n",
            "Epoch 72: 96 -> Training Loss 6.284838309511542e-05 Validation Loss 6.52859453111887e-05\n",
            "Epoch 73: 0 -> Training Loss 6.150311673991382e-05 Validation Loss 6.503898475784808e-05\n",
            "Epoch 73: 4 -> Training Loss 6.72007299726829e-05 Validation Loss 6.47929628030397e-05\n",
            "Epoch 73: 8 -> Training Loss 6.24146414338611e-05 Validation Loss 6.454849790316075e-05\n",
            "Epoch 73: 12 -> Training Loss 6.298950029304251e-05 Validation Loss 6.430617941077799e-05\n",
            "Epoch 73: 16 -> Training Loss 6.150429544504732e-05 Validation Loss 6.406727334251627e-05\n",
            "Epoch 73: 20 -> Training Loss 5.6951521401060745e-05 Validation Loss 6.38297206023708e-05\n",
            "Epoch 73: 24 -> Training Loss 5.9495192545000464e-05 Validation Loss 6.35939504718408e-05\n",
            "Epoch 73: 28 -> Training Loss 6.098378435126506e-05 Validation Loss 6.336046499200165e-05\n",
            "Epoch 73: 32 -> Training Loss 6.355504592647776e-05 Validation Loss 6.312716868706048e-05\n",
            "Epoch 73: 36 -> Training Loss 6.075480268918909e-05 Validation Loss 6.289384327828884e-05\n",
            "Epoch 73: 40 -> Training Loss 6.101164763094857e-05 Validation Loss 6.266294803936034e-05\n",
            "Epoch 73: 44 -> Training Loss 5.9941710787825286e-05 Validation Loss 6.243503594305366e-05\n",
            "Epoch 73: 48 -> Training Loss 6.186733662616462e-05 Validation Loss 6.220881914487109e-05\n",
            "Epoch 73: 52 -> Training Loss 6.204331293702126e-05 Validation Loss 6.1981932958588e-05\n",
            "Epoch 73: 56 -> Training Loss 5.4382682719733566e-05 Validation Loss 6.175511953188106e-05\n",
            "Epoch 73: 60 -> Training Loss 5.7419860240770504e-05 Validation Loss 6.15307581028901e-05\n",
            "Epoch 73: 64 -> Training Loss 6.363228021655232e-05 Validation Loss 6.130635301815346e-05\n",
            "Epoch 73: 68 -> Training Loss 6.17235345998779e-05 Validation Loss 6.108361412771046e-05\n",
            "Epoch 73: 72 -> Training Loss 6.332385237328708e-05 Validation Loss 6.0860336816404015e-05\n",
            "Epoch 73: 76 -> Training Loss 6.347614544210956e-05 Validation Loss 6.063777982490137e-05\n",
            "Epoch 73: 80 -> Training Loss 5.9900106862187386e-05 Validation Loss 6.0417041822802275e-05\n",
            "Epoch 73: 84 -> Training Loss 6.212307926034555e-05 Validation Loss 6.019726424710825e-05\n",
            "Epoch 73: 88 -> Training Loss 5.648566730087623e-05 Validation Loss 5.997781772748567e-05\n",
            "Epoch 73: 92 -> Training Loss 5.658174632117152e-05 Validation Loss 5.9762034652521834e-05\n",
            "Epoch 73: 96 -> Training Loss 5.7334123994223773e-05 Validation Loss 5.954924563411623e-05\n",
            "Epoch 74: 0 -> Training Loss 6.503362237708643e-05 Validation Loss 5.933959619142115e-05\n",
            "Epoch 74: 4 -> Training Loss 5.69143048778642e-05 Validation Loss 5.912952474318445e-05\n",
            "Epoch 74: 8 -> Training Loss 5.5125667131505907e-05 Validation Loss 5.892004264751449e-05\n",
            "Epoch 74: 12 -> Training Loss 5.812009476358071e-05 Validation Loss 5.871431858395226e-05\n",
            "Epoch 74: 16 -> Training Loss 5.8155026636086404e-05 Validation Loss 5.850773231941275e-05\n",
            "Epoch 74: 20 -> Training Loss 6.0889233282068744e-05 Validation Loss 5.8299818192608654e-05\n",
            "Epoch 74: 24 -> Training Loss 5.7593697420088574e-05 Validation Loss 5.809152207802981e-05\n",
            "Epoch 74: 28 -> Training Loss 5.647737270919606e-05 Validation Loss 5.788660200778395e-05\n",
            "Epoch 74: 32 -> Training Loss 6.209700950421393e-05 Validation Loss 5.768377013737336e-05\n",
            "Epoch 74: 36 -> Training Loss 5.3280804422684014e-05 Validation Loss 5.7479963288642466e-05\n",
            "Epoch 74: 40 -> Training Loss 5.5430740758311003e-05 Validation Loss 5.727948882849887e-05\n",
            "Epoch 74: 44 -> Training Loss 5.726111703552306e-05 Validation Loss 5.7081728300545365e-05\n",
            "Epoch 74: 48 -> Training Loss 5.2799154218519107e-05 Validation Loss 5.688527744496241e-05\n",
            "Epoch 74: 52 -> Training Loss 5.809326466987841e-05 Validation Loss 5.6691722420509905e-05\n",
            "Epoch 74: 56 -> Training Loss 5.197115751798265e-05 Validation Loss 5.649817467201501e-05\n",
            "Epoch 74: 60 -> Training Loss 5.6592361943330616e-05 Validation Loss 5.630657688016072e-05\n",
            "Epoch 74: 64 -> Training Loss 5.4002408433007076e-05 Validation Loss 5.611497181234881e-05\n",
            "Epoch 74: 68 -> Training Loss 5.2438233979046345e-05 Validation Loss 5.592426168732345e-05\n",
            "Epoch 74: 72 -> Training Loss 5.025729842600413e-05 Validation Loss 5.573628732236102e-05\n",
            "Epoch 74: 76 -> Training Loss 5.585879989666864e-05 Validation Loss 5.5550011893501505e-05\n",
            "Epoch 74: 80 -> Training Loss 5.3096580813871697e-05 Validation Loss 5.5363678256981075e-05\n",
            "Epoch 74: 84 -> Training Loss 5.5806573072914034e-05 Validation Loss 5.5179858463816345e-05\n",
            "Epoch 74: 88 -> Training Loss 5.912876804359257e-05 Validation Loss 5.499594044522382e-05\n",
            "Epoch 74: 92 -> Training Loss 5.0626127631403506e-05 Validation Loss 5.4811891459394246e-05\n",
            "Epoch 74: 96 -> Training Loss 5.280334153212607e-05 Validation Loss 5.462860281113535e-05\n",
            "Epoch 75: 0 -> Training Loss 5.460522515932098e-05 Validation Loss 5.4445878049591556e-05\n",
            "Epoch 75: 4 -> Training Loss 5.116030661156401e-05 Validation Loss 5.4264437494566664e-05\n",
            "Epoch 75: 8 -> Training Loss 5.463151683215983e-05 Validation Loss 5.408592551248148e-05\n",
            "Epoch 75: 12 -> Training Loss 5.012887413613498e-05 Validation Loss 5.390928708948195e-05\n",
            "Epoch 75: 16 -> Training Loss 5.287660678732209e-05 Validation Loss 5.3733565437141806e-05\n",
            "Epoch 75: 20 -> Training Loss 5.46508417755831e-05 Validation Loss 5.3557676437776536e-05\n",
            "Epoch 75: 24 -> Training Loss 5.4355325119104236e-05 Validation Loss 5.338200571713969e-05\n",
            "Epoch 75: 28 -> Training Loss 5.034497007727623e-05 Validation Loss 5.320725904311985e-05\n",
            "Epoch 75: 32 -> Training Loss 4.725815961137414e-05 Validation Loss 5.3033338190289214e-05\n",
            "Epoch 75: 36 -> Training Loss 4.904200613964349e-05 Validation Loss 5.2862866141367704e-05\n",
            "Epoch 75: 40 -> Training Loss 4.854016151512042e-05 Validation Loss 5.269364919513464e-05\n",
            "Epoch 75: 44 -> Training Loss 4.7915815230226144e-05 Validation Loss 5.252650589682162e-05\n",
            "Epoch 75: 48 -> Training Loss 5.405685442383401e-05 Validation Loss 5.236213110038079e-05\n",
            "Epoch 75: 52 -> Training Loss 4.966388951288536e-05 Validation Loss 5.2195784519426525e-05\n",
            "Epoch 75: 56 -> Training Loss 5.286109080770984e-05 Validation Loss 5.202926695346832e-05\n",
            "Epoch 75: 60 -> Training Loss 5.0129972805734724e-05 Validation Loss 5.186309135751799e-05\n",
            "Epoch 75: 64 -> Training Loss 5.326214886736125e-05 Validation Loss 5.169891664991155e-05\n",
            "Epoch 75: 68 -> Training Loss 4.920669744024053e-05 Validation Loss 5.153696474735625e-05\n",
            "Epoch 75: 72 -> Training Loss 4.912401345791295e-05 Validation Loss 5.137643893249333e-05\n",
            "Epoch 75: 76 -> Training Loss 5.167849303688854e-05 Validation Loss 5.121623689774424e-05\n",
            "Epoch 75: 80 -> Training Loss 5.2376424719113857e-05 Validation Loss 5.105783202452585e-05\n",
            "Epoch 75: 84 -> Training Loss 4.719010030385107e-05 Validation Loss 5.0901740905828774e-05\n",
            "Epoch 75: 88 -> Training Loss 5.3067939006723464e-05 Validation Loss 5.074763248558156e-05\n",
            "Epoch 75: 92 -> Training Loss 4.49724102509208e-05 Validation Loss 5.059450995759107e-05\n",
            "Epoch 75: 96 -> Training Loss 4.969778092345223e-05 Validation Loss 5.044245335739106e-05\n",
            "Epoch 76: 0 -> Training Loss 5.4269388783723116e-05 Validation Loss 5.029070598538965e-05\n",
            "Epoch 76: 4 -> Training Loss 4.9344293074682355e-05 Validation Loss 5.01388858538121e-05\n",
            "Epoch 76: 8 -> Training Loss 5.071288614999503e-05 Validation Loss 4.998887743568048e-05\n",
            "Epoch 76: 12 -> Training Loss 4.9933667469304055e-05 Validation Loss 4.9840164138004184e-05\n",
            "Epoch 76: 16 -> Training Loss 4.8320856876671314e-05 Validation Loss 4.9692695029079914e-05\n",
            "Epoch 76: 20 -> Training Loss 4.611627809936181e-05 Validation Loss 4.954479663865641e-05\n",
            "Epoch 76: 24 -> Training Loss 5.254115239949897e-05 Validation Loss 4.939863720210269e-05\n",
            "Epoch 76: 28 -> Training Loss 4.98451991006732e-05 Validation Loss 4.9255177145823836e-05\n",
            "Epoch 76: 32 -> Training Loss 5.070403858553618e-05 Validation Loss 4.911312862532213e-05\n",
            "Epoch 76: 36 -> Training Loss 5.121705908095464e-05 Validation Loss 4.897078179055825e-05\n",
            "Epoch 76: 40 -> Training Loss 5.151987716089934e-05 Validation Loss 4.88292243971955e-05\n",
            "Epoch 76: 44 -> Training Loss 5.02274269820191e-05 Validation Loss 4.868823816650547e-05\n",
            "Epoch 76: 48 -> Training Loss 4.9047830543713644e-05 Validation Loss 4.8549969505984336e-05\n",
            "Epoch 76: 52 -> Training Loss 4.915787576464936e-05 Validation Loss 4.841216650675051e-05\n",
            "Epoch 76: 56 -> Training Loss 5.011187749914825e-05 Validation Loss 4.827546581509523e-05\n",
            "Epoch 76: 60 -> Training Loss 4.593023186316714e-05 Validation Loss 4.8138812417164445e-05\n",
            "Epoch 76: 64 -> Training Loss 4.905326204607263e-05 Validation Loss 4.800401075044647e-05\n",
            "Epoch 76: 68 -> Training Loss 4.551513120532036e-05 Validation Loss 4.786991485161707e-05\n",
            "Epoch 76: 72 -> Training Loss 4.7000805352581665e-05 Validation Loss 4.7737110435264185e-05\n",
            "Epoch 76: 76 -> Training Loss 5.304297883412801e-05 Validation Loss 4.760499359690584e-05\n",
            "Epoch 76: 80 -> Training Loss 4.459676711121574e-05 Validation Loss 4.747296770801768e-05\n",
            "Epoch 76: 84 -> Training Loss 4.229815385770053e-05 Validation Loss 4.734265530714765e-05\n",
            "Epoch 76: 88 -> Training Loss 4.678624827647582e-05 Validation Loss 4.721424193121493e-05\n",
            "Epoch 76: 92 -> Training Loss 5.4388074204325676e-05 Validation Loss 4.708657070295885e-05\n",
            "Epoch 76: 96 -> Training Loss 4.482125223148614e-05 Validation Loss 4.69589649583213e-05\n",
            "Epoch 77: 0 -> Training Loss 4.24271056544967e-05 Validation Loss 4.683203587774187e-05\n",
            "Epoch 77: 4 -> Training Loss 4.429754335433245e-05 Validation Loss 4.670619091484696e-05\n",
            "Epoch 77: 8 -> Training Loss 4.475286914384924e-05 Validation Loss 4.657764293369837e-05\n",
            "Epoch 77: 12 -> Training Loss 4.7177309170365334e-05 Validation Loss 4.645069930120371e-05\n",
            "Epoch 77: 16 -> Training Loss 5.0213569920742884e-05 Validation Loss 4.632624768419191e-05\n",
            "Epoch 77: 20 -> Training Loss 4.944279498886317e-05 Validation Loss 4.620053732651286e-05\n",
            "Epoch 77: 24 -> Training Loss 4.5653683628188446e-05 Validation Loss 4.607568553183228e-05\n",
            "Epoch 77: 28 -> Training Loss 4.77653338748496e-05 Validation Loss 4.595293285092339e-05\n",
            "Epoch 77: 32 -> Training Loss 4.441118289832957e-05 Validation Loss 4.583274858305231e-05\n",
            "Epoch 77: 36 -> Training Loss 4.466506288736127e-05 Validation Loss 4.571400131681003e-05\n",
            "Epoch 77: 40 -> Training Loss 4.767876089317724e-05 Validation Loss 4.559511580737308e-05\n",
            "Epoch 77: 44 -> Training Loss 4.1348434024257585e-05 Validation Loss 4.547571006696671e-05\n",
            "Epoch 77: 48 -> Training Loss 3.954827843699604e-05 Validation Loss 4.5357272028923035e-05\n",
            "Epoch 77: 52 -> Training Loss 4.1284962208010256e-05 Validation Loss 4.52401909569744e-05\n",
            "Epoch 77: 56 -> Training Loss 4.629441173165105e-05 Validation Loss 4.512520899879746e-05\n",
            "Epoch 77: 60 -> Training Loss 4.782754331245087e-05 Validation Loss 4.501116927713156e-05\n",
            "Epoch 77: 64 -> Training Loss 4.895933307125233e-05 Validation Loss 4.4899377826368436e-05\n",
            "Epoch 77: 68 -> Training Loss 4.680822530644946e-05 Validation Loss 4.478809569263831e-05\n",
            "Epoch 77: 72 -> Training Loss 4.117225034860894e-05 Validation Loss 4.467588587431237e-05\n",
            "Epoch 77: 76 -> Training Loss 4.4725478801410645e-05 Validation Loss 4.456431997823529e-05\n",
            "Epoch 77: 80 -> Training Loss 4.505703691393137e-05 Validation Loss 4.445363811100833e-05\n",
            "Epoch 77: 84 -> Training Loss 4.735713082482107e-05 Validation Loss 4.434479706105776e-05\n",
            "Epoch 77: 88 -> Training Loss 4.426785744726658e-05 Validation Loss 4.423757491167635e-05\n",
            "Epoch 77: 92 -> Training Loss 4.69196165795438e-05 Validation Loss 4.4131276808911934e-05\n",
            "Epoch 77: 96 -> Training Loss 4.267246549716219e-05 Validation Loss 4.402452759677544e-05\n",
            "Epoch 78: 0 -> Training Loss 4.161072865827009e-05 Validation Loss 4.391938273329288e-05\n",
            "Epoch 78: 4 -> Training Loss 3.9249083783943206e-05 Validation Loss 4.381446342449635e-05\n",
            "Epoch 78: 8 -> Training Loss 3.654041938716546e-05 Validation Loss 4.370905662653968e-05\n",
            "Epoch 78: 12 -> Training Loss 3.993109567090869e-05 Validation Loss 4.36061127402354e-05\n",
            "Epoch 78: 16 -> Training Loss 4.0825070755090564e-05 Validation Loss 4.350400558905676e-05\n",
            "Epoch 78: 20 -> Training Loss 4.235725646140054e-05 Validation Loss 4.340258237789385e-05\n",
            "Epoch 78: 24 -> Training Loss 4.199632894597016e-05 Validation Loss 4.330178126110695e-05\n",
            "Epoch 78: 28 -> Training Loss 4.374999116407707e-05 Validation Loss 4.3200969230383635e-05\n",
            "Epoch 78: 32 -> Training Loss 4.321441156207584e-05 Validation Loss 4.310105578042567e-05\n",
            "Epoch 78: 36 -> Training Loss 4.708278720499948e-05 Validation Loss 4.3002139136660844e-05\n",
            "Epoch 78: 40 -> Training Loss 3.665464828372933e-05 Validation Loss 4.290159995434806e-05\n",
            "Epoch 78: 44 -> Training Loss 4.298747444408946e-05 Validation Loss 4.2803083488252014e-05\n",
            "Epoch 78: 48 -> Training Loss 4.5098997361492366e-05 Validation Loss 4.270532008376904e-05\n",
            "Epoch 78: 52 -> Training Loss 4.323083703638986e-05 Validation Loss 4.2609106458257884e-05\n",
            "Epoch 78: 56 -> Training Loss 3.963493145420216e-05 Validation Loss 4.25149301008787e-05\n",
            "Epoch 78: 60 -> Training Loss 3.97333424189128e-05 Validation Loss 4.242199793225154e-05\n",
            "Epoch 78: 64 -> Training Loss 3.944958370993845e-05 Validation Loss 4.2331805161666125e-05\n",
            "Epoch 78: 68 -> Training Loss 4.3664804252330214e-05 Validation Loss 4.224205622449517e-05\n",
            "Epoch 78: 72 -> Training Loss 4.1404113289900124e-05 Validation Loss 4.215274384478107e-05\n",
            "Epoch 78: 76 -> Training Loss 4.30194049840793e-05 Validation Loss 4.206320227240212e-05\n",
            "Epoch 78: 80 -> Training Loss 4.6148630644893274e-05 Validation Loss 4.197440648567863e-05\n",
            "Epoch 78: 84 -> Training Loss 4.648098547477275e-05 Validation Loss 4.188475577393547e-05\n",
            "Epoch 78: 88 -> Training Loss 4.418124444782734e-05 Validation Loss 4.1796170989982784e-05\n",
            "Epoch 78: 92 -> Training Loss 4.174238711129874e-05 Validation Loss 4.170848478679545e-05\n",
            "Epoch 78: 96 -> Training Loss 4.189428000245243e-05 Validation Loss 4.1621307900641114e-05\n",
            "Epoch 79: 0 -> Training Loss 4.037743929075077e-05 Validation Loss 4.153403278905898e-05\n",
            "Epoch 79: 4 -> Training Loss 4.3316213123034686e-05 Validation Loss 4.144782724324614e-05\n",
            "Epoch 79: 8 -> Training Loss 4.1860803321469575e-05 Validation Loss 4.136200368520804e-05\n",
            "Epoch 79: 12 -> Training Loss 4.178567178314552e-05 Validation Loss 4.12763147323858e-05\n",
            "Epoch 79: 16 -> Training Loss 3.9346719859167933e-05 Validation Loss 4.119177901884541e-05\n",
            "Epoch 79: 20 -> Training Loss 3.995466613559984e-05 Validation Loss 4.110905865672976e-05\n",
            "Epoch 79: 24 -> Training Loss 3.87113141187001e-05 Validation Loss 4.1026840335689485e-05\n",
            "Epoch 79: 28 -> Training Loss 3.983634815085679e-05 Validation Loss 4.0944942156784236e-05\n",
            "Epoch 79: 32 -> Training Loss 4.0460334275849164e-05 Validation Loss 4.086421540705487e-05\n",
            "Epoch 79: 36 -> Training Loss 4.003654612461105e-05 Validation Loss 4.078422352904454e-05\n",
            "Epoch 79: 40 -> Training Loss 4.2513085645623505e-05 Validation Loss 4.070539216627367e-05\n",
            "Epoch 79: 44 -> Training Loss 3.742886474356055e-05 Validation Loss 4.062576044816524e-05\n",
            "Epoch 79: 48 -> Training Loss 4.385670035844669e-05 Validation Loss 4.0546794480178505e-05\n",
            "Epoch 79: 52 -> Training Loss 4.120652738492936e-05 Validation Loss 4.046899266541004e-05\n",
            "Epoch 79: 56 -> Training Loss 4.375105709186755e-05 Validation Loss 4.039261693833396e-05\n",
            "Epoch 79: 60 -> Training Loss 4.1626033635111526e-05 Validation Loss 4.031509888591245e-05\n",
            "Epoch 79: 64 -> Training Loss 4.2627081711543724e-05 Validation Loss 4.023798828711733e-05\n",
            "Epoch 79: 68 -> Training Loss 3.761671541724354e-05 Validation Loss 4.0160877688322216e-05\n",
            "Epoch 79: 72 -> Training Loss 4.2098385165445507e-05 Validation Loss 4.008788528153673e-05\n",
            "Epoch 79: 76 -> Training Loss 4.4358963350532576e-05 Validation Loss 4.001575871370733e-05\n",
            "Epoch 79: 80 -> Training Loss 4.2075771489180624e-05 Validation Loss 3.994521830463782e-05\n",
            "Epoch 79: 84 -> Training Loss 4.112972237635404e-05 Validation Loss 3.987610398326069e-05\n",
            "Epoch 79: 88 -> Training Loss 3.910035593435168e-05 Validation Loss 3.980641849921085e-05\n",
            "Epoch 79: 92 -> Training Loss 4.159196396358311e-05 Validation Loss 3.973698039771989e-05\n",
            "Epoch 79: 96 -> Training Loss 3.945631397073157e-05 Validation Loss 3.9667287637712434e-05\n",
            "Epoch 80: 0 -> Training Loss 4.381037433631718e-05 Validation Loss 3.9597634895471856e-05\n",
            "Epoch 80: 4 -> Training Loss 4.0913022530730814e-05 Validation Loss 3.952684346586466e-05\n",
            "Epoch 80: 8 -> Training Loss 3.649592326837592e-05 Validation Loss 3.9457096136175096e-05\n",
            "Epoch 80: 12 -> Training Loss 3.6917994293617085e-05 Validation Loss 3.938896406907588e-05\n",
            "Epoch 80: 16 -> Training Loss 4.0237307985080406e-05 Validation Loss 3.932111576432362e-05\n",
            "Epoch 80: 20 -> Training Loss 3.9562022720929235e-05 Validation Loss 3.925320561393164e-05\n",
            "Epoch 80: 24 -> Training Loss 3.576272138161585e-05 Validation Loss 3.9186590583994985e-05\n",
            "Epoch 80: 28 -> Training Loss 3.788867616094649e-05 Validation Loss 3.911986277671531e-05\n",
            "Epoch 80: 32 -> Training Loss 3.8702190067851916e-05 Validation Loss 3.9053731597959995e-05\n",
            "Epoch 80: 36 -> Training Loss 4.0742921555647627e-05 Validation Loss 3.898810609825887e-05\n",
            "Epoch 80: 40 -> Training Loss 4.293656093068421e-05 Validation Loss 3.892336826538667e-05\n",
            "Epoch 80: 44 -> Training Loss 3.5629564081318676e-05 Validation Loss 3.885984551743604e-05\n",
            "Epoch 80: 48 -> Training Loss 3.402333095436916e-05 Validation Loss 3.8798036257503554e-05\n",
            "Epoch 80: 52 -> Training Loss 4.068871203344315e-05 Validation Loss 3.8737052818760276e-05\n",
            "Epoch 80: 56 -> Training Loss 3.463244502199814e-05 Validation Loss 3.867658961098641e-05\n",
            "Epoch 80: 60 -> Training Loss 3.453344106674194e-05 Validation Loss 3.861599543597549e-05\n",
            "Epoch 80: 64 -> Training Loss 3.9793063479010016e-05 Validation Loss 3.8556900108233094e-05\n",
            "Epoch 80: 68 -> Training Loss 3.6101955629419535e-05 Validation Loss 3.849792847177014e-05\n",
            "Epoch 80: 72 -> Training Loss 4.116749914828688e-05 Validation Loss 3.844023012788966e-05\n",
            "Epoch 80: 76 -> Training Loss 3.7852543755434453e-05 Validation Loss 3.838291740976274e-05\n",
            "Epoch 80: 80 -> Training Loss 3.667785495053977e-05 Validation Loss 3.832566289929673e-05\n",
            "Epoch 80: 84 -> Training Loss 4.0369552152697e-05 Validation Loss 3.826867032330483e-05\n",
            "Epoch 80: 88 -> Training Loss 3.480182931525633e-05 Validation Loss 3.8209913327591494e-05\n",
            "Epoch 80: 92 -> Training Loss 4.0544116927776486e-05 Validation Loss 3.815251693595201e-05\n",
            "Epoch 80: 96 -> Training Loss 4.16181719629094e-05 Validation Loss 3.809469490079209e-05\n",
            "Epoch 81: 0 -> Training Loss 3.8168858736753464e-05 Validation Loss 3.803848085226491e-05\n",
            "Epoch 81: 4 -> Training Loss 3.5994467907585204e-05 Validation Loss 3.7983634683769196e-05\n",
            "Epoch 81: 8 -> Training Loss 3.8462967495433986e-05 Validation Loss 3.792900679400191e-05\n",
            "Epoch 81: 12 -> Training Loss 3.757455124286935e-05 Validation Loss 3.78745753550902e-05\n",
            "Epoch 81: 16 -> Training Loss 3.790440678130835e-05 Validation Loss 3.7819569115526974e-05\n",
            "Epoch 81: 20 -> Training Loss 3.5399010812398046e-05 Validation Loss 3.7767429603263736e-05\n",
            "Epoch 81: 24 -> Training Loss 3.759476021514274e-05 Validation Loss 3.7715868529630825e-05\n",
            "Epoch 81: 28 -> Training Loss 3.51279741153121e-05 Validation Loss 3.7665980926249176e-05\n",
            "Epoch 81: 32 -> Training Loss 3.725249189301394e-05 Validation Loss 3.7615907785948366e-05\n",
            "Epoch 81: 36 -> Training Loss 3.676925916806795e-05 Validation Loss 3.7564695958280936e-05\n",
            "Epoch 81: 40 -> Training Loss 3.638588896137662e-05 Validation Loss 3.751480107894167e-05\n",
            "Epoch 81: 44 -> Training Loss 3.614616798586212e-05 Validation Loss 3.746465517906472e-05\n",
            "Epoch 81: 48 -> Training Loss 4.190907202428207e-05 Validation Loss 3.74144728993997e-05\n",
            "Epoch 81: 52 -> Training Loss 3.5475881304591894e-05 Validation Loss 3.736423241207376e-05\n",
            "Epoch 81: 56 -> Training Loss 3.652922168839723e-05 Validation Loss 3.731365723069757e-05\n",
            "Epoch 81: 60 -> Training Loss 3.917334834113717e-05 Validation Loss 3.726430804817937e-05\n",
            "Epoch 81: 64 -> Training Loss 3.41146151185967e-05 Validation Loss 3.721464236150496e-05\n",
            "Epoch 81: 68 -> Training Loss 3.888839273713529e-05 Validation Loss 3.716547507792711e-05\n",
            "Epoch 81: 72 -> Training Loss 3.697849024320021e-05 Validation Loss 3.711694444064051e-05\n",
            "Epoch 81: 76 -> Training Loss 3.470866795396432e-05 Validation Loss 3.706863572006114e-05\n",
            "Epoch 81: 80 -> Training Loss 3.517171717248857e-05 Validation Loss 3.702061076182872e-05\n",
            "Epoch 81: 84 -> Training Loss 3.864281461574137e-05 Validation Loss 3.6972520319977775e-05\n",
            "Epoch 81: 88 -> Training Loss 4.337478458182886e-05 Validation Loss 3.692371683428064e-05\n",
            "Epoch 81: 92 -> Training Loss 3.397536784177646e-05 Validation Loss 3.6875924706691876e-05\n",
            "Epoch 81: 96 -> Training Loss 3.8465019315481186e-05 Validation Loss 3.6829944292549044e-05\n",
            "Epoch 82: 0 -> Training Loss 3.5373683203943074e-05 Validation Loss 3.67846769222524e-05\n",
            "Epoch 82: 4 -> Training Loss 3.689021468744613e-05 Validation Loss 3.674034451250918e-05\n",
            "Epoch 82: 8 -> Training Loss 3.6494177038548514e-05 Validation Loss 3.669590660138056e-05\n",
            "Epoch 82: 12 -> Training Loss 3.931293758796528e-05 Validation Loss 3.6651901609729975e-05\n",
            "Epoch 82: 16 -> Training Loss 3.2597818062640727e-05 Validation Loss 3.6610683309845626e-05\n",
            "Epoch 82: 20 -> Training Loss 3.9748905692249537e-05 Validation Loss 3.657156048575416e-05\n",
            "Epoch 82: 24 -> Training Loss 3.686689160531387e-05 Validation Loss 3.653138628578745e-05\n",
            "Epoch 82: 28 -> Training Loss 3.833285882137716e-05 Validation Loss 3.649100108304992e-05\n",
            "Epoch 82: 32 -> Training Loss 3.6596800782717764e-05 Validation Loss 3.6450182960834354e-05\n",
            "Epoch 82: 36 -> Training Loss 3.7630870792781934e-05 Validation Loss 3.640960494522005e-05\n",
            "Epoch 82: 40 -> Training Loss 3.957436274504289e-05 Validation Loss 3.637065674411133e-05\n",
            "Epoch 82: 44 -> Training Loss 3.865532926283777e-05 Validation Loss 3.633206142694689e-05\n",
            "Epoch 82: 48 -> Training Loss 3.582147473935038e-05 Validation Loss 3.629334969446063e-05\n",
            "Epoch 82: 52 -> Training Loss 3.617032780312002e-05 Validation Loss 3.6254728911444545e-05\n",
            "Epoch 82: 56 -> Training Loss 3.245001789764501e-05 Validation Loss 3.62150531145744e-05\n",
            "Epoch 82: 60 -> Training Loss 3.496262797852978e-05 Validation Loss 3.617592665250413e-05\n",
            "Epoch 82: 64 -> Training Loss 3.386477328604087e-05 Validation Loss 3.613865192164667e-05\n",
            "Epoch 82: 68 -> Training Loss 3.850139182759449e-05 Validation Loss 3.6103083402849734e-05\n",
            "Epoch 82: 72 -> Training Loss 3.493117037578486e-05 Validation Loss 3.606613609008491e-05\n",
            "Epoch 82: 76 -> Training Loss 3.854145688819699e-05 Validation Loss 3.6030778574058786e-05\n",
            "Epoch 82: 80 -> Training Loss 3.4118966141249985e-05 Validation Loss 3.5994929930893704e-05\n",
            "Epoch 82: 84 -> Training Loss 3.206043038517237e-05 Validation Loss 3.595963789848611e-05\n",
            "Epoch 82: 88 -> Training Loss 3.5615441447589546e-05 Validation Loss 3.592535722418688e-05\n",
            "Epoch 82: 92 -> Training Loss 3.709405427798629e-05 Validation Loss 3.5889854189008474e-05\n",
            "Epoch 82: 96 -> Training Loss 3.5570938052842394e-05 Validation Loss 3.585481317713857e-05\n",
            "Epoch 83: 0 -> Training Loss 3.685625415528193e-05 Validation Loss 3.582006320357323e-05\n",
            "Epoch 83: 4 -> Training Loss 3.2765667128842324e-05 Validation Loss 3.5784563806373626e-05\n",
            "Epoch 83: 8 -> Training Loss 3.4599412174429744e-05 Validation Loss 3.574945731088519e-05\n",
            "Epoch 83: 12 -> Training Loss 3.5244727769168094e-05 Validation Loss 3.5715402191272005e-05\n",
            "Epoch 83: 16 -> Training Loss 3.956771979574114e-05 Validation Loss 3.56813870894257e-05\n",
            "Epoch 83: 20 -> Training Loss 3.678848224808462e-05 Validation Loss 3.564851431292482e-05\n",
            "Epoch 83: 24 -> Training Loss 3.6605830246116966e-05 Validation Loss 3.561628545867279e-05\n",
            "Epoch 83: 28 -> Training Loss 3.5405166272539645e-05 Validation Loss 3.5583143471740186e-05\n",
            "Epoch 83: 32 -> Training Loss 3.48494213540107e-05 Validation Loss 3.555035073077306e-05\n",
            "Epoch 83: 36 -> Training Loss 3.5351116821402684e-05 Validation Loss 3.551845293259248e-05\n",
            "Epoch 83: 40 -> Training Loss 3.6133053072262555e-05 Validation Loss 3.548652966856025e-05\n",
            "Epoch 83: 44 -> Training Loss 3.7573314330074936e-05 Validation Loss 3.5454289900371805e-05\n",
            "Epoch 83: 48 -> Training Loss 3.3502532460261136e-05 Validation Loss 3.542192280292511e-05\n",
            "Epoch 83: 52 -> Training Loss 3.4172415325883776e-05 Validation Loss 3.5389966797083616e-05\n",
            "Epoch 83: 56 -> Training Loss 3.6261644709156826e-05 Validation Loss 3.535874202498235e-05\n",
            "Epoch 83: 60 -> Training Loss 3.731926699401811e-05 Validation Loss 3.532785194693133e-05\n",
            "Epoch 83: 64 -> Training Loss 3.394456871319562e-05 Validation Loss 3.5296463465783745e-05\n",
            "Epoch 83: 68 -> Training Loss 4.0433726098854095e-05 Validation Loss 3.526511864038184e-05\n",
            "Epoch 83: 72 -> Training Loss 4.048141272505745e-05 Validation Loss 3.5235083487350494e-05\n",
            "Epoch 83: 76 -> Training Loss 3.551059853634797e-05 Validation Loss 3.520521931932308e-05\n",
            "Epoch 83: 80 -> Training Loss 3.474015829851851e-05 Validation Loss 3.517439472489059e-05\n",
            "Epoch 83: 84 -> Training Loss 3.42683051712811e-05 Validation Loss 3.5143639252055436e-05\n",
            "Epoch 83: 88 -> Training Loss 3.3689269912429154e-05 Validation Loss 3.511316390358843e-05\n",
            "Epoch 83: 92 -> Training Loss 3.4047763620037585e-05 Validation Loss 3.508311783662066e-05\n",
            "Epoch 83: 96 -> Training Loss 3.1109098927117884e-05 Validation Loss 3.505438144202344e-05\n",
            "Epoch 84: 0 -> Training Loss 3.371557977516204e-05 Validation Loss 3.502725303405896e-05\n",
            "Epoch 84: 4 -> Training Loss 3.3187796361744404e-05 Validation Loss 3.500068123685196e-05\n",
            "Epoch 84: 8 -> Training Loss 3.444057438173331e-05 Validation Loss 3.497483703540638e-05\n",
            "Epoch 84: 12 -> Training Loss 3.564430517144501e-05 Validation Loss 3.494896009215154e-05\n",
            "Epoch 84: 16 -> Training Loss 3.3463842555647716e-05 Validation Loss 3.492481482680887e-05\n",
            "Epoch 84: 20 -> Training Loss 3.614743400248699e-05 Validation Loss 3.490023300400935e-05\n",
            "Epoch 84: 24 -> Training Loss 3.1964376830728725e-05 Validation Loss 3.487629146547988e-05\n",
            "Epoch 84: 28 -> Training Loss 3.785911394516006e-05 Validation Loss 3.485326305963099e-05\n",
            "Epoch 84: 32 -> Training Loss 3.608207043725997e-05 Validation Loss 3.4829263313440606e-05\n",
            "Epoch 84: 36 -> Training Loss 3.569214095477946e-05 Validation Loss 3.480465238681063e-05\n",
            "Epoch 84: 40 -> Training Loss 3.3491112844785675e-05 Validation Loss 3.477976133581251e-05\n",
            "Epoch 84: 44 -> Training Loss 3.5530705645214766e-05 Validation Loss 3.475567064015195e-05\n",
            "Epoch 84: 48 -> Training Loss 3.274308983236551e-05 Validation Loss 3.4731368941720575e-05\n",
            "Epoch 84: 52 -> Training Loss 3.555274452082813e-05 Validation Loss 3.4705473808571696e-05\n",
            "Epoch 84: 56 -> Training Loss 3.7017678550910205e-05 Validation Loss 3.467976057436317e-05\n",
            "Epoch 84: 60 -> Training Loss 3.7571102438960224e-05 Validation Loss 3.4654862247407436e-05\n",
            "Epoch 84: 64 -> Training Loss 3.312804619781673e-05 Validation Loss 3.463147004367784e-05\n",
            "Epoch 84: 68 -> Training Loss 3.406719770282507e-05 Validation Loss 3.46086235367693e-05\n",
            "Epoch 84: 72 -> Training Loss 3.3589796657906845e-05 Validation Loss 3.458559513092041e-05\n",
            "Epoch 84: 76 -> Training Loss 3.7123616493772715e-05 Validation Loss 3.456216654740274e-05\n",
            "Epoch 84: 80 -> Training Loss 3.470048977760598e-05 Validation Loss 3.453915269346908e-05\n",
            "Epoch 84: 84 -> Training Loss 3.733151970664039e-05 Validation Loss 3.4516877349233255e-05\n",
            "Epoch 84: 88 -> Training Loss 3.41433405992575e-05 Validation Loss 3.44943291565869e-05\n",
            "Epoch 84: 92 -> Training Loss 3.356251909281127e-05 Validation Loss 3.4471820981707424e-05\n",
            "Epoch 84: 96 -> Training Loss 3.115961590083316e-05 Validation Loss 3.4449090890120715e-05\n",
            "Epoch 85: 0 -> Training Loss 3.3721840736689046e-05 Validation Loss 3.4427437640260905e-05\n",
            "Epoch 85: 4 -> Training Loss 3.728064621100202e-05 Validation Loss 3.440612636040896e-05\n",
            "Epoch 85: 8 -> Training Loss 3.6151915992377326e-05 Validation Loss 3.438509884290397e-05\n",
            "Epoch 85: 12 -> Training Loss 3.5925178963225335e-05 Validation Loss 3.436373299336992e-05\n",
            "Epoch 85: 16 -> Training Loss 3.628284684964456e-05 Validation Loss 3.434284735703841e-05\n",
            "Epoch 85: 20 -> Training Loss 3.497723082546145e-05 Validation Loss 3.432264566072263e-05\n",
            "Epoch 85: 24 -> Training Loss 3.674034815048799e-05 Validation Loss 3.4302422136534005e-05\n",
            "Epoch 85: 28 -> Training Loss 3.4074797440553084e-05 Validation Loss 3.4281008993275464e-05\n",
            "Epoch 85: 32 -> Training Loss 3.733438643394038e-05 Validation Loss 3.425919567234814e-05\n",
            "Epoch 85: 36 -> Training Loss 3.575511072995141e-05 Validation Loss 3.4238139051012695e-05\n",
            "Epoch 85: 40 -> Training Loss 4.173248817096464e-05 Validation Loss 3.421762812649831e-05\n",
            "Epoch 85: 44 -> Training Loss 3.64232728315983e-05 Validation Loss 3.419832501094788e-05\n",
            "Epoch 85: 48 -> Training Loss 3.399579873075709e-05 Validation Loss 3.41802260663826e-05\n",
            "Epoch 85: 52 -> Training Loss 3.6295095924288034e-05 Validation Loss 3.4163647796958685e-05\n",
            "Epoch 85: 56 -> Training Loss 3.339980321470648e-05 Validation Loss 3.4146461985073984e-05\n",
            "Epoch 85: 60 -> Training Loss 3.384417505003512e-05 Validation Loss 3.412874502828345e-05\n",
            "Epoch 85: 64 -> Training Loss 3.571373235899955e-05 Validation Loss 3.411228317418136e-05\n",
            "Epoch 85: 68 -> Training Loss 3.256688796682283e-05 Validation Loss 3.409602868487127e-05\n",
            "Epoch 85: 72 -> Training Loss 3.563473728718236e-05 Validation Loss 3.407897020224482e-05\n",
            "Epoch 85: 76 -> Training Loss 3.1811439839657396e-05 Validation Loss 3.40633159794379e-05\n",
            "Epoch 85: 80 -> Training Loss 3.445279799052514e-05 Validation Loss 3.404733433853835e-05\n",
            "Epoch 85: 84 -> Training Loss 3.443401146796532e-05 Validation Loss 3.403059963602573e-05\n",
            "Epoch 85: 88 -> Training Loss 3.230026413802989e-05 Validation Loss 3.401451976969838e-05\n",
            "Epoch 85: 92 -> Training Loss 3.7872654502280056e-05 Validation Loss 3.399909837753512e-05\n",
            "Epoch 85: 96 -> Training Loss 3.7033576518297195e-05 Validation Loss 3.398399712750688e-05\n",
            "Epoch 86: 0 -> Training Loss 3.226487024221569e-05 Validation Loss 3.3968361094594e-05\n",
            "Epoch 86: 4 -> Training Loss 3.907360223820433e-05 Validation Loss 3.395261592231691e-05\n",
            "Epoch 86: 8 -> Training Loss 3.481073508737609e-05 Validation Loss 3.393712904653512e-05\n",
            "Epoch 86: 12 -> Training Loss 3.374226071173325e-05 Validation Loss 3.392246435396373e-05\n",
            "Epoch 86: 16 -> Training Loss 3.474222103250213e-05 Validation Loss 3.390840720385313e-05\n",
            "Epoch 86: 20 -> Training Loss 3.399267006898299e-05 Validation Loss 3.389481935300864e-05\n",
            "Epoch 86: 24 -> Training Loss 3.2992717024171725e-05 Validation Loss 3.3879856346175075e-05\n",
            "Epoch 86: 28 -> Training Loss 3.891221058438532e-05 Validation Loss 3.386542448424734e-05\n",
            "Epoch 86: 32 -> Training Loss 3.30450275214389e-05 Validation Loss 3.3851192711154e-05\n",
            "Epoch 86: 36 -> Training Loss 3.5754055716097355e-05 Validation Loss 3.383608418516815e-05\n",
            "Epoch 86: 40 -> Training Loss 3.2879484933800995e-05 Validation Loss 3.381989517947659e-05\n",
            "Epoch 86: 44 -> Training Loss 3.4305496228626e-05 Validation Loss 3.380367343197577e-05\n",
            "Epoch 86: 48 -> Training Loss 3.383137300261296e-05 Validation Loss 3.378822293598205e-05\n",
            "Epoch 86: 52 -> Training Loss 3.4489617974031717e-05 Validation Loss 3.3772426832001656e-05\n",
            "Epoch 86: 56 -> Training Loss 3.278867370681837e-05 Validation Loss 3.375709638930857e-05\n",
            "Epoch 86: 60 -> Training Loss 3.4864613553509116e-05 Validation Loss 3.3743221138138324e-05\n",
            "Epoch 86: 64 -> Training Loss 3.4380667784716934e-05 Validation Loss 3.372960782144219e-05\n",
            "Epoch 86: 68 -> Training Loss 3.2817326427903026e-05 Validation Loss 3.371685306774452e-05\n",
            "Epoch 86: 72 -> Training Loss 3.662479139165953e-05 Validation Loss 3.3704527595546097e-05\n",
            "Epoch 86: 76 -> Training Loss 3.521156031638384e-05 Validation Loss 3.369286787346937e-05\n",
            "Epoch 86: 80 -> Training Loss 3.324903082102537e-05 Validation Loss 3.3680873457342386e-05\n",
            "Epoch 86: 84 -> Training Loss 3.415496030356735e-05 Validation Loss 3.366852251929231e-05\n",
            "Epoch 86: 88 -> Training Loss 3.517552249832079e-05 Validation Loss 3.365544034750201e-05\n",
            "Epoch 86: 92 -> Training Loss 3.22835985571146e-05 Validation Loss 3.3642158086877316e-05\n",
            "Epoch 86: 96 -> Training Loss 3.29951144522056e-05 Validation Loss 3.363040741533041e-05\n",
            "Epoch 87: 0 -> Training Loss 3.1323790608439595e-05 Validation Loss 3.3618904126342386e-05\n",
            "Epoch 87: 4 -> Training Loss 3.662089875433594e-05 Validation Loss 3.360830305609852e-05\n",
            "Epoch 87: 8 -> Training Loss 3.490149538265541e-05 Validation Loss 3.359802212798968e-05\n",
            "Epoch 87: 12 -> Training Loss 3.414192542550154e-05 Validation Loss 3.358871254022233e-05\n",
            "Epoch 87: 16 -> Training Loss 3.1964875233825296e-05 Validation Loss 3.3578475267859176e-05\n",
            "Epoch 87: 20 -> Training Loss 3.597454633563757e-05 Validation Loss 3.35680233547464e-05\n",
            "Epoch 87: 24 -> Training Loss 3.451632073847577e-05 Validation Loss 3.3557396818650886e-05\n",
            "Epoch 87: 28 -> Training Loss 3.401266803848557e-05 Validation Loss 3.35469376295805e-05\n",
            "Epoch 87: 32 -> Training Loss 3.721386019606143e-05 Validation Loss 3.353607462486252e-05\n",
            "Epoch 87: 36 -> Training Loss 3.540740362950601e-05 Validation Loss 3.352526982780546e-05\n",
            "Epoch 87: 40 -> Training Loss 3.298522642580792e-05 Validation Loss 3.351437408127822e-05\n",
            "Epoch 87: 44 -> Training Loss 3.3833573979791254e-05 Validation Loss 3.350289262016304e-05\n",
            "Epoch 87: 48 -> Training Loss 3.683909017127007e-05 Validation Loss 3.349188045831397e-05\n",
            "Epoch 87: 52 -> Training Loss 3.508877853164449e-05 Validation Loss 3.3481581340311095e-05\n",
            "Epoch 87: 56 -> Training Loss 3.0864008294884115e-05 Validation Loss 3.3470634662080556e-05\n",
            "Epoch 87: 60 -> Training Loss 3.4688189771259204e-05 Validation Loss 3.346092125866562e-05\n",
            "Epoch 87: 64 -> Training Loss 3.6066929169464856e-05 Validation Loss 3.345128789078444e-05\n",
            "Epoch 87: 68 -> Training Loss 3.3657794119790196e-05 Validation Loss 3.3442527637816966e-05\n",
            "Epoch 87: 72 -> Training Loss 3.7202044040896e-05 Validation Loss 3.343340358696878e-05\n",
            "Epoch 87: 76 -> Training Loss 3.2603926229057834e-05 Validation Loss 3.342425043229014e-05\n",
            "Epoch 87: 80 -> Training Loss 3.482289321254939e-05 Validation Loss 3.341546471347101e-05\n",
            "Epoch 87: 84 -> Training Loss 3.3193075068993494e-05 Validation Loss 3.340575494803488e-05\n",
            "Epoch 87: 88 -> Training Loss 3.495356213534251e-05 Validation Loss 3.339672548463568e-05\n",
            "Epoch 87: 92 -> Training Loss 3.342962736496702e-05 Validation Loss 3.338788519613445e-05\n",
            "Epoch 87: 96 -> Training Loss 3.6983867175877094e-05 Validation Loss 3.3379146771039814e-05\n",
            "Epoch 88: 0 -> Training Loss 3.554958675522357e-05 Validation Loss 3.3371001336490735e-05\n",
            "Epoch 88: 4 -> Training Loss 3.4089331165887415e-05 Validation Loss 3.336293593747541e-05\n",
            "Epoch 88: 8 -> Training Loss 3.369628393556923e-05 Validation Loss 3.335418296046555e-05\n",
            "Epoch 88: 12 -> Training Loss 3.471616582828574e-05 Validation Loss 3.33447169396095e-05\n",
            "Epoch 88: 16 -> Training Loss 3.9443999412469566e-05 Validation Loss 3.3335250918753445e-05\n",
            "Epoch 88: 20 -> Training Loss 3.48779940395616e-05 Validation Loss 3.332638516440056e-05\n",
            "Epoch 88: 24 -> Training Loss 3.40966071235016e-05 Validation Loss 3.331735933898017e-05\n",
            "Epoch 88: 28 -> Training Loss 3.6517907574307173e-05 Validation Loss 3.330775143695064e-05\n",
            "Epoch 88: 32 -> Training Loss 3.561354969860986e-05 Validation Loss 3.329814353492111e-05\n",
            "Epoch 88: 36 -> Training Loss 3.392563667148352e-05 Validation Loss 3.328873935970478e-05\n",
            "Epoch 88: 40 -> Training Loss 3.4563061490189284e-05 Validation Loss 3.327974263811484e-05\n",
            "Epoch 88: 44 -> Training Loss 3.0332745154737495e-05 Validation Loss 3.3271542633883655e-05\n",
            "Epoch 88: 48 -> Training Loss 3.482242755126208e-05 Validation Loss 3.326260775793344e-05\n",
            "Epoch 88: 52 -> Training Loss 3.775275399675593e-05 Validation Loss 3.325443685753271e-05\n",
            "Epoch 88: 56 -> Training Loss 3.36841112584807e-05 Validation Loss 3.324615681776777e-05\n",
            "Epoch 88: 60 -> Training Loss 3.046575511689298e-05 Validation Loss 3.323810233268887e-05\n",
            "Epoch 88: 64 -> Training Loss 3.129392280243337e-05 Validation Loss 3.323105920571834e-05\n",
            "Epoch 88: 68 -> Training Loss 3.210786235285923e-05 Validation Loss 3.322645716252737e-05\n",
            "Epoch 88: 72 -> Training Loss 3.323910277686082e-05 Validation Loss 3.3221469493582845e-05\n",
            "Epoch 88: 76 -> Training Loss 3.0141958632157184e-05 Validation Loss 3.321556869195774e-05\n",
            "Epoch 88: 80 -> Training Loss 3.370812555658631e-05 Validation Loss 3.3209886169061065e-05\n",
            "Epoch 88: 84 -> Training Loss 3.282225952716544e-05 Validation Loss 3.32030831486918e-05\n",
            "Epoch 88: 88 -> Training Loss 3.4209617297165096e-05 Validation Loss 3.319669849588536e-05\n",
            "Epoch 88: 92 -> Training Loss 3.313778142910451e-05 Validation Loss 3.3190397516591474e-05\n",
            "Epoch 88: 96 -> Training Loss 3.492899122647941e-05 Validation Loss 3.3183918276336044e-05\n",
            "Epoch 89: 0 -> Training Loss 3.3443877327954397e-05 Validation Loss 3.31771225319244e-05\n",
            "Epoch 89: 4 -> Training Loss 3.482806641841307e-05 Validation Loss 3.3170308597618714e-05\n",
            "Epoch 89: 8 -> Training Loss 3.5874916648026556e-05 Validation Loss 3.316328366054222e-05\n",
            "Epoch 89: 12 -> Training Loss 3.3132753742393106e-05 Validation Loss 3.315535286674276e-05\n",
            "Epoch 89: 16 -> Training Loss 3.3356904168613255e-05 Validation Loss 3.3148702641483396e-05\n",
            "Epoch 89: 20 -> Training Loss 3.2890206057345495e-05 Validation Loss 3.314164132461883e-05\n",
            "Epoch 89: 24 -> Training Loss 3.276797360740602e-05 Validation Loss 3.313469642307609e-05\n",
            "Epoch 89: 28 -> Training Loss 3.512170951580629e-05 Validation Loss 3.312851185910404e-05\n",
            "Epoch 89: 32 -> Training Loss 3.359914990141988e-05 Validation Loss 3.312343324068934e-05\n",
            "Epoch 89: 36 -> Training Loss 3.336240479256958e-05 Validation Loss 3.3117983548436314e-05\n",
            "Epoch 89: 40 -> Training Loss 3.175934762111865e-05 Validation Loss 3.311198088340461e-05\n",
            "Epoch 89: 44 -> Training Loss 3.4174794564023614e-05 Validation Loss 3.310601459816098e-05\n",
            "Epoch 89: 48 -> Training Loss 3.317484515719116e-05 Validation Loss 3.310106330900453e-05\n",
            "Epoch 89: 52 -> Training Loss 3.816268144873902e-05 Validation Loss 3.30961738654878e-05\n",
            "Epoch 89: 56 -> Training Loss 3.392367216292769e-05 Validation Loss 3.3090862416429445e-05\n",
            "Epoch 89: 60 -> Training Loss 3.3945354516617954e-05 Validation Loss 3.308567829662934e-05\n",
            "Epoch 89: 64 -> Training Loss 3.6231860576663166e-05 Validation Loss 3.3082029403885826e-05\n",
            "Epoch 89: 68 -> Training Loss 3.4135715395677835e-05 Validation Loss 3.307785664219409e-05\n",
            "Epoch 89: 72 -> Training Loss 3.325562283862382e-05 Validation Loss 3.3073651138693094e-05\n",
            "Epoch 89: 76 -> Training Loss 2.9188580811023712e-05 Validation Loss 3.306956568849273e-05\n",
            "Epoch 89: 80 -> Training Loss 3.1846589990891516e-05 Validation Loss 3.3064206945709884e-05\n",
            "Epoch 89: 84 -> Training Loss 3.193183511029929e-05 Validation Loss 3.3058728149626404e-05\n",
            "Epoch 89: 88 -> Training Loss 3.367779208929278e-05 Validation Loss 3.3054588129743934e-05\n",
            "Epoch 89: 92 -> Training Loss 3.5059667425230145e-05 Validation Loss 3.3049622288672253e-05\n",
            "Epoch 89: 96 -> Training Loss 3.511517934384756e-05 Validation Loss 3.304449637653306e-05\n",
            "Epoch 90: 0 -> Training Loss 3.2126867154147476e-05 Validation Loss 3.304061101516709e-05\n",
            "Epoch 90: 4 -> Training Loss 3.5246390325482935e-05 Validation Loss 3.303677658550441e-05\n",
            "Epoch 90: 8 -> Training Loss 3.6377969081513584e-05 Validation Loss 3.303134144516662e-05\n",
            "Epoch 90: 12 -> Training Loss 3.52530405507423e-05 Validation Loss 3.3026291930582374e-05\n",
            "Epoch 90: 16 -> Training Loss 3.433392339502461e-05 Validation Loss 3.302218465250917e-05\n",
            "Epoch 90: 20 -> Training Loss 3.414262755541131e-05 Validation Loss 3.3018208341673017e-05\n",
            "Epoch 90: 24 -> Training Loss 3.0407703889068216e-05 Validation Loss 3.301361357443966e-05\n",
            "Epoch 90: 28 -> Training Loss 3.492603718768805e-05 Validation Loss 3.300816024420783e-05\n",
            "Epoch 90: 32 -> Training Loss 3.74690571334213e-05 Validation Loss 3.3002885174937546e-05\n",
            "Epoch 90: 36 -> Training Loss 2.9321776310098357e-05 Validation Loss 3.2998959795804694e-05\n",
            "Epoch 90: 40 -> Training Loss 3.535057476256043e-05 Validation Loss 3.29952708852943e-05\n",
            "Epoch 90: 44 -> Training Loss 3.31302362610586e-05 Validation Loss 3.299078525742516e-05\n",
            "Epoch 90: 48 -> Training Loss 3.4872969990829006e-05 Validation Loss 3.298697265563533e-05\n",
            "Epoch 90: 52 -> Training Loss 3.299324453109875e-05 Validation Loss 3.298275623819791e-05\n",
            "Epoch 90: 56 -> Training Loss 3.184948582202196e-05 Validation Loss 3.297845250926912e-05\n",
            "Epoch 90: 60 -> Training Loss 3.576814924599603e-05 Validation Loss 3.29744434566237e-05\n",
            "Epoch 90: 64 -> Training Loss 3.230637958040461e-05 Validation Loss 3.297079092590138e-05\n",
            "Epoch 90: 68 -> Training Loss 3.540033503668383e-05 Validation Loss 3.296658906037919e-05\n",
            "Epoch 90: 72 -> Training Loss 3.1282710551749915e-05 Validation Loss 3.2962219847831875e-05\n",
            "Epoch 90: 76 -> Training Loss 3.2332078262697905e-05 Validation Loss 3.295869828434661e-05\n",
            "Epoch 90: 80 -> Training Loss 3.433900565141812e-05 Validation Loss 3.295527130831033e-05\n",
            "Epoch 90: 84 -> Training Loss 3.547348751453683e-05 Validation Loss 3.295132773928344e-05\n",
            "Epoch 90: 88 -> Training Loss 3.2973577617667615e-05 Validation Loss 3.294835187261924e-05\n",
            "Epoch 90: 92 -> Training Loss 3.617401671363041e-05 Validation Loss 3.294464841019362e-05\n",
            "Epoch 90: 96 -> Training Loss 3.2412350265076384e-05 Validation Loss 3.294113412266597e-05\n",
            "Epoch 91: 0 -> Training Loss 3.200781065970659e-05 Validation Loss 3.293872214271687e-05\n",
            "Epoch 91: 4 -> Training Loss 3.30255352309905e-05 Validation Loss 3.2935138733591884e-05\n",
            "Epoch 91: 8 -> Training Loss 3.7682770198443905e-05 Validation Loss 3.293048939667642e-05\n",
            "Epoch 91: 12 -> Training Loss 3.563414793461561e-05 Validation Loss 3.2926145649980754e-05\n",
            "Epoch 91: 16 -> Training Loss 3.1914740247884765e-05 Validation Loss 3.292231122031808e-05\n",
            "Epoch 91: 20 -> Training Loss 3.3733413147274405e-05 Validation Loss 3.291798202553764e-05\n",
            "Epoch 91: 24 -> Training Loss 3.424850001465529e-05 Validation Loss 3.291502798674628e-05\n",
            "Epoch 91: 28 -> Training Loss 3.300858224974945e-05 Validation Loss 3.2913259929046035e-05\n",
            "Epoch 91: 32 -> Training Loss 3.4514694561949e-05 Validation Loss 3.291097527835518e-05\n",
            "Epoch 91: 36 -> Training Loss 3.419465429033153e-05 Validation Loss 3.290781387477182e-05\n",
            "Epoch 91: 40 -> Training Loss 3.1976560421753675e-05 Validation Loss 3.290513268439099e-05\n",
            "Epoch 91: 44 -> Training Loss 3.251468297094107e-05 Validation Loss 3.2902426028158516e-05\n",
            "Epoch 91: 48 -> Training Loss 3.1144460081122816e-05 Validation Loss 3.289969026809558e-05\n",
            "Epoch 91: 52 -> Training Loss 3.4117216273443773e-05 Validation Loss 3.289677988504991e-05\n",
            "Epoch 91: 56 -> Training Loss 3.319114330224693e-05 Validation Loss 3.28946734953206e-05\n",
            "Epoch 91: 60 -> Training Loss 3.347997699165717e-05 Validation Loss 3.289287269581109e-05\n",
            "Epoch 91: 64 -> Training Loss 3.5999186366098e-05 Validation Loss 3.289152300567366e-05\n",
            "Epoch 91: 68 -> Training Loss 3.592736175050959e-05 Validation Loss 3.289076266810298e-05\n",
            "Epoch 91: 72 -> Training Loss 3.1180763471638784e-05 Validation Loss 3.2889092835830525e-05\n",
            "Epoch 91: 76 -> Training Loss 3.383092553121969e-05 Validation Loss 3.288561129011214e-05\n",
            "Epoch 91: 80 -> Training Loss 3.5031796869589016e-05 Validation Loss 3.288137668278068e-05\n",
            "Epoch 91: 84 -> Training Loss 3.219905192963779e-05 Validation Loss 3.28767164319288e-05\n",
            "Epoch 91: 88 -> Training Loss 3.4381479053990915e-05 Validation Loss 3.2872307201614603e-05\n",
            "Epoch 91: 92 -> Training Loss 3.515638309181668e-05 Validation Loss 3.286880019004457e-05\n",
            "Epoch 91: 96 -> Training Loss 3.567757812561467e-05 Validation Loss 3.2866500987438485e-05\n",
            "Epoch 92: 0 -> Training Loss 3.256768104620278e-05 Validation Loss 3.2863950764294714e-05\n",
            "Epoch 92: 4 -> Training Loss 3.4693355701165274e-05 Validation Loss 3.286136416136287e-05\n",
            "Epoch 92: 8 -> Training Loss 3.42327548423782e-05 Validation Loss 3.2858384656719863e-05\n",
            "Epoch 92: 12 -> Training Loss 3.4475007851142436e-05 Validation Loss 3.285723505541682e-05\n",
            "Epoch 92: 16 -> Training Loss 3.6071181966690347e-05 Validation Loss 3.285516140749678e-05\n",
            "Epoch 92: 20 -> Training Loss 3.3093776437453926e-05 Validation Loss 3.285349521320313e-05\n",
            "Epoch 92: 24 -> Training Loss 3.135246515739709e-05 Validation Loss 3.285139609943144e-05\n",
            "Epoch 92: 28 -> Training Loss 3.2891242881305516e-05 Validation Loss 3.2849813578650355e-05\n",
            "Epoch 92: 32 -> Training Loss 3.5962031688541174e-05 Validation Loss 3.2848925911821425e-05\n",
            "Epoch 92: 36 -> Training Loss 3.2080122764455155e-05 Validation Loss 3.2847601687535644e-05\n",
            "Epoch 92: 40 -> Training Loss 3.2910393201746047e-05 Validation Loss 3.284568811068311e-05\n",
            "Epoch 92: 44 -> Training Loss 2.973774098791182e-05 Validation Loss 3.284351259935647e-05\n",
            "Epoch 92: 48 -> Training Loss 3.2956439099507406e-05 Validation Loss 3.284083504695445e-05\n",
            "Epoch 92: 52 -> Training Loss 3.346905214129947e-05 Validation Loss 3.2836418540682644e-05\n",
            "Epoch 92: 56 -> Training Loss 3.460215521045029e-05 Validation Loss 3.28320347762201e-05\n",
            "Epoch 92: 60 -> Training Loss 3.3943651942536235e-05 Validation Loss 3.282869147369638e-05\n",
            "Epoch 92: 64 -> Training Loss 3.787609966821037e-05 Validation Loss 3.282638863311149e-05\n",
            "Epoch 92: 68 -> Training Loss 3.7157929909881204e-05 Validation Loss 3.2824184017954394e-05\n",
            "Epoch 92: 72 -> Training Loss 3.324666613480076e-05 Validation Loss 3.282099714851938e-05\n",
            "Epoch 92: 76 -> Training Loss 3.0045259336475283e-05 Validation Loss 3.2817777537275106e-05\n",
            "Epoch 92: 80 -> Training Loss 3.2551310141570866e-05 Validation Loss 3.281526733189821e-05\n",
            "Epoch 92: 84 -> Training Loss 3.54062212863937e-05 Validation Loss 3.281352837802842e-05\n",
            "Epoch 92: 88 -> Training Loss 3.259300137870014e-05 Validation Loss 3.281259705545381e-05\n",
            "Epoch 92: 92 -> Training Loss 3.5459517675917596e-05 Validation Loss 3.281131648691371e-05\n",
            "Epoch 92: 96 -> Training Loss 3.4740802220767364e-05 Validation Loss 3.2810385164339095e-05\n",
            "Epoch 93: 0 -> Training Loss 3.560277400538325e-05 Validation Loss 3.2808908144943416e-05\n",
            "Epoch 93: 4 -> Training Loss 3.379324698471464e-05 Validation Loss 3.2807045499794185e-05\n",
            "Epoch 93: 8 -> Training Loss 3.26097069773823e-05 Validation Loss 3.280553937656805e-05\n",
            "Epoch 93: 12 -> Training Loss 3.724666385096498e-05 Validation Loss 3.2803156500449404e-05\n",
            "Epoch 93: 16 -> Training Loss 3.382114664418623e-05 Validation Loss 3.2800038752611727e-05\n",
            "Epoch 93: 20 -> Training Loss 3.296259819762781e-05 Validation Loss 3.2797615858726203e-05\n",
            "Epoch 93: 24 -> Training Loss 3.425508475629613e-05 Validation Loss 3.2795451261335984e-05\n",
            "Epoch 93: 28 -> Training Loss 3.531033144099638e-05 Validation Loss 3.279282827861607e-05\n",
            "Epoch 93: 32 -> Training Loss 3.3246949897147715e-05 Validation Loss 3.2790085242595524e-05\n",
            "Epoch 93: 36 -> Training Loss 3.349505641381256e-05 Validation Loss 3.2787454983917996e-05\n",
            "Epoch 93: 40 -> Training Loss 3.5442342777969316e-05 Validation Loss 3.278470830991864e-05\n",
            "Epoch 93: 44 -> Training Loss 3.73359362129122e-05 Validation Loss 3.278257281635888e-05\n",
            "Epoch 93: 48 -> Training Loss 3.278698568465188e-05 Validation Loss 3.2781463232822716e-05\n",
            "Epoch 93: 52 -> Training Loss 3.424571332288906e-05 Validation Loss 3.278023359598592e-05\n",
            "Epoch 93: 56 -> Training Loss 3.4123491786886007e-05 Validation Loss 3.278029180364683e-05\n",
            "Epoch 93: 60 -> Training Loss 3.253998147556558e-05 Validation Loss 3.27804300468415e-05\n",
            "Epoch 93: 64 -> Training Loss 3.582517456379719e-05 Validation Loss 3.2780139008536935e-05\n",
            "Epoch 93: 68 -> Training Loss 3.3544551115483046e-05 Validation Loss 3.278021176811308e-05\n",
            "Epoch 93: 72 -> Training Loss 3.596996975829825e-05 Validation Loss 3.2779687899164855e-05\n",
            "Epoch 93: 76 -> Training Loss 3.360151458764449e-05 Validation Loss 3.27786328853108e-05\n",
            "Epoch 93: 80 -> Training Loss 3.785428998526186e-05 Validation Loss 3.277874566265382e-05\n",
            "Epoch 93: 84 -> Training Loss 3.358865069458261e-05 Validation Loss 3.277958603575826e-05\n",
            "Epoch 93: 88 -> Training Loss 3.322513657622039e-05 Validation Loss 3.277939686086029e-05\n",
            "Epoch 93: 92 -> Training Loss 3.314765126560815e-05 Validation Loss 3.277946962043643e-05\n",
            "Epoch 93: 96 -> Training Loss 3.4828524803742766e-05 Validation Loss 3.2779011235106736e-05\n",
            "Epoch 94: 0 -> Training Loss 3.1352785299532115e-05 Validation Loss 3.27773523167707e-05\n",
            "Epoch 94: 4 -> Training Loss 3.405669122003019e-05 Validation Loss 3.277535506640561e-05\n",
            "Epoch 94: 8 -> Training Loss 3.344313154229894e-05 Validation Loss 3.277360156062059e-05\n",
            "Epoch 94: 12 -> Training Loss 3.317536538816057e-05 Validation Loss 3.277207724750042e-05\n",
            "Epoch 94: 16 -> Training Loss 3.540228499332443e-05 Validation Loss 3.277171708759852e-05\n",
            "Epoch 94: 20 -> Training Loss 3.3831456676125526e-05 Validation Loss 3.2769843528512865e-05\n",
            "Epoch 94: 24 -> Training Loss 3.72059948858805e-05 Validation Loss 3.276643110439181e-05\n",
            "Epoch 94: 28 -> Training Loss 3.484233093331568e-05 Validation Loss 3.27635498251766e-05\n",
            "Epoch 94: 32 -> Training Loss 3.471125819487497e-05 Validation Loss 3.2761323382146657e-05\n",
            "Epoch 94: 36 -> Training Loss 3.2472140446770936e-05 Validation Loss 3.2759675377747044e-05\n",
            "Epoch 94: 40 -> Training Loss 3.414399179746397e-05 Validation Loss 3.2758736779214814e-05\n",
            "Epoch 94: 44 -> Training Loss 3.21363695547916e-05 Validation Loss 3.275775088695809e-05\n",
            "Epoch 94: 48 -> Training Loss 3.3341857488267124e-05 Validation Loss 3.2756172004155815e-05\n",
            "Epoch 94: 52 -> Training Loss 3.167279646731913e-05 Validation Loss 3.2754120184108615e-05\n",
            "Epoch 94: 56 -> Training Loss 3.568130705389194e-05 Validation Loss 3.275312337791547e-05\n",
            "Epoch 94: 60 -> Training Loss 3.507451037876308e-05 Validation Loss 3.275315975770354e-05\n",
            "Epoch 94: 64 -> Training Loss 3.359117545187473e-05 Validation Loss 3.275322524132207e-05\n",
            "Epoch 94: 68 -> Training Loss 3.35394506691955e-05 Validation Loss 3.275408016634174e-05\n",
            "Epoch 94: 72 -> Training Loss 3.328645834699273e-05 Validation Loss 3.275607741670683e-05\n",
            "Epoch 94: 76 -> Training Loss 3.407496478757821e-05 Validation Loss 3.2757274311734363e-05\n",
            "Epoch 94: 80 -> Training Loss 3.184033994330093e-05 Validation Loss 3.275805647717789e-05\n",
            "Epoch 94: 84 -> Training Loss 3.091537655564025e-05 Validation Loss 3.275895869592205e-05\n",
            "Epoch 94: 88 -> Training Loss 3.268678119638935e-05 Validation Loss 3.275930794188753e-05\n",
            "Epoch 94: 92 -> Training Loss 2.8756294341292232e-05 Validation Loss 3.2759962778072804e-05\n",
            "Epoch 94: 96 -> Training Loss 3.392449434613809e-05 Validation Loss 3.276138522778638e-05\n",
            "Epoch 95: 0 -> Training Loss 3.562764322850853e-05 Validation Loss 3.276355710113421e-05\n",
            "Epoch 95: 4 -> Training Loss 3.356241722940467e-05 Validation Loss 3.2764866773504764e-05\n",
            "Epoch 95: 8 -> Training Loss 3.204854147043079e-05 Validation Loss 3.2764688512543216e-05\n",
            "Epoch 95: 12 -> Training Loss 3.358373578521423e-05 Validation Loss 3.276378265582025e-05\n",
            "Epoch 95: 16 -> Training Loss 3.456027479842305e-05 Validation Loss 3.276270945207216e-05\n",
            "Epoch 95: 20 -> Training Loss 3.4399690775899217e-05 Validation Loss 3.2761687180027366e-05\n",
            "Epoch 95: 24 -> Training Loss 3.407464100746438e-05 Validation Loss 3.2761148759163916e-05\n",
            "Epoch 95: 28 -> Training Loss 3.5248929634690285e-05 Validation Loss 3.27601155731827e-05\n",
            "Epoch 95: 32 -> Training Loss 3.3180913305841386e-05 Validation Loss 3.275861308793537e-05\n",
            "Epoch 95: 36 -> Training Loss 3.2749303500168025e-05 Validation Loss 3.2756684959167615e-05\n",
            "Epoch 95: 40 -> Training Loss 3.136696250294335e-05 Validation Loss 3.275386552559212e-05\n",
            "Epoch 95: 44 -> Training Loss 3.466447378741577e-05 Validation Loss 3.2751806429587305e-05\n",
            "Epoch 95: 48 -> Training Loss 3.067356738029048e-05 Validation Loss 3.275133349234238e-05\n",
            "Epoch 95: 52 -> Training Loss 3.3038839319488034e-05 Validation Loss 3.274944901932031e-05\n",
            "Epoch 95: 56 -> Training Loss 3.448615825618617e-05 Validation Loss 3.274695700383745e-05\n",
            "Epoch 95: 60 -> Training Loss 3.371147613506764e-05 Validation Loss 3.274454502388835e-05\n",
            "Epoch 95: 64 -> Training Loss 3.301741526229307e-05 Validation Loss 3.274189657531679e-05\n",
            "Epoch 95: 68 -> Training Loss 3.4924094507005066e-05 Validation Loss 3.2740899769123644e-05\n",
            "Epoch 95: 72 -> Training Loss 3.981187182944268e-05 Validation Loss 3.273950642324053e-05\n",
            "Epoch 95: 76 -> Training Loss 3.1806361221242696e-05 Validation Loss 3.273754919064231e-05\n",
            "Epoch 95: 80 -> Training Loss 3.4982775105163455e-05 Validation Loss 3.273555194027722e-05\n",
            "Epoch 95: 84 -> Training Loss 3.4376738767605275e-05 Validation Loss 3.2734380511101335e-05\n",
            "Epoch 95: 88 -> Training Loss 3.094890416832641e-05 Validation Loss 3.2733256375649944e-05\n",
            "Epoch 95: 92 -> Training Loss 3.168286639265716e-05 Validation Loss 3.2732430554460734e-05\n",
            "Epoch 95: 96 -> Training Loss 3.639638453023508e-05 Validation Loss 3.2731968531152233e-05\n",
            "Epoch 96: 0 -> Training Loss 3.2930009183473885e-05 Validation Loss 3.273112088209018e-05\n",
            "Epoch 96: 4 -> Training Loss 3.528100933181122e-05 Validation Loss 3.272968751844019e-05\n",
            "Epoch 96: 8 -> Training Loss 3.099129389738664e-05 Validation Loss 3.2727301004342735e-05\n",
            "Epoch 96: 12 -> Training Loss 3.672199818538502e-05 Validation Loss 3.2725976780056953e-05\n",
            "Epoch 96: 16 -> Training Loss 3.3124713809229434e-05 Validation Loss 3.2725925848353654e-05\n",
            "Epoch 96: 20 -> Training Loss 3.4776589018292725e-05 Validation Loss 3.272544563515112e-05\n",
            "Epoch 96: 24 -> Training Loss 3.5271019442006946e-05 Validation Loss 3.272547837696038e-05\n",
            "Epoch 96: 28 -> Training Loss 3.4599212085595354e-05 Validation Loss 3.2725849450798705e-05\n",
            "Epoch 96: 32 -> Training Loss 3.5328175727045164e-05 Validation Loss 3.2725474738981575e-05\n",
            "Epoch 96: 36 -> Training Loss 3.377140819793567e-05 Validation Loss 3.2724768971093e-05\n",
            "Epoch 96: 40 -> Training Loss 3.216050026821904e-05 Validation Loss 3.272404137533158e-05\n",
            "Epoch 96: 44 -> Training Loss 3.479277802398428e-05 Validation Loss 3.272294270573184e-05\n",
            "Epoch 96: 48 -> Training Loss 3.638381895143539e-05 Validation Loss 3.272191679570824e-05\n",
            "Epoch 96: 52 -> Training Loss 3.437705527176149e-05 Validation Loss 3.272121102781966e-05\n",
            "Epoch 96: 56 -> Training Loss 3.0397475711652078e-05 Validation Loss 3.2720196031732485e-05\n",
            "Epoch 96: 60 -> Training Loss 3.193950033164583e-05 Validation Loss 3.2720083254389465e-05\n",
            "Epoch 96: 64 -> Training Loss 3.537158772815019e-05 Validation Loss 3.272100366302766e-05\n",
            "Epoch 96: 68 -> Training Loss 3.371321508893743e-05 Validation Loss 3.27217931044288e-05\n",
            "Epoch 96: 72 -> Training Loss 3.307186489109881e-05 Validation Loss 3.272249159635976e-05\n",
            "Epoch 96: 76 -> Training Loss 3.760772233363241e-05 Validation Loss 3.272255344199948e-05\n",
            "Epoch 96: 80 -> Training Loss 3.40030892402865e-05 Validation Loss 3.272214962635189e-05\n",
            "Epoch 96: 84 -> Training Loss 3.346677840454504e-05 Validation Loss 3.272099274909124e-05\n",
            "Epoch 96: 88 -> Training Loss 3.3613701816648245e-05 Validation Loss 3.272065077908337e-05\n",
            "Epoch 96: 92 -> Training Loss 3.534302959451452e-05 Validation Loss 3.272054891567677e-05\n",
            "Epoch 96: 96 -> Training Loss 3.4835968108382076e-05 Validation Loss 3.271932291681878e-05\n",
            "Epoch 97: 0 -> Training Loss 3.447241397225298e-05 Validation Loss 3.2718082366045564e-05\n",
            "Epoch 97: 4 -> Training Loss 3.392433427507058e-05 Validation Loss 3.2717682188376784e-05\n",
            "Epoch 97: 8 -> Training Loss 3.357791865710169e-05 Validation Loss 3.2715215638745576e-05\n",
            "Epoch 97: 12 -> Training Loss 3.267628562753089e-05 Validation Loss 3.2712952815927565e-05\n",
            "Epoch 97: 16 -> Training Loss 3.3746884582797065e-05 Validation Loss 3.271259629400447e-05\n",
            "Epoch 97: 20 -> Training Loss 3.754477802431211e-05 Validation Loss 3.271209425292909e-05\n",
            "Epoch 97: 24 -> Training Loss 3.291307803010568e-05 Validation Loss 3.271211244282313e-05\n",
            "Epoch 97: 28 -> Training Loss 3.2347365049645305e-05 Validation Loss 3.271173045504838e-05\n",
            "Epoch 97: 32 -> Training Loss 3.3408952731406316e-05 Validation Loss 3.271044261055067e-05\n",
            "Epoch 97: 36 -> Training Loss 3.692052996484563e-05 Validation Loss 3.270965316914953e-05\n",
            "Epoch 97: 40 -> Training Loss 3.1328607292380184e-05 Validation Loss 3.2710187952034175e-05\n",
            "Epoch 97: 44 -> Training Loss 3.643063246272504e-05 Validation Loss 3.2710238883737475e-05\n",
            "Epoch 97: 48 -> Training Loss 3.318383460282348e-05 Validation Loss 3.2710337109165266e-05\n",
            "Epoch 97: 52 -> Training Loss 3.49713591276668e-05 Validation Loss 3.271011883043684e-05\n",
            "Epoch 97: 56 -> Training Loss 3.55284646502696e-05 Validation Loss 3.271025707363151e-05\n",
            "Epoch 97: 60 -> Training Loss 3.197626938344911e-05 Validation Loss 3.2709012884879485e-05\n",
            "Epoch 97: 64 -> Training Loss 3.3239419281017035e-05 Validation Loss 3.270889283157885e-05\n",
            "Epoch 97: 68 -> Training Loss 3.857248884742148e-05 Validation Loss 3.270812885602936e-05\n",
            "Epoch 97: 72 -> Training Loss 3.671160084195435e-05 Validation Loss 3.2707412174204364e-05\n",
            "Epoch 97: 76 -> Training Loss 3.2513384212506935e-05 Validation Loss 3.270665183663368e-05\n",
            "Epoch 97: 80 -> Training Loss 3.167573959217407e-05 Validation Loss 3.2704858313081786e-05\n",
            "Epoch 97: 84 -> Training Loss 3.254683542763814e-05 Validation Loss 3.270182787673548e-05\n",
            "Epoch 97: 88 -> Training Loss 3.244035906391218e-05 Validation Loss 3.2698917493689805e-05\n",
            "Epoch 97: 92 -> Training Loss 3.399267006898299e-05 Validation Loss 3.2698328141123056e-05\n",
            "Epoch 97: 96 -> Training Loss 3.531476977514103e-05 Validation Loss 3.269871376687661e-05\n",
            "Epoch 98: 0 -> Training Loss 3.52802308043465e-05 Validation Loss 3.269909575465135e-05\n",
            "Epoch 98: 4 -> Training Loss 3.357591413077898e-05 Validation Loss 3.2698080758564174e-05\n",
            "Epoch 98: 8 -> Training Loss 3.0429782782448456e-05 Validation Loss 3.269729495514184e-05\n",
            "Epoch 98: 12 -> Training Loss 3.252153328503482e-05 Validation Loss 3.269818262197077e-05\n",
            "Epoch 98: 16 -> Training Loss 3.6284989619161934e-05 Validation Loss 3.269853914389387e-05\n",
            "Epoch 98: 20 -> Training Loss 3.426893090363592e-05 Validation Loss 3.269893932156265e-05\n",
            "Epoch 98: 24 -> Training Loss 3.267469583079219e-05 Validation Loss 3.2698117138352245e-05\n",
            "Epoch 98: 28 -> Training Loss 3.315098729217425e-05 Validation Loss 3.269728404120542e-05\n",
            "Epoch 98: 32 -> Training Loss 3.631499203038402e-05 Validation Loss 3.26970694004558e-05\n",
            "Epoch 98: 36 -> Training Loss 3.125154034933075e-05 Validation Loss 3.269669832661748e-05\n",
            "Epoch 98: 40 -> Training Loss 3.5667479096446186e-05 Validation Loss 3.26952722389251e-05\n",
            "Epoch 98: 44 -> Training Loss 3.352565909153782e-05 Validation Loss 3.2694762921892107e-05\n",
            "Epoch 98: 48 -> Training Loss 3.607376856962219e-05 Validation Loss 3.2696116250008345e-05\n",
            "Epoch 98: 52 -> Training Loss 3.0465162126347423e-05 Validation Loss 3.2697527785785496e-05\n",
            "Epoch 98: 56 -> Training Loss 3.528995512169786e-05 Validation Loss 3.2698430004529655e-05\n",
            "Epoch 98: 60 -> Training Loss 3.099090827163309e-05 Validation Loss 3.269952139817178e-05\n",
            "Epoch 98: 64 -> Training Loss 3.206071414751932e-05 Validation Loss 3.2699106668587774e-05\n",
            "Epoch 98: 68 -> Training Loss 3.264650149503723e-05 Validation Loss 3.269923763582483e-05\n",
            "Epoch 98: 72 -> Training Loss 3.3087697374867275e-05 Validation Loss 3.2699404982849956e-05\n",
            "Epoch 98: 76 -> Training Loss 3.260419180151075e-05 Validation Loss 3.27002489939332e-05\n",
            "Epoch 98: 80 -> Training Loss 2.9671449738088995e-05 Validation Loss 3.270145316491835e-05\n",
            "Epoch 98: 84 -> Training Loss 3.350368933752179e-05 Validation Loss 3.2702319003874436e-05\n",
            "Epoch 98: 88 -> Training Loss 3.455924888839945e-05 Validation Loss 3.2702584576327354e-05\n",
            "Epoch 98: 92 -> Training Loss 3.351842315169051e-05 Validation Loss 3.270383604103699e-05\n",
            "Epoch 98: 96 -> Training Loss 3.155716694891453e-05 Validation Loss 3.270396700827405e-05\n",
            "Epoch 99: 0 -> Training Loss 3.534148709150031e-05 Validation Loss 3.270364322816022e-05\n",
            "Epoch 99: 4 -> Training Loss 3.710456076078117e-05 Validation Loss 3.270374145358801e-05\n",
            "Epoch 99: 8 -> Training Loss 3.5444609238766134e-05 Validation Loss 3.270307934144512e-05\n",
            "Epoch 99: 12 -> Training Loss 3.128541720798239e-05 Validation Loss 3.270338856964372e-05\n",
            "Epoch 99: 16 -> Training Loss 3.348808240843937e-05 Validation Loss 3.2703646866139024e-05\n",
            "Epoch 99: 20 -> Training Loss 3.2979820389300585e-05 Validation Loss 3.2703421311452985e-05\n",
            "Epoch 99: 24 -> Training Loss 2.9294202249730006e-05 Validation Loss 3.270338856964372e-05\n",
            "Epoch 99: 28 -> Training Loss 3.5584995202952996e-05 Validation Loss 3.2702733733458444e-05\n",
            "Epoch 99: 32 -> Training Loss 3.396242391318083e-05 Validation Loss 3.270216257078573e-05\n",
            "Epoch 99: 36 -> Training Loss 3.120104520348832e-05 Validation Loss 3.2702497264835984e-05\n",
            "Epoch 99: 40 -> Training Loss 3.169633055222221e-05 Validation Loss 3.2702271710149944e-05\n",
            "Epoch 99: 44 -> Training Loss 3.2307034416589886e-05 Validation Loss 3.270054730819538e-05\n",
            "Epoch 99: 48 -> Training Loss 3.3785723644541577e-05 Validation Loss 3.2699481380404904e-05\n",
            "Epoch 99: 52 -> Training Loss 3.227463093935512e-05 Validation Loss 3.26977860822808e-05\n",
            "Epoch 99: 56 -> Training Loss 3.263012695242651e-05 Validation Loss 3.2695792469894513e-05\n",
            "Epoch 99: 60 -> Training Loss 3.282437683083117e-05 Validation Loss 3.26943991240114e-05\n",
            "Epoch 99: 64 -> Training Loss 3.1355411920230836e-05 Validation Loss 3.2693766115698963e-05\n",
            "Epoch 99: 68 -> Training Loss 3.6051606002729386e-05 Validation Loss 3.269346052547917e-05\n",
            "Epoch 99: 72 -> Training Loss 3.2372969144489616e-05 Validation Loss 3.269315493525937e-05\n",
            "Epoch 99: 76 -> Training Loss 3.4786069591064006e-05 Validation Loss 3.2692536478862166e-05\n",
            "Epoch 99: 80 -> Training Loss 3.2042196835391223e-05 Validation Loss 3.2692099921405315e-05\n",
            "Epoch 99: 84 -> Training Loss 3.5055414628004655e-05 Validation Loss 3.2692048989702016e-05\n",
            "Epoch 99: 88 -> Training Loss 3.6562276363838464e-05 Validation Loss 3.2691787055227906e-05\n",
            "Epoch 99: 92 -> Training Loss 3.1677518563810736e-05 Validation Loss 3.269125591032207e-05\n",
            "Epoch 99: 96 -> Training Loss 3.535203723004088e-05 Validation Loss 3.269092121627182e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "e4oMHPO2hZN9",
        "outputId": "5f9d14c2-a0c2-4507-cc1d-d9bdc4e7a5a6"
      },
      "source": [
        "# Graph \n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set(style=\"darkgrid\")\n",
        "\n",
        "plt.plot(range(10000),training_loss , label='Training Loss')\n",
        "plt.plot(range(10000), validation_loss, label = 'Validation Loss')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.savefig(\"Loss_Graph\", dpi=300)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOV7G8fEFQHQ"
      },
      "source": [
        "validation_dataset = []\n",
        "validation_prediction = []"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouqAggpEfxPj"
      },
      "source": [
        "for x in range(100):\n",
        "    valid_features, valid_labels = next(iter(valid_dl))\n",
        "    valid_pred = model(valid_features)\n",
        "    validation_dataset.extend(list(valid_features))\n",
        "    validation_prediction.extend(list(valid_pred))\n",
        "\n",
        "valid_pred_dataset = []\n",
        "for i in range(4533):\n",
        "    valid_pred_dataset.append([validation_dataset[i][0].item(), validation_dataset[i][1].item(), validation_dataset[i][2].item(), validation_dataset[i][3].item(),\n",
        "        validation_prediction[i][0].item(), validation_prediction[i][1].item(), validation_prediction[i][2].item(), validation_prediction[i][3].item()])\n",
        "    \n",
        "valid_pred_dataset = pd.DataFrame(valid_pred_dataset)\n",
        "valid_pred_dataset.to_csv(\"Predictions.csv\", index=False)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rHSxMMWFTm5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}